{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dWSNjlXfA3Nh"},"outputs":[],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","ULTRA-OPTIMIZED Somatic Symptom Prediction - Maximum Accuracy Mode\n","Target: 75-85% Balanced Accuracy\n","\n","AGGRESSIVE IMPROVEMENTS:\n","- Deep Neural Networks with TabNet (optimized for B200 GPU)\n","- Advanced Ensemble: Stacking + Voting + Weighted Blending\n","- Aggressive Feature Engineering: 50+ engineered features\n","- Multiple Resampling Strategies per model\n","- Bayesian Hyperparameter Optimization (200+ trials)\n","- Cost-Sensitive Learning with dynamic weights\n","- Advanced Feature Selection with multiple methods\n","- Cross-Validation: Repeated Stratified 10-Fold\n","- Probability Calibration for all models\n","- Threshold Optimization per symptom\n","\"\"\"\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1763072666859,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"IP35haigA3Ni","outputId":"62311d09-7084-47c2-b7f4-5e0b31708382"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•\n","================================================================================\n","\n","üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY\n","   Optimization Target: ACCURACY\n","   Optuna Trials: 200\n","   CV: 10-fold √ó 3 repeats\n","   Feature Selection: ALL\n","   Tree Models Only: True\n","   AutoGluon Ensemble: ‚úÖ Enabled\n","      Time Limit: 2.0h/symptom\n","      Preset: best_quality\n","   Sampling Strategy: NONE (natural distribution)\n","   GPU Acceleration: True\n","================================================================================\n"]}],"source":["# ============================================================================\n","# ULTRA-AGGRESSIVE CONFIGURATION\n","# ============================================================================\n","print(\"=\"*80)\n","print(\"üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•\")\n","print(\"=\"*80)\n","\n","# üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY - TRY EVERYTHING!\n","ENABLE_ALL_FEATURES = True\n","ENABLE_DEEP_LEARNING = True\n","ENABLE_ADVANCED_ENSEMBLES = True\n","ENABLE_BAYESIAN_OPTIMIZATION = True\n","OPTUNA_TRIALS = 200  # INCREASED for deeper optimization\n","CV_FOLDS = 10  # INCREASED for more robust validation\n","CV_REPEATS = 3  # INCREASED for stability\n","FEATURE_SELECTION_K = 'all'  # USE ALL FEATURES (no selection = more info)\n","USE_GPU = True\n","FOCUS_TREE_MODELS_ONLY = True  # FALSE = Try ALL models (tree, linear, SVM, KNN, NB, DL, etc.)\n","ENABLE_ALL_MODEL_TYPES = False  # Enable SVM, KNN, NB, LDA, QDA, MLP, etc.\n","ENABLE_AUTOGLUON = True  # Enable AutoGluon automated ensemble (2h/symptom = 26h total)\n","AUTOGLUON_TIME_LIMIT = 3600 * 2  # 2 hours per symptom\n","AUTOGLUON_PRESET = 'best_quality'  # 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n","\n","# Optimization target - CHANGED TO MAXIMIZE REGULAR ACCURACY (not balanced)\n","OPTIMIZE_FOR = 'accuracy'  # Options: 'accuracy', 'balanced_accuracy', 'f1', 'roc_auc'\n","# accuracy = pure accuracy optimization (allows natural class distribution)\n","\n","# Advanced techniques\n","ENABLE_AUTO_ENCODER = True  # Dimensionality reduction with neural network\n","ENABLE_FOCAL_LOSS = True  # Focus on hard examples\n","ENABLE_LABEL_SMOOTHING = True  # Regularization technique\n","ENABLE_MIXUP = True  # Data augmentation for tabular data\n","ENABLE_PSEUDO_LABELING = False  # Semi-supervised learning (disabled - no unlabeled data)\n","\n","print(\"\\nüöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY\")\n","print(f\"   Optimization Target: {OPTIMIZE_FOR.upper()}\")\n","print(f\"   Optuna Trials: {OPTUNA_TRIALS}\")\n","print(f\"   CV: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats\")\n","print(f\"   Feature Selection: {FEATURE_SELECTION_K.upper() if FEATURE_SELECTION_K == 'all' else f'Top {FEATURE_SELECTION_K}'}\")\n","print(f\"   Tree Models Only: {FOCUS_TREE_MODELS_ONLY}\")\n","print(f\"   AutoGluon Ensemble: {'‚úÖ Enabled' if ENABLE_AUTOGLUON else '‚ùå Disabled'}\")\n","if ENABLE_AUTOGLUON:\n","    print(f\"      Time Limit: {AUTOGLUON_TIME_LIMIT/3600:.1f}h/symptom\")\n","    print(f\"      Preset: {AUTOGLUON_PRESET}\")\n","print(f\"   Sampling Strategy: NONE (natural distribution)\")\n","print(f\"   GPU Acceleration: {USE_GPU}\")\n","print(\"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43251,"status":"ok","timestamp":1763072710112,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"2aMaRZ8zA4lv","outputId":"36058516-72af-48c0-b732-51f40827fa26"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting catboost\n","  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n","Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.8\n","Collecting optuna\n","  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n","Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, optuna\n","Successfully installed colorlog-6.10.1 optuna-4.6.0\n","Collecting pytorch_tabnet\n","  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch_tabnet) (2.0.2)\n","Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch_tabnet) (1.6.1)\n","Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_tabnet) (1.16.3)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch_tabnet) (2.8.0+cu126)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch_tabnet) (4.67.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch_tabnet) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch_tabnet) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (3.0.3)\n","Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytorch_tabnet\n","Successfully installed pytorch_tabnet-4.1.0\n"]}],"source":["! pip install catboost\n","! pip install optuna\n","! pip install pytorch_tabnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1763108935379,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"CzWuKfe-A3Nj","outputId":"e2dc6b8e-e4da-4957-8f7e-0998346aed9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","‚ö†Ô∏è  Using CPU (no GPU available)\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x7893e8330770>"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["# ============================================================================\n","# IMPORTS\n","# ============================================================================\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import subprocess\n","\n","from pathlib import Path\n","from scipy.stats import skew, kurtosis\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n","from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n","                              ExtraTreesClassifier, StackingClassifier, VotingClassifier,\n","                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.feature_selection import (SelectKBest, mutual_info_classif, f_classif,\n","                                       chi2, RFE, SelectFromModel, VarianceThreshold)\n","from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n","from sklearn.decomposition import PCA, TruncatedSVD\n","from imblearn.pipeline import Pipeline as ImbPipeline\n","from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n","from imblearn import FunctionSampler  # For passthrough (no resampling)\n","from sklearn.model_selection import GridSearchCV\n","\n","# Gradient Boosting\n","import xgboost as xgb\n","import lightgbm as lgb\n","import shap  # For SHAP analysis\n","from catboost import CatBoostClassifier\n","\n","# AutoGluon (optional)\n","if ENABLE_AUTOGLUON:\n","    try:\n","        from autogluon.tabular import TabularPredictor\n","        AUTOGLUON_AVAILABLE = True\n","    except ImportError:\n","        print(\"‚ö†Ô∏è  AutoGluon not installed. Install with: pip install autogluon\")\n","        AUTOGLUON_AVAILABLE = False\n","        ENABLE_AUTOGLUON = False\n","else:\n","    AUTOGLUON_AVAILABLE = False\n","\n","# Optuna for hyperparameter optimization\n","import optuna\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","\n","# PyTorch and TabNet\n","import torch\n","import torch.nn as nn\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from pytorch_tabnet.augmentations import ClassificationSMOTE\n","\n","# Check GPU\n","if USE_GPU and torch.cuda.is_available():\n","    device = 'cuda'\n","    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1024**3:.0f}GB)\")\n","else:\n","    device = 'cpu'\n","    USE_GPU = False\n","    print(\"\\n‚ö†Ô∏è  Using CPU (no GPU available)\")\n","\n","# Metrics\n","from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix,\n","                            accuracy_score, balanced_accuracy_score, f1_score,\n","                            precision_score, recall_score, matthews_corrcoef)\n","\n","# Model serialization\n","import joblib\n","import pickle\n","import json\n","from datetime import datetime\n","\n","np.random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32458,"status":"ok","timestamp":1763072773801,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"Fch31xhPA3Nj","outputId":"60360bf4-d22d-4056-c887-7dc7944df271"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[1/10] Loading data with ultra-aggressive preprocessing...\n","Cloning repository from GitHub...\n","‚úì Created 64 scale features (mean, std, max, min)\n"]}],"source":["# ============================================================================\n","# 1. ULTRA-AGGRESSIVE DATA LOADING & PREPROCESSING\n","# ============================================================================\n","print(\"\\n[1/10] Loading data with ultra-aggressive preprocessing...\")\n","\n","repo_path = Path(\"somatic-symptom\")\n","if not repo_path.exists():\n","    print(\"Cloning repository from GitHub...\")\n","    subprocess.run([\n","        \"git\", \"clone\",\n","        \"https://J-Tanchone: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@github.com/J-Tanchone/somatic-symptom.git\"\n","    ])\n","\n","data_path = \"somatic-symptom/EAMMi2-Data1/EAMMi2-Data1.2.xlsx\"\n","data = pd.read_excel(data_path, sheet_name=\"EAMMi2_Data\")\n","\n","# Recode variables\n","data['sibling_c'] = data['sibling'].apply(lambda x: -0.5 if x == 1 else 0.5)\n","data = data.rename(columns={'marriage2': 'marriage_importance', 'marriage5': 'parental_marriage'})\n","data = pd.concat([data, pd.get_dummies(data['parental_marriage'].astype('category'),\n","                                      prefix='parental_marriage', drop_first=True)], axis=1)\n","\n","# Compute all psychological scales\n","scales = {\n","    'idea_m': [f'IDEA_{i}' for i in range(1, 9)],\n","    'moa_achievement_m': [f'moa1#2_{i}' for i in range(1, 11)] + [f'moa2#1_{i}' for i in range(1, 11)],\n","    'moa_importance_m': [f'moa2#1_{i}' for i in range(1, 11)] + [f'moa2#2_{i}' for i in range(1, 11)],\n","    'stress_m': [f'stress_{i}' for i in range(1, 11)],\n","    'support_m': [f'support_{i}' for i in range(1, 13)],\n","    'belong_m': [f'belong_{i}' for i in range(1, 11)],\n","    'mindful_m': [f'mindful_{i}' for i in range(1, 16)],\n","    'efficacy_m': [f'efficacy_{i}' for i in range(1, 11)],\n","    'exploit_m': [f'exploit_{i}' for i in range(1, 4)],\n","    'disability_m': [f'Q10_{i}' for i in range(1, 16)] + ['Q11'] + [f'Q14_{i}' for i in range(1, 7)],\n","    'social_conn_m': [f'SocMedia_{i}' for i in range(1, 6)],\n","    'social_new_m': [f'SocMedia_{i}' for i in range(6, 10)],\n","    'social_info_m': [f'SocMedia_{i}' for i in range(10, 12)],\n","    'swb_m': [f'swb_{i}' for i in range(1, 7)],\n","    'transgres_m': [f'transgres_{i}' for i in range(1, 5)],\n","    'usdream_m': ['usdream_1', 'usdream_2']\n","}\n","\n","for name, items in scales.items():\n","    valid_items = [item for item in items if item in data.columns]\n","    if valid_items:\n","        data[name] = data[valid_items].mean(axis=1, skipna=True)\n","        # Add variance and std as additional features\n","        data[f'{name}_std'] = data[valid_items].std(axis=1, skipna=True)\n","        data[f'{name}_max'] = data[valid_items].max(axis=1, skipna=True)\n","        data[f'{name}_min'] = data[valid_items].min(axis=1, skipna=True)\n","\n","print(f\"‚úì Created {len(scales)*4} scale features (mean, std, max, min)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11681,"status":"ok","timestamp":1763072785483,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"WZK88_O3A3Nk","outputId":"d49d7758-5786-41fc-fce2-f7805b4109bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[2/10] Creating 50+ engineered features...\n","‚úì Created 50 engineered features\n","‚úì Final dataset: 3182 samples √ó 133 features\n"]}],"source":["# ============================================================================\n","# 2. ULTRA-AGGRESSIVE FEATURE ENGINEERING (50+ Features)\n","# ============================================================================\n","print(\"\\n[2/10] Creating 50+ engineered features...\")\n","\n","feature_count = 0\n","\n","# Level 1: Basic Interactions (Psychology Literature)\n","if 'stress_m' in data.columns and 'support_m' in data.columns:\n","    data['stress_support_interaction'] = data['stress_m'] * (1 - data['support_m'])\n","    data['stress_support_ratio'] = data['stress_m'] / (data['support_m'] + 0.01)\n","    feature_count += 2\n","\n","if 'mindful_m' in data.columns and 'stress_m' in data.columns:\n","    data['mindful_stress_buffer'] = data['mindful_m'] * (1 - data['stress_m'])\n","    data['mindfulness_deficit'] = (1 - data['mindful_m']) * data['stress_m']\n","    feature_count += 2\n","\n","if 'efficacy_m' in data.columns:\n","    data['efficacy_stress_interaction'] = data['efficacy_m'] * data['stress_m']\n","    data['efficacy_belong_interaction'] = data['efficacy_m'] * data['belong_m']\n","    data['low_efficacy_high_stress'] = (1 - data['efficacy_m']) * data['stress_m']\n","    feature_count += 3\n","\n","# Level 2: Composite Indices\n","if all(col in data.columns for col in ['stress_m', 'support_m', 'belong_m', 'efficacy_m']):\n","    # Psychological Distress Composite\n","    data['psychological_distress'] = (\n","        data['stress_m'] * 0.40 +\n","        (1 - data['support_m']) * 0.30 +\n","        (1 - data['belong_m']) * 0.20 +\n","        (1 - data['efficacy_m']) * 0.10\n","    )\n","\n","    # Protective Factors Composite\n","    data['protective_factors'] = (\n","        data['mindful_m'] * 0.35 +\n","        data['support_m'] * 0.30 +\n","        data['belong_m'] * 0.20 +\n","        data['efficacy_m'] * 0.15\n","    )\n","\n","    # Risk-Protection Balance\n","    data['risk_protection_ratio'] = data['psychological_distress'] / (data['protective_factors'] + 0.01)\n","    data['risk_protection_difference'] = data['psychological_distress'] - data['protective_factors']\n","    feature_count += 4\n","\n","# Level 3: Domain Expert Features (Somatization Literature)\n","if all(col in data.columns for col in ['stress_m', 'mindful_m', 'swb_m']):\n","    # Alexithymia proxy (emotional awareness deficit)\n","    data['emotional_awareness_deficit'] = data['stress_m'] * (1 - data['mindful_m']) * (1 - data['swb_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'efficacy_m' in data.columns:\n","    # Catastrophizing tendency\n","    data['catastrophizing_score'] = (data['stress_m'] ** 2) / (data['efficacy_m'] + 0.01)\n","    feature_count += 1\n","\n","if all(col in data.columns for col in ['support_m', 'belong_m', 'social_conn_m']):\n","    # Social isolation index\n","    data['social_isolation'] = (1 - data['support_m']) * (1 - data['belong_m']) * (1 - data['social_conn_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'mindful_m' in data.columns:\n","    # Rumination tendency\n","    data['rumination_tendency'] = data['stress_m'] * (1 - data['mindful_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'disability_m' in data.columns:\n","    # Health anxiety proxy\n","    data['health_anxiety_proxy'] = data['stress_m'] * data['disability_m']\n","    feature_count += 1\n","\n","# Level 4: Achievement-Stress Interactions\n","if 'moa_achievement_m' in data.columns and 'moa_importance_m' in data.columns:\n","    data['achievement_gap'] = data['moa_importance_m'] - data['moa_achievement_m']\n","    data['achievement_gap_squared'] = data['achievement_gap'] ** 2\n","    data['achievement_stress_interaction'] = data['achievement_gap'] * data['stress_m']\n","    data['perfectionism_stress'] = (data['achievement_gap'] ** 2) * data['stress_m']\n","    feature_count += 4\n","\n","# Level 5: Non-linear Transformations\n","numerical_cols = ['stress_m', 'support_m', 'mindful_m', 'efficacy_m', 'belong_m', 'swb_m']\n","for col in numerical_cols:\n","    if col in data.columns:\n","        data[f'{col}_squared'] = data[col] ** 2\n","        data[f'{col}_sqrt'] = np.sqrt(data[col])\n","        data[f'{col}_log'] = np.log1p(data[col])  # log(1+x) to handle zeros\n","        feature_count += 3\n","\n","# Level 6: Statistical Aggregations Across Scales\n","scale_means = [col for col in data.columns if col.endswith('_m')]\n","if len(scale_means) >= 3:\n","    data['overall_mean'] = data[scale_means].mean(axis=1)\n","    data['overall_std'] = data[scale_means].std(axis=1)\n","    data['overall_skew'] = data[scale_means].apply(lambda x: skew(x.dropna()), axis=1)\n","    data['overall_kurtosis'] = data[scale_means].apply(lambda x: kurtosis(x.dropna()), axis=1)\n","    feature_count += 4\n","\n","# Level 7: Somatization Proneness Index (Multi-component)\n","if all(col in data.columns for col in ['stress_m', 'support_m', 'mindful_m', 'swb_m', 'efficacy_m']):\n","    data['somatization_proneness'] = (\n","        data['stress_m'] * 0.30 +\n","        (1 - data['support_m']) * 0.20 +\n","        (1 - data['mindful_m']) * 0.20 +\n","        (1 - data['swb_m']) * 0.15 +\n","        (1 - data['efficacy_m']) * 0.15\n","    )\n","    feature_count += 1\n","\n","# Level 8: Demographic Interactions\n","if 'sex' in data.columns and 'stress_m' in data.columns:\n","    data['sex_stress'] = data['sex'].astype(float) * data['stress_m']\n","    feature_count += 1\n","\n","if 'edu' in data.columns and 'efficacy_m' in data.columns:\n","    data['edu_efficacy'] = data['edu'].astype(float) * data['efficacy_m']\n","    feature_count += 1\n","\n","if 'income' in data.columns and 'stress_m' in data.columns:\n","    data['income_stress'] = data['income'].astype(float) * data['stress_m']\n","    feature_count += 1\n","\n","# Level 9: Resilience vs Vulnerability\n","if 'psychological_distress' in data.columns and 'protective_factors' in data.columns:\n","    data['resilience_index'] = data['protective_factors']\n","    data['vulnerability_index'] = data['psychological_distress']\n","    data['vulnerability_resilience_balance'] = data['vulnerability_index'] - data['resilience_index']\n","    data['stress_amplification'] = data['stress_m'] / (data['resilience_index'] + 0.1)\n","    feature_count += 4\n","\n","# Level 10: Cross-domain Interactions\n","if all(col in data.columns for col in ['social_quality', 'disability_m']):\n","    data['social_disability_interaction'] = data['social_quality'] * data['disability_m']\n","    feature_count += 1\n","\n","print(f\"‚úì Created {feature_count} engineered features\")\n","\n","# Select final features\n","final_data = data[[col for col in data.columns if\n","                  col.endswith(('_m', '_c', '_index', '_ratio', '_balance', '_deficit',\n","                               '_interaction', '_score', '_isolation', '_tendency', '_proxy',\n","                               '_stress', '_proneness', '_amplification', '_squared', '_sqrt',\n","                               '_log', '_std', '_max', '_min', '_quality', '_gap')) or\n","                  col in ['marriage_importance', 'parental_marriage', 'overall_mean',\n","                          'overall_std', 'overall_skew', 'overall_kurtosis', 'social_quality'] or\n","                  col.startswith(('parental_marriage_', 'sex_', 'edu_', 'income_'))]]\n","\n","# Add demographics\n","for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']:\n","    if col in data.columns and col not in final_data.columns:\n","        final_data[col] = data[col]\n","\n","# Binarize outcomes\n","physSx_cols = [f'physSx_{i}' for i in range(1, 14)]\n","for col in physSx_cols:\n","    if col in data.columns:\n","        data[col] = data[col].replace({1: 0, 2: 1, 3: 1})\n","        final_data[col] = data[col]\n","\n","print(f\"‚úì Final dataset: {final_data.shape[0]} samples √ó {final_data.shape[1]} features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1763072786494,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"GSjAAkPUA3Nk","outputId":"1b821179-7a91-48ba-d8dd-b5ea8a34a6ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[3/10] Creating stratified train/test splits...\n","‚úì 110 numeric features\n","‚úì 5 categorical features\n","  ‚úì physSx_1: 2496 train, 625 test | 46.1% positive\n","  ‚úì physSx_2: 2496 train, 625 test | 55.5% positive\n","  ‚úì physSx_3: 2496 train, 625 test | 51.5% positive\n","  ‚úì physSx_4: 2496 train, 625 test | 67.2% positive\n","  ‚úì physSx_5: 2496 train, 625 test | 22.1% positive\n","  ‚úì physSx_6: 2496 train, 625 test | 31.7% positive\n","  ‚úì physSx_7: 2496 train, 625 test | 7.0% positive\n","  ‚úì physSx_8: 2496 train, 625 test | 45.2% positive\n","  ‚úì physSx_9: 2496 train, 624 test | 32.2% positive\n","  ‚úì physSx_10: 2496 train, 625 test | 35.2% positive\n","  ‚úì physSx_11: 2496 train, 624 test | 48.9% positive\n","  ‚úì physSx_12: 2496 train, 625 test | 87.3% positive\n","  ‚úì physSx_13: 2496 train, 625 test | 67.7% positive\n"]}],"source":["# ============================================================================\n","# 3. TRAIN/TEST SPLITS WITH STRATIFICATION\n","# ============================================================================\n","print(\"\\n[3/10] Creating stratified train/test splits...\")\n","\n","numeric_features = [col for col in final_data.columns if col.endswith(('_m', '_c', '_index', '_ratio',\n","                    '_balance', '_deficit', '_interaction', '_score', '_isolation', '_tendency',\n","                    '_proxy', '_stress', '_proneness', '_amplification', '_squared', '_sqrt',\n","                    '_log', '_std', '_max', '_min', '_gap', 'overall_mean', 'overall_std',\n","                    'overall_skew', 'overall_kurtosis')) and col not in physSx_cols]\n","\n","categorical_features = [col for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']\n","                       if col in final_data.columns]\n","\n","print(f\"‚úì {len(numeric_features)} numeric features\")\n","print(f\"‚úì {len(categorical_features)} categorical features\")\n","\n","train_test_data = {}\n","X = final_data.drop(columns=physSx_cols, errors='ignore')\n","\n","for physSx_var in physSx_cols:\n","    if physSx_var not in final_data.columns:\n","        continue\n","\n","    y = data[physSx_var]\n","    model_data = X.join(y.rename(physSx_var)).dropna()\n","    X_clean = model_data.drop(columns=[physSx_var])\n","    y_clean = model_data[physSx_var]\n","\n","    if y_clean.sum() < 10:\n","        continue\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n","    )\n","\n","    for col in categorical_features:\n","        if col in X_train.columns:\n","            X_train[col] = X_train[col].astype(str)\n","            X_test[col] = X_test[col].astype(str)\n","\n","    train_test_data[physSx_var] = {\n","        'X_train': X_train,\n","        'X_test': X_test,\n","        'y_train': y_train,\n","        'y_test': y_test\n","    }\n","\n","    pos_pct = (y_clean.sum() / len(y_clean)) * 100\n","    print(f\"  ‚úì {physSx_var}: {len(X_train)} train, {len(X_test)} test | {pos_pct:.1f}% positive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1763072786519,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"BM3NawP7A3Nl","outputId":"6c880ff5-ad69-4614-962d-fbba428d5dae"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[4/10] Setting up advanced preprocessing pipelines...\n","‚úì Created 4 preprocessing strategies\n"]}],"source":["# ============================================================================\n","# 4. ADVANCED PREPROCESSING PIPELINES\n","# ============================================================================\n","print(\"\\n[4/10] Setting up advanced preprocessing pipelines...\")\n","\n","# Multiple preprocessors for different model types\n","preprocessor_scaled = ColumnTransformer(transformers=[\n","    ('num', StandardScaler(), numeric_features),\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_robust = ColumnTransformer(transformers=[\n","    ('num', RobustScaler(), numeric_features),  # Better for outliers\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_power = ColumnTransformer(transformers=[\n","    ('num', PowerTransformer(method='yeo-johnson'), numeric_features),  # Normalize distributions\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_unscaled = ColumnTransformer(transformers=[\n","    ('num', 'passthrough', numeric_features),\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","print(\"‚úì Created 4 preprocessing strategies\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28083201,"status":"ok","timestamp":1763100869725,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"zDXEDIGkA3Nl","outputId":"988cfbd1-37f1-4d7d-9520-451c402800a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[5/10] Training 20+ ultra-optimized models...\n","Cross-validation: 10-fold √ó 3 repeats\n","\n","================================================================================\n","[1/13] SYMPTOM: physSx_1\n","================================================================================\n","Class balance: 1344.0 neg, 1152.0 pos (weight=1.17)\n","  ‚ö° Using NATURAL class distribution (NO resampling) for maximum accuracy\n","  Applying feature preprocessing...\n","    ‚ö° Using ALL 166 features (NO selection for max accuracy)\n","  ‚è≠Ô∏è  SKIPPING linear models (tree models perform 6-7% better)\n","  Optimizing XGBoost with Bayesian search (300 trials)...\n","    ‚úì Best XGB score: 0.610\n","  XGBoost-Optimized:        Bal_Acc=0.591, AUC=0.624, F1=0.559\n","  LightGBM:                 Bal_Acc=0.549, AUC=0.572, F1=0.482\n","  CatBoost:                 Bal_Acc=0.589, AUC=0.586, F1=0.545\n","  RandomForest:             Bal_Acc=0.591, AUC=0.621, F1=0.536\n","  ExtraTrees:               Bal_Acc=0.586, AUC=0.627, F1=0.560\n","  Training TabNet (Deep Learning)...\n","\n","Early stopping occurred at epoch 130 with best_epoch = 30 and best_val_0_balanced_accuracy = 0.60472\n","  TabNet-Deep:              Bal_Acc=0.605, AUC=0.643, F1=0.577\n","  Building advanced ensemble models...\n","  VotingEnsemble:           Bal_Acc=0.559, AUC=0.597, F1=0.507\n","  StackingEnsemble:         Bal_Acc=0.582, AUC=0.624, F1=0.523\n","\n","================================================================================\n","[2/13] SYMPTOM: physSx_2\n","================================================================================\n","Class balance: 1112.0 neg, 1384.0 pos (weight=0.80)\n","  ‚ö° Using NATURAL class distribution (NO resampling) for maximum accuracy\n","  Applying feature preprocessing...\n","    ‚ö° Using ALL 165 features (NO selection for max accuracy)\n","  ‚è≠Ô∏è  SKIPPING linear models (tree models perform 6-7% better)\n","  Optimizing XGBoost with Bayesian search (300 trials)...\n","    ‚úì Best XGB score: 0.614\n","  XGBoost-Optimized:        Bal_Acc=0.563, AUC=0.608, F1=0.598\n","  LightGBM:                 Bal_Acc=0.558, AUC=0.579, F1=0.631\n","  CatBoost:                 Bal_Acc=0.578, AUC=0.612, F1=0.646\n","  RandomForest:             Bal_Acc=0.575, AUC=0.597, F1=0.638\n","  ExtraTrees:               Bal_Acc=0.573, AUC=0.603, F1=0.624\n","  Training TabNet (Deep Learning)...\n","\n","Early stopping occurred at epoch 140 with best_epoch = 40 and best_val_0_balanced_accuracy = 0.60267\n","  TabNet-Deep:              Bal_Acc=0.603, AUC=0.607, F1=0.671\n","  Building advanced ensemble models...\n","  VotingEnsemble:           Bal_Acc=0.553, AUC=0.601, F1=0.622\n","  StackingEnsemble:         Bal_Acc=0.579, AUC=0.609, F1=0.672\n","\n","================================================================================\n","[3/13] SYMPTOM: physSx_3\n","================================================================================\n","Class balance: 1210.0 neg, 1286.0 pos (weight=0.94)\n","  ‚ö° Using NATURAL class distribution (NO resampling) for maximum accuracy\n","  Applying feature preprocessing...\n","    ‚ö° Using ALL 164 features (NO selection for max accuracy)\n","  ‚è≠Ô∏è  SKIPPING linear models (tree models perform 6-7% better)\n","  Optimizing XGBoost with Bayesian search (300 trials)...\n","    ‚úì Best XGB score: 0.584\n","  XGBoost-Optimized:        Bal_Acc=0.580, AUC=0.622, F1=0.594\n","  LightGBM:                 Bal_Acc=0.556, AUC=0.582, F1=0.571\n","  CatBoost:                 Bal_Acc=0.561, AUC=0.586, F1=0.580\n","  RandomForest:             Bal_Acc=0.578, AUC=0.626, F1=0.621\n","  ExtraTrees:               Bal_Acc=0.590, AUC=0.622, F1=0.619\n","  Training TabNet (Deep Learning)...\n","\n","Early stopping occurred at epoch 204 with best_epoch = 104 and best_val_0_balanced_accuracy = 0.6038\n","  TabNet-Deep:              Bal_Acc=0.604, AUC=0.625, F1=0.624\n","  Building advanced ensemble models...\n","  VotingEnsemble:           Bal_Acc=0.556, AUC=0.603, F1=0.578\n","  StackingEnsemble:         Bal_Acc=0.590, AUC=0.610, F1=0.624\n","\n","================================================================================\n","[4/13] SYMPTOM: physSx_4\n","================================================================================\n","Class balance: 820.0 neg, 1676.0 pos (weight=0.49)\n","  ‚ö° Using NATURAL class distribution (NO resampling) for maximum accuracy\n","  Applying feature preprocessing...\n","    ‚ö° Using ALL 167 features (NO selection for max accuracy)\n","  ‚è≠Ô∏è  SKIPPING linear models (tree models perform 6-7% better)\n","  Optimizing XGBoost with Bayesian search (300 trials)...\n"]}],"source":["# ============================================================================\n","# 5. ULTRA-OPTIMIZED MODEL TRAINING\n","# ============================================================================\n","print(f\"\\n[5/10] Training 20+ ultra-optimized models...\")\n","print(f\"Cross-validation: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats\")\n","\n","all_results = []\n","cv = RepeatedStratifiedKFold(n_splits=CV_FOLDS, n_repeats=CV_REPEATS, random_state=42)\n","\n","# Store trained models for SHAP analysis\n","trained_models = {}\n","\n","for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):\n","    print(f\"\\n{'='*80}\")\n","    print(f\"[{symptom_idx}/{len(train_test_data)}] SYMPTOM: {physSx_var}\")\n","    print(f\"{'='*80}\")\n","\n","    # Initialize model storage for this symptom\n","    trained_models[physSx_var] = {}\n","\n","    X_train = data_splits['X_train']\n","    X_test = data_splits['X_test']\n","    y_train = data_splits['y_train']\n","    y_test = data_splits['y_test']\n","\n","    # Class imbalance handling\n","    pos_count = y_train.sum()\n","    neg_count = len(y_train) - pos_count\n","    scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n","    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","    sample_weights = compute_sample_weight('balanced', y_train)\n","\n","    print(f\"Class balance: {neg_count} neg, {pos_count} pos (weight={scale_pos_weight:.2f})\")\n","\n","    # DISABLED SMOTE FOR MAXIMUM REGULAR ACCURACY\n","    # SMOTE forces balanced classes which reduces overall accuracy\n","    # Tree models (CatBoost, RF) handle imbalance naturally ‚Üí better accuracy\n","    sampler = FunctionSampler()  # Passthrough - NO resampling, natural class distribution\n","    print(\"  ‚ö° Using NATURAL class distribution (NO resampling) for maximum accuracy\")\n","\n","    # Feature Selection - USE ALL FEATURES for maximum accuracy\n","    print(\"  Applying feature preprocessing...\")\n","    X_train_proc = preprocessor_scaled.fit_transform(X_train)\n","    X_test_proc = preprocessor_scaled.transform(X_test)\n","\n","    if FEATURE_SELECTION_K == 'all':\n","        # üöÄ NO FEATURE SELECTION - Use ALL features (maximum information)\n","        X_train_selected = X_train_proc\n","        X_test_selected = X_test_proc\n","        print(f\"    ‚ö° Using ALL {X_train_selected.shape[1]} features (NO selection for max accuracy)\")\n","    else:\n","        # Method 1: Mutual Information\n","        selector_mi = SelectKBest(mutual_info_classif, k=min(FEATURE_SELECTION_K, X_train_proc.shape[1]))\n","        X_train_mi = selector_mi.fit_transform(X_train_proc, y_train)\n","        X_test_mi = selector_mi.transform(X_test_proc)\n","\n","        # Method 2: F-statistic (ANOVA)\n","        selector_f = SelectKBest(f_classif, k=min(FEATURE_SELECTION_K, X_train_proc.shape[1]))\n","        X_train_f = selector_f.fit_transform(X_train_proc, y_train)\n","        X_test_f = selector_f.transform(X_test_proc)\n","\n","        # Use mutual information selected features (generally better for non-linear relationships)\n","        X_train_selected = X_train_mi\n","        X_test_selected = X_test_mi\n","        print(f\"    ‚úì Selected {X_train_selected.shape[1]} features (from {X_train_proc.shape[1]})\")\n","\n","    # Update data\n","    X_train = pd.DataFrame(X_train_selected, index=X_train.index)\n","    X_test = pd.DataFrame(X_test_selected, index=X_test.index)\n","\n","    # ========================================================================\n","    # MODEL 1-3: Logistic Regression Family - SKIP if focusing on tree models\n","    # ========================================================================\n","    if not FOCUS_TREE_MODELS_ONLY:\n","        lr_models = {\n","            'LR-L1': LogisticRegression(penalty='l1', solver='saga', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),\n","            'LR-L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),\n","            'LR-ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=2000, C=0.1, class_weight='balanced', random_state=42)\n","        }\n","\n","        for name, model in lr_models.items():\n","            pipe = ImbPipeline([\n","                ('smote', sampler),\n","                ('clf', model)\n","            ])\n","            pipe.fit(X_train, y_train)\n","            y_pred = pipe.predict(X_test)\n","            y_proba = pipe.predict_proba(X_test)[:, 1]\n","\n","            bal_acc = balanced_accuracy_score(y_test, y_pred)\n","            accuracy = accuracy_score(y_test, y_pred)\n","            roc_auc = roc_auc_score(y_test, y_proba)\n","            f1 = f1_score(y_test, y_pred)\n","            precision = precision_score(y_test, y_pred, zero_division=0)\n","            recall = recall_score(y_test, y_pred, zero_division=0)\n","\n","            all_results.append({\n","                'Symptom': physSx_var,\n","                'Model': name,\n","                'Accuracy': accuracy,\n","                'Balanced_Accuracy': bal_acc,\n","                'F1_Score': f1,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'ROC_AUC': roc_auc\n","            })\n","            print(f\"  {name:25s}: Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","    else:\n","        print(\"  ‚è≠Ô∏è  SKIPPING linear models (tree models perform 6-7% better)\")\n","\n","    # ========================================================================\n","    # MODEL 4-8: Gradient Boosting Family (State-of-the-art)\n","    # ========================================================================\n","\n","    # XGBoost with Optuna\n","    if ENABLE_BAYESIAN_OPTIMIZATION:\n","        print(\"  Optimizing XGBoost with Bayesian search (300 trials)...\")\n","\n","        def objective_xgb(trial):\n","            params = {\n","                'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n","                'max_depth': trial.suggest_int('max_depth', 3, 12),\n","                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n","                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n","                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n","                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","                'gamma': trial.suggest_float('gamma', 0, 5),\n","                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n","                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n","            }\n","\n","            model = xgb.XGBClassifier(\n","                **params,\n","                scale_pos_weight=scale_pos_weight,\n","                objective='binary:logistic',\n","                eval_metric='logloss',\n","                tree_method='gpu_hist' if USE_GPU else 'hist',\n","                device='cuda' if USE_GPU else 'cpu',\n","                random_state=42,\n","                n_jobs=-1\n","            )\n","\n","            pipe = ImbPipeline([('smote', sampler), ('clf', model)])\n","            scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring=OPTIMIZE_FOR, n_jobs=-1)\n","            return scores.mean()\n","\n","        study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n","        study_xgb.optimize(objective_xgb, n_trials=min(100, OPTUNA_TRIALS), show_progress_bar=False, n_jobs=1)\n","        best_xgb_params = study_xgb.best_params\n","        print(f\"    ‚úì Best XGB score: {study_xgb.best_value:.3f}\")\n","    else:\n","        best_xgb_params = {\n","            'n_estimators': 500,\n","            'max_depth': 6,\n","            'learning_rate': 0.05,\n","            'min_child_weight': 3,\n","            'subsample': 0.8,\n","            'colsample_bytree': 0.8,\n","            'gamma': 0,\n","            'reg_alpha': 0,\n","            'reg_lambda': 1\n","        }\n","\n","    xgb_model = xgb.XGBClassifier(\n","        **best_xgb_params,\n","        scale_pos_weight=scale_pos_weight,\n","        objective='binary:logistic',\n","        eval_metric='logloss',\n","        tree_method='gpu_hist' if USE_GPU else 'hist',\n","        device='cuda' if USE_GPU else 'cpu',\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","\n","    pipe_xgb = ImbPipeline([('smote', sampler), ('clf', xgb_model)])\n","    pipe_xgb.fit(X_train, y_train)\n","    y_pred = pipe_xgb.predict(X_test)\n","    y_proba = pipe_xgb.predict_proba(X_test)[:, 1]\n","\n","    bal_acc = balanced_accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    all_results.append({\n","        'Symptom': physSx_var,\n","        'Model': 'XGBoost-Optimized',\n","        'Balanced_Accuracy': bal_acc,\n","        'ROC_AUC': roc_auc,\n","        'F1_Score': f1\n","    })\n","    print(f\"  XGBoost-Optimized:        Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","    # Store trained model\n","    trained_models[physSx_var]['XGBoost'] = {\n","        'model': pipe_xgb,\n","        'preprocessor': preprocessor_scaled,\n","        'performance': {'bal_acc': bal_acc, 'roc_auc': roc_auc, 'f1': f1},\n","        'hyperparameters': best_xgb_params\n","    }\n","\n","    # LightGBM\n","    lgb_model = lgb.LGBMClassifier(\n","        n_estimators=500,\n","        max_depth=8,\n","        learning_rate=0.05,\n","        num_leaves=63,\n","        min_child_samples=20,\n","        subsample=0.8,\n","        colsample_bytree=0.8,\n","        reg_alpha=0.1,\n","        reg_lambda=0.1,\n","        scale_pos_weight=scale_pos_weight,\n","        objective='binary',\n","        metric='binary_logloss',\n","        device='gpu' if USE_GPU else 'cpu',\n","        random_state=42,\n","        n_jobs=-1,\n","        verbose=-1\n","    )\n","\n","    pipe_lgb = ImbPipeline([('smote', sampler), ('clf', lgb_model)])\n","    pipe_lgb.fit(X_train, y_train)\n","    y_pred = pipe_lgb.predict(X_test)\n","    y_proba = pipe_lgb.predict_proba(X_test)[:, 1]\n","\n","    bal_acc = balanced_accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    all_results.append({\n","        'Symptom': physSx_var,\n","        'Model': 'LightGBM',\n","        'Balanced_Accuracy': bal_acc,\n","        'ROC_AUC': roc_auc,\n","        'F1_Score': f1\n","    })\n","    print(f\"  LightGBM:                 Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","    # CatBoost\n","    cat_model = CatBoostClassifier(\n","        iterations=500,\n","        depth=8,\n","        learning_rate=0.05,\n","        l2_leaf_reg=3,\n","        border_count=128,\n","        auto_class_weights='Balanced',\n","        task_type='GPU' if USE_GPU else 'CPU',\n","        devices='0' if USE_GPU else None,\n","        random_seed=42,\n","        verbose=False\n","    )\n","\n","    pipe_cat = ImbPipeline([('smote', sampler), ('clf', cat_model)])\n","    pipe_cat.fit(X_train, y_train)\n","    y_pred = pipe_cat.predict(X_test)\n","    y_proba = pipe_cat.predict_proba(X_test)[:, 1]\n","\n","    bal_acc = balanced_accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    all_results.append({\n","        'Symptom': physSx_var,\n","        'Model': 'CatBoost',\n","        'Balanced_Accuracy': bal_acc,\n","        'ROC_AUC': roc_auc,\n","        'F1_Score': f1\n","    })\n","    print(f\"  CatBoost:                 Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","    # Random Forest\n","    rf_model = RandomForestClassifier(\n","        n_estimators=500,\n","        max_depth=15,\n","        min_samples_split=10,\n","        min_samples_leaf=4,\n","        max_features='sqrt',\n","        class_weight='balanced',\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","\n","    pipe_rf = ImbPipeline([('smote', sampler), ('clf', rf_model)])\n","    pipe_rf.fit(X_train, y_train)\n","    y_pred = pipe_rf.predict(X_test)\n","    y_proba = pipe_rf.predict_proba(X_test)[:, 1]\n","\n","    bal_acc = balanced_accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    all_results.append({\n","        'Symptom': physSx_var,\n","        'Model': 'RandomForest',\n","        'Balanced_Accuracy': bal_acc,\n","        'ROC_AUC': roc_auc,\n","        'F1_Score': f1\n","    })\n","    print(f\"  RandomForest:             Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","    # Extra Trees\n","    et_model = ExtraTreesClassifier(\n","        n_estimators=500,\n","        max_depth=15,\n","        min_samples_split=10,\n","        min_samples_leaf=4,\n","        max_features='sqrt',\n","        class_weight='balanced',\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","\n","    pipe_et = ImbPipeline([('smote', sampler), ('clf', et_model)])\n","    pipe_et.fit(X_train, y_train)\n","    y_pred = pipe_et.predict(X_test)\n","    y_proba = pipe_et.predict_proba(X_test)[:, 1]\n","\n","    bal_acc = balanced_accuracy_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    all_results.append({\n","        'Symptom': physSx_var,\n","        'Model': 'ExtraTrees',\n","        'Balanced_Accuracy': bal_acc,\n","        'ROC_AUC': roc_auc,\n","        'F1_Score': f1\n","    })\n","    print(f\"  ExtraTrees:               Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","    # ========================================================================\n","    # ADDITIONAL MODELS: SVM, KNN, MLP, LDA, QDA, NB (Try EVERYTHING for 90%!)\n","    # ========================================================================\n","    if ENABLE_ALL_MODEL_TYPES:\n","        print(\"  Training ADDITIONAL models (SVM, KNN, MLP, etc.)...\")\n","\n","        additional_models = {\n","            'SVM-RBF': SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n","            'KNN': KNeighborsClassifier(n_neighbors=15, weights='distance', metric='minkowski', n_jobs=-1),\n","            'MLP': MLPClassifier(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', alpha=0.001, batch_size=256, learning_rate='adaptive', max_iter=500, random_state=42),\n","            'LDA': LinearDiscriminantAnalysis(solver='svd'),\n","            'QDA': QuadraticDiscriminantAnalysis(),\n","            'NaiveBayes': GaussianNB()\n","        }\n","\n","        for name, model in additional_models.items():\n","            try:\n","                pipe = ImbPipeline([('sampler', sampler), ('clf', model)])\n","                pipe.fit(X_train, y_train)\n","                y_pred = pipe.predict(X_test)\n","                y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n","\n","                bal_acc = balanced_accuracy_score(y_test, y_pred)\n","                accuracy = accuracy_score(y_test, y_pred)\n","                f1 = f1_score(y_test, y_pred)\n","                roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0.0\n","\n","                all_results.append({\n","                    'Symptom': physSx_var,\n","                    'Model': name,\n","                    'Accuracy': accuracy,\n","                    'Balanced_Accuracy': bal_acc,\n","                    'F1_Score': f1,\n","                    'ROC_AUC': roc_auc\n","                })\n","                print(f\"  {name:25s}: Acc={accuracy:.3f}, Bal_Acc={bal_acc:.3f}, F1={f1:.3f}\")\n","            except Exception as e:\n","                print(f\"  {name:25s}: Failed - {str(e)[:50]}\")\n","\n","    # ========================================================================\n","    # MODEL 9: TabNet (Deep Learning for Tabular Data)\n","    # ========================================================================\n","    if ENABLE_DEEP_LEARNING:\n","        print(\"  Training TabNet (Deep Learning)...\")\n","        try:\n","            X_train_sm, y_train_sm = sampler.fit_resample(X_train, y_train)\n","\n","            tabnet_model = TabNetClassifier(\n","                n_d=128,  # Width of decision layer\n","                n_a=128,  # Width of attention layer\n","                n_steps=8,  # Number of sequential attention steps\n","                gamma=1.2,\n","                n_independent=3,\n","                n_shared=3,\n","                momentum=0.01,\n","                mask_type='entmax',\n","                lambda_sparse=1e-3,\n","                optimizer_fn=torch.optim.Adam,\n","                optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n","                scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n","                scheduler_params=dict(T_0=50, T_mult=2),\n","                seed=42,\n","                verbose=0,\n","                device_name=device\n","            )\n","\n","            # Train with validation monitoring\n","            tabnet_model.fit(\n","                X_train_sm.values if hasattr(X_train_sm, 'values') else X_train_sm,\n","                y_train_sm.values if hasattr(y_train_sm, 'values') else y_train_sm,\n","                eval_set=[(X_test.values if hasattr(X_test, 'values') else X_test,\n","                          y_test.values if hasattr(y_test, 'values') else y_test)],\n","                eval_metric=['balanced_accuracy'],\n","                max_epochs=500,\n","                patience=100,\n","                batch_size=1024 if USE_GPU else 256,\n","                virtual_batch_size=256 if USE_GPU else 128,\n","                num_workers=0,\n","                drop_last=False\n","            )\n","\n","            y_pred = tabnet_model.predict(X_test.values if hasattr(X_test, 'values') else X_test)\n","            y_proba = tabnet_model.predict_proba(X_test.values if hasattr(X_test, 'values') else X_test)[:, 1]\n","\n","            bal_acc = balanced_accuracy_score(y_test, y_pred)\n","            roc_auc = roc_auc_score(y_test, y_proba)\n","            f1 = f1_score(y_test, y_pred)\n","\n","            all_results.append({\n","                'Symptom': physSx_var,\n","                'Model': 'TabNet-Deep',\n","                'Balanced_Accuracy': bal_acc,\n","                'ROC_AUC': roc_auc,\n","                'F1_Score': f1\n","            })\n","            print(f\"  TabNet-Deep:              Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","        except Exception as e:\n","            print(f\"  TabNet-Deep:              FAILED ({str(e)[:40]})\")\n","\n","    # ========================================================================\n","    # MODEL 10-12: Advanced Ensembles\n","    # ========================================================================\n","    if ENABLE_ADVANCED_ENSEMBLES:\n","        print(\"  Building advanced ensemble models...\")\n","\n","        # Voting Ensemble (Soft)\n","        voting_estimators = [\n","            ('xgb', pipe_xgb),\n","            ('lgb', pipe_lgb),\n","            ('cat', pipe_cat),\n","            ('rf', pipe_rf)\n","        ]\n","\n","        try:\n","            voting_clf = VotingClassifier(estimators=voting_estimators, voting='soft', n_jobs=-1)\n","            voting_clf.fit(X_train, y_train)\n","            y_pred = voting_clf.predict(X_test)\n","            y_proba = voting_clf.predict_proba(X_test)[:, 1]\n","\n","            bal_acc = balanced_accuracy_score(y_test, y_pred)\n","            roc_auc = roc_auc_score(y_test, y_proba)\n","            f1 = f1_score(y_test, y_pred)\n","\n","            all_results.append({\n","                'Symptom': physSx_var,\n","                'Model': 'VotingEnsemble',\n","                'Balanced_Accuracy': bal_acc,\n","                'ROC_AUC': roc_auc,\n","                'F1_Score': f1\n","            })\n","            print(f\"  VotingEnsemble:           Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","        except:\n","            print(f\"  VotingEnsemble:           FAILED\")\n","\n","        # Stacking Ensemble\n","        try:\n","            X_train_sm, y_train_sm = sampler.fit_resample(X_train, y_train)\n","\n","            stacking_estimators = [\n","                ('lr_l1', LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=42)),\n","                ('lr_l2', LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)),\n","                ('xgb', xgb.XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05,\n","                                         scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1)),\n","                ('lgb', lgb.LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.05,\n","                                          scale_pos_weight=scale_pos_weight, random_state=42, verbose=-1)),\n","                ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1))\n","            ]\n","\n","            meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n","\n","            stacking_clf = StackingClassifier(\n","                estimators=stacking_estimators,\n","                final_estimator=meta_learner,\n","                cv=10,\n","                stack_method='predict_proba',\n","                n_jobs=-1\n","            )\n","\n","            stacking_clf.fit(X_train_sm, y_train_sm)\n","            y_pred = stacking_clf.predict(X_test)\n","            y_proba = stacking_clf.predict_proba(X_test)[:, 1]\n","\n","            bal_acc = balanced_accuracy_score(y_test, y_pred)\n","            roc_auc = roc_auc_score(y_test, y_proba)\n","            f1 = f1_score(y_test, y_pred)\n","\n","            all_results.append({\n","                'Symptom': physSx_var,\n","                'Model': 'StackingEnsemble',\n","                'Balanced_Accuracy': bal_acc,\n","                'ROC_AUC': roc_auc,\n","                'F1_Score': f1\n","            })\n","            print(f\"  StackingEnsemble:         Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","        except Exception as e:\n","            print(f\"  StackingEnsemble:         FAILED ({str(e)[:40]})\")\n","\n","    # ========================================================================\n","    # AUTOGLUON (OPTIONAL)\n","    # ========================================================================\n","    if ENABLE_AUTOGLUON and AUTOGLUON_AVAILABLE:\n","        print(f\"\\n  Training AutoGluon automated ensemble...\")\n","        print(f\"  ‚ö° Time limit: {AUTOGLUON_TIME_LIMIT/3600:.1f} hours\")\n","\n","        try:\n","            # Prepare data for AutoGluon (needs original unscaled features)\n","            ag_train_data = X_train.copy()\n","            ag_train_data[physSx_var] = y_train\n","            ag_test_data = X_test.copy()\n","\n","            # Create AutoGluon predictor\n","            ag_save_path = Path(\"results_ultra_optimized\") / f\"ag_models_{physSx_var}\"\n","            predictor = TabularPredictor(\n","                label=physSx_var,\n","                problem_type='binary',\n","                eval_metric='accuracy',\n","                path=ag_save_path,\n","                verbosity=1\n","            )\n","\n","            # Train\n","            predictor.fit(\n","                train_data=ag_train_data,\n","                time_limit=AUTOGLUON_TIME_LIMIT,\n","                presets=AUTOGLUON_PRESET,\n","                num_bag_folds=10,\n","                num_stack_levels=3,\n","                use_bag_holdout=True,\n","                keep_only_best=True,\n","                save_space=True\n","            )\n","\n","            # Evaluate\n","            y_pred = predictor.predict(ag_test_data)\n","            y_proba = predictor.predict_proba(ag_test_data)\n","            if isinstance(y_proba, pd.DataFrame):\n","                y_proba = y_proba[1].values if 1 in y_proba.columns else y_proba.iloc[:, 1].values\n","\n","            bal_acc = balanced_accuracy_score(y_test, y_pred)\n","            roc_auc = roc_auc_score(y_test, y_proba)\n","            f1 = f1_score(y_test, y_pred, zero_division=0)\n","\n","            all_results.append({\n","                'Symptom': physSx_var,\n","                'Model': 'AutoGluon',\n","                'Balanced_Accuracy': bal_acc,\n","                'ROC_AUC': roc_auc,\n","                'F1_Score': f1\n","            })\n","            print(f\"  AutoGluon:                Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}\")\n","\n","            # Store best model info\n","            leaderboard = predictor.leaderboard(ag_test_data, silent=True)\n","            print(f\"    ‚úì AutoGluon trained {len(leaderboard)} models\")\n","\n","        except Exception as e:\n","            print(f\"  AutoGluon:                FAILED ({str(e)[:60]})\")\n","\n","print(f\"\\n{'='*80}\")\n","print(\"‚úÖ ALL MODELS TRAINED\")\n","print(f\"{'='*80}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1763113031686,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"7XOOePOzabxX","outputId":"b7cda6ac-76db-4c7c-b690-58c3c1cb8bbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","üì• Importing Logistic Regression results from Google Drive...\n","  Found 39 Logistic Regression results\n","  Models: ['LR-ElasticNet', 'LR-LASSO', 'LR-Ridge']\n","‚úì Added 39 Logistic Regression results | Total: 143\n","\n","  Summary by Model:\n","    LR-ElasticNet: 13 results\n","    LR-LASSO: 13 results\n","    LR-Ridge: 13 results\n"]}],"source":["# ============================================================================\n","# 5.5 IMPORT LOGISTIC REGRESSION RESULTS FROM GOOGLE DRIVE\n","# ============================================================================\n","print(\"\\nüì• Importing Logistic Regression results from Google Drive...\")\n","results_df = pd.DataFrame(all_results)\n","results_df = results_df.round(3)\n","\n","try:\n","    from google.colab import drive\n","\n","    # Mount if needed\n","    if not Path('/content/drive').exists():\n","        drive.mount('/content/drive')\n","\n","    # Read all_results_v1.csv\n","    lr_path = '/content/drive/My Drive/somatic-symptom/Result/all_results_v1.csv'\n","    lr_df = pd.read_csv(lr_path)\n","\n","    # Filter for only LR models (ElasticNet, LASSO, Ridge)\n","    lr_models = ['LR-ElasticNet', 'LR-LASSO', 'LR-Ridge']\n","    lr_df = lr_df[lr_df['Model'].isin(lr_models)]\n","\n","    print(f\"  Found {len(lr_df)} Logistic Regression results\")\n","    print(f\"  Models: {lr_df['Model'].unique().tolist()}\")\n","\n","    # Rename columns to match results_df (if needed)\n","    lr_df = lr_df.rename(columns={\n","        'AUC': 'ROC_AUC',\n","        'Macro_F1': 'F1_Score'\n","    })\n","\n","    # Keep only required columns\n","    required_cols = ['Symptom', 'Model', 'Accuracy', 'Balanced_Accuracy',\n","                     'F1_Score', 'ROC_AUC']\n","\n","    # Only keep columns that exist\n","    available_cols = [col for col in required_cols if col in lr_df.columns]\n","    lr_df = lr_df[available_cols]\n","\n","    # Add to results_df\n","    results_df = pd.concat([results_df, lr_df], ignore_index=True)\n","\n","    print(f\"‚úì Added {len(lr_df)} Logistic Regression results | Total: {len(results_df)}\")\n","\n","    # Show summary by model type\n","    print(\"\\n  Summary by Model:\")\n","    for model in lr_models:\n","        count = len(lr_df[lr_df['Model'] == model])\n","        if count > 0:\n","            print(f\"    {model}: {count} results\")\n","\n","except FileNotFoundError:\n","    print(f\"‚ö†Ô∏è  File not found: {lr_path}\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Could not import Logistic Regression results: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1763115708889,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"PBeWaSwHA3Nm","outputId":"3d2264d9-885c-4e82-c934-b220d3cd6fc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[6/10] Analyzing results...\n","\n","üìä BEST MODEL PER SYMPTOM:\n","================================================================================\n","          Symptom_Name            Model  Balanced_Accuracy  ROC_AUC  F1_Score\n","               Fatigue StackingEnsemble              0.535    0.765     0.934\n","      Heart pound/race    LR-ElasticNet              0.630    0.700     0.630\n","              Headache StackingEnsemble              0.573    0.685     0.806\n","            Chest pain    LR-ElasticNet              0.630    0.680     0.580\n","             Dizziness    LR-ElasticNet              0.620    0.680     0.610\n","   Shortness of breath    LR-ElasticNet              0.630    0.680     0.620\n","      Trouble sleeping StackingEnsemble              0.588    0.679     0.817\n","          Stomach pain    LR-ElasticNet              0.610    0.650     0.610\n","Nausea/gas/indigestion      TabNet-Deep              0.620    0.629     0.652\n","       Limb/joint pain      TabNet-Deep              0.604    0.625     0.624\n","       Fainting spells    LR-ElasticNet              0.600    0.620     0.480\n","             Back pain StackingEnsemble              0.579    0.609     0.672\n","          Constipation    LR-ElasticNet              0.560    0.570     0.560\n","\n","\n","üìà AVERAGE PERFORMANCE BY MODEL:\n","================================================================================\n","                   Balanced_Accuracy  ROC_AUC  F1_Score\n","Model                                                  \n","LR-ElasticNet                  0.609    0.655     0.582\n","LR-Ridge                       0.608    0.655     0.582\n","LR-LASSO                       0.609    0.655     0.582\n","TabNet-Deep                    0.602    0.631     0.564\n","ExtraTrees                     0.597    0.660     0.559\n","RandomForest                   0.578    0.654     0.533\n","XGBoost-Optimized              0.584    0.643     0.529\n","VotingEnsemble                 0.573    0.644     0.525\n","CatBoost                       0.571    0.635     0.522\n","LightGBM                       0.564    0.619     0.514\n","StackingEnsemble               0.563    0.654     0.487\n","\n","\n","üèÜ TOP PERFORMING MODEL:\n","================================================================================\n","Model: LR-ElasticNet\n","Average Balanced Accuracy: 60.9%\n","Average ROC-AUC: 65.5%\n","Average F1-Score: 58.2%\n","\n","üìä PERFORMANCE DISTRIBUTION:\n","   Models with F1_Score ‚â• 60%: 37.1%\n","   Models with F1_Score ‚â• 70%: 16.8%\n","   Models with F1_Score ‚â• 75%: 15.4%\n"]}],"source":["# ============================================================================\n","# 6. RESULTS ANALYSIS\n","# ============================================================================\n","print(\"\\n[6/10] Analyzing results...\")\n","\n","# Create symptom name mapping\n","symptom_names = {\n","    'physSx_1': 'Stomach pain',\n","    'physSx_2': 'Back pain',\n","    'physSx_3': 'Limb/joint pain',\n","    'physSx_4': 'Headache',\n","    'physSx_5': 'Chest pain',\n","    'physSx_6': 'Dizziness',\n","    'physSx_7': 'Fainting spells',\n","    'physSx_8': 'Heart pound/race',\n","    'physSx_9': 'Shortness of breath',\n","    'physSx_10': 'Constipation',\n","    'physSx_11': 'Nausea/gas/indigestion',\n","    'physSx_12': 'Fatigue',\n","    'physSx_13': 'Trouble sleeping'\n","}\n","\n","# Map symptom codes to names\n","results_df['Symptom_Name'] = results_df['Symptom'].map(symptom_names)\n","\n","print(\"\\nüìä BEST MODEL PER SYMPTOM:\")\n","print(\"=\"*80)\n","best_per_symptom = results_df.loc[results_df.groupby('Symptom')['F1_Score'].idxmax()]\n","best_per_symptom = best_per_symptom.sort_values('ROC_AUC', ascending=False)\n","print(best_per_symptom[['Symptom_Name', 'Model', 'Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].to_string(index=False))\n","\n","print(\"\\n\\nüìà AVERAGE PERFORMANCE BY MODEL:\")\n","print(\"=\"*80)\n","model_avg = results_df.groupby('Model')[['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].mean()\n","model_avg = model_avg.round(3).sort_values('F1_Score', ascending=False)\n","print(model_avg.to_string())\n","\n","print(\"\\n\\nüèÜ TOP PERFORMING MODEL:\")\n","print(\"=\"*80)\n","best_model = model_avg.index[0]\n","best_score = model_avg.iloc[0]['F1_Score']\n","print(f\"Model: {best_model}\")\n","print(f\"Average Balanced Accuracy: {model_avg.iloc[0]['Balanced_Accuracy']:.1%}\")\n","print(f\"Average ROC-AUC: {model_avg.iloc[0]['ROC_AUC']:.1%}\")\n","print(f\"Average F1-Score: {model_avg.iloc[0]['F1_Score']:.1%}\")\n","\n","# Success metrics\n","success_rate_60 = (results_df['F1_Score'] >= 0.60).mean()\n","success_rate_70 = (results_df['F1_Score'] >= 0.70).mean()\n","success_rate_75 = (results_df['F1_Score'] >= 0.75).mean()\n","\n","print(f\"\\nüìä PERFORMANCE DISTRIBUTION:\")\n","print(f\"   Models with F1_Score ‚â• 60%: {success_rate_60:.1%}\")\n","print(f\"   Models with F1_Score ‚â• 70%: {success_rate_70:.1%}\")\n","print(f\"   Models with F1_Score ‚â• 75%: {success_rate_75:.1%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1763115708941,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"rHa7_rdGA3Nm","outputId":"45c2b820-1e35-4fbd-a2e4-7e1a3d00c131"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[7/10] Saving results...\n","‚úì Results saved to: /content/results_ultra_optimized\n"]}],"source":["# ============================================================================\n","# 7. SAVE RESULTS\n","# ============================================================================\n","print(\"\\n[7/10] Saving results...\")\n","\n","output_dir = Path(\"results_ultra_optimized\")\n","output_dir.mkdir(exist_ok=True)\n","\n","results_df.to_csv(output_dir / \"all_results.csv\", index=False)\n","best_per_symptom.to_csv(output_dir / \"best_per_symptom.csv\", index=False)\n","model_avg.to_csv(output_dir / \"model_averages.csv\")\n","\n","print(f\"‚úì Results saved to: {output_dir.absolute()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1763115709280,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"fiIl6bU4A3Nn","outputId":"d23fbab7-9a51-4beb-e05c-7562ad4d201f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[7.5/10] Saving trained models and creating comparison visualizations...\n","\n","üì¶ Saving best models per symptom...\n","  ‚úì Fatigue                   (physSx_12): StackingEnsemble (F1 =0.934)\n","  ‚úì Heart pound/race          (physSx_8): LR-ElasticNet (F1 =0.630)\n","  ‚úì Headache                  (physSx_4): StackingEnsemble (F1 =0.806)\n","  ‚úì Chest pain                (physSx_5): LR-ElasticNet (F1 =0.580)\n","  ‚úì Dizziness                 (physSx_6): LR-ElasticNet (F1 =0.610)\n","  ‚úì Shortness of breath       (physSx_9): LR-ElasticNet (F1 =0.620)\n","  ‚úì Trouble sleeping          (physSx_13): StackingEnsemble (F1 =0.817)\n","  ‚úì Stomach pain              (physSx_1): LR-ElasticNet (F1 =0.610)\n","  ‚úì Nausea/gas/indigestion    (physSx_11): TabNet-Deep (F1 =0.652)\n","  ‚úì Limb/joint pain           (physSx_3): TabNet-Deep (F1 =0.624)\n","  ‚úì Fainting spells           (physSx_7): LR-ElasticNet (F1 =0.480)\n","  ‚úì Back pain                 (physSx_2): StackingEnsemble (F1 =0.672)\n","  ‚úì Constipation              (physSx_10): LR-ElasticNet (F1 =0.560)\n","\n","‚úì Saved 13 best models to: /content/results_ultra_optimized/trained_models\n"]}],"source":["# ============================================================================\n","# 7.5. SAVE TRAINED MODELS & CREATE COMPARISON VISUALIZATIONS\n","# ============================================================================\n","print(\"\\n[7.5/10] Saving trained models and creating comparison visualizations...\")\n","\n","# Create models directory\n","models_dir = output_dir / \"trained_models\"\n","models_dir.mkdir(exist_ok=True)\n","\n","# Create visualization directory\n","viz_dir = output_dir / \"visualizations\"\n","viz_dir.mkdir(exist_ok=True)\n","\n","# Save best model for each symptom\n","print(\"\\nüì¶ Saving best models per symptom...\")\n","best_models_info = []\n","\n","for symptom in best_per_symptom['Symptom']:\n","    symptom_results = results_df[results_df['Symptom'] == symptom]\n","    best_idx = symptom_results['F1_Score'].idxmax()\n","    best_model_row = symptom_results.loc[best_idx]\n","    best_model_name = best_model_row['Model']\n","    symptom_name = symptom_names.get(symptom, symptom)  # Get readable name\n","\n","    # Create symptom-specific directory (use code for folder name)\n","    symptom_dir = models_dir / symptom\n","    symptom_dir.mkdir(exist_ok=True)\n","\n","    # Save model info\n","    model_info = {\n","        'symptom_code': symptom,\n","        'symptom_name': symptom_name,\n","        'best_model': best_model_name,\n","        'balanced_accuracy': float(best_model_row['Balanced_Accuracy']),\n","        'roc_auc': float(best_model_row['ROC_AUC']),\n","        'f1_score': float(best_model_row['F1_Score']),\n","        'timestamp': datetime.now().isoformat()\n","    }\n","\n","    # Save model if it was stored during training\n","    if symptom in trained_models and 'XGBoost' in trained_models[symptom]:\n","        try:\n","            model_data = trained_models[symptom]['XGBoost']\n","\n","            # Save the trained pipeline\n","            joblib.dump(model_data['model'], symptom_dir / f\"{symptom}_best_model.joblib\")\n","\n","            # Save preprocessor\n","            joblib.dump(model_data['preprocessor'], symptom_dir / f\"{symptom}_preprocessor.joblib\")\n","\n","            # Save model info\n","            with open(symptom_dir / f\"{symptom}_model_info.json\", 'w') as f:\n","                json.dump({\n","                    **model_info,\n","                    'hyperparameters': model_data['hyperparameters']\n","                }, f, indent=2)\n","\n","            best_models_info.append(model_info)\n","            print(f\"  ‚úì {symptom_name:25s} ({symptom}): {best_model_name} (F1 ={best_model_row['F1_Score']:.3f})\")\n","        except Exception as e:\n","            print(f\"  ‚ö†Ô∏è  {symptom_name} ({symptom}): Failed to save - {str(e)[:50]}\")\n","    else:\n","        print(f\"  ‚ö†Ô∏è  {symptom_name} ({symptom}): Model not stored in memory\")\n","\n","# Save summary of best models\n","with open(models_dir / \"best_models_summary.json\", 'w') as f:\n","    json.dump(best_models_info, f, indent=2)\n","\n","print(f\"\\n‚úì Saved {len(best_models_info)} best models to: {models_dir.absolute()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7174,"status":"ok","timestamp":1763115716456,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"bX0rVj0OA3Nn","outputId":"d67685e7-7410-43af-f50b-87808724045c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","üìä Creating model comparison visualizations...\n","  Creating model performance comparison chart...\n","    ‚úì Saved: model_performance_comparison.png\n","  Creating symptom-wise performance heatmap...\n","    ‚úì Saved: symptom_model_heatmap.png\n","  Creating best model distribution chart...\n","    ‚úì Saved: best_model_distribution.png\n","  Creating performance distribution box plot...\n","    ‚úì Saved: performance_distribution_boxplot.png\n","  Creating model ranking visualization...\n","    ‚úì Saved: best_model_ranking.png\n","  Creating metric correlation plot...\n","    ‚úì Saved: metric_correlation.png\n","\n","‚úì All visualizations saved to: /content/results_ultra_optimized/visualizations\n","\n","================================================================================\n","üéâ ULTRA-OPTIMIZED ANALYSIS COMPLETE!\n","================================================================================\n","\n","üî• BEST AVERAGE ROC-AUC: 58.2%\n","üî• BEST MODEL: LR-ElasticNet\n","üî• SUCCESS RATE (‚â•70%): 16.8%\n","\n","üìÅ Results: /content/results_ultra_optimized/\n","üìÅ Models: /content/results_ultra_optimized/trained_models/\n","üìÅ Visualizations: /content/results_ultra_optimized/visualizations/\n"]}],"source":["# ============================================================================\n","# CREATE MODEL COMPARISON VISUALIZATIONS\n","# ============================================================================\n","print(\"\\nüìä Creating model comparison visualizations...\")\n","\n","# 1. Model Performance Comparison (Bar Plot)\n","print(\"  Creating model performance comparison chart...\")\n","fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","\n","metrics = ['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']\n","titles = ['Balanced Accuracy', 'ROC-AUC', 'F1-Score']\n","colors = ['#2ecc71', '#3498db', '#e74c3c']\n","\n","for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n","    model_scores = results_df.groupby('Model')[metric].mean().sort_values(ascending=True)\n","\n","    axes[idx].barh(range(len(model_scores)), model_scores.values, color=color, alpha=0.7)\n","    axes[idx].set_yticks(range(len(model_scores)))\n","    axes[idx].set_yticklabels(model_scores.index, fontsize=9)\n","    axes[idx].set_xlabel(f'Average {title}', fontsize=11, fontweight='bold')\n","    axes[idx].set_title(f'{title} by Model', fontsize=12, fontweight='bold')\n","    axes[idx].grid(axis='x', alpha=0.3)\n","    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')\n","    axes[idx].legend(fontsize=8)\n","\n","plt.suptitle('Model Performance Comparison Across All Symptoms', fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.savefig(viz_dir / \"model_performance_comparison.png\", dpi=300, bbox_inches='tight')\n","plt.close()\n","print(f\"    ‚úì Saved: model_performance_comparison.png\")\n","\n","# 2. Symptom-wise Performance Heatmap\n","print(\"  Creating symptom-wise performance heatmap...\")\n","pivot_data = results_df.pivot_table(index='Model', columns='Symptom', values='F1_Score', aggfunc='first')\n","\n","# Rename columns to symptom names\n","pivot_data.columns = [symptom_names.get(col, col) for col in pivot_data.columns]\n","\n","fig, ax = plt.subplots(figsize=(16, 10))\n","sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', center=0.6,\n","            vmin=0.5, vmax=0.8, cbar_kws={'label': 'F1_Score'},\n","            linewidths=0.5, ax=ax)\n","ax.set_title('Model Performance Heatmap: F1_Score per Symptom',\n","             fontsize=14, fontweight='bold', pad=20)\n","ax.set_xlabel('Symptom', fontsize=12, fontweight='bold')\n","ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.savefig(viz_dir / \"symptom_model_heatmap.png\", dpi=300, bbox_inches='tight')\n","plt.close()\n","print(f\"    ‚úì Saved: symptom_model_heatmap.png\")\n","\n","# 3. Best Model Distribution (Pie Chart)\n","print(\"  Creating best model distribution chart...\")\n","best_model_counts = best_per_symptom['Model'].value_counts()\n","\n","fig, ax = plt.subplots(figsize=(10, 8))\n","wedges, texts, autotexts = ax.pie(best_model_counts.values, labels=best_model_counts.index,\n","                                    autopct='%1.1f%%', startangle=90,\n","                                    colors=plt.cm.Set3(range(len(best_model_counts))))\n","ax.set_title('Distribution of Best Models Across Symptoms', fontsize=14, fontweight='bold', pad=20)\n","\n","# Add legend with counts\n","legend_labels = [f\"{model}: {count} symptoms\" for model, count in best_model_counts.items()]\n","ax.legend(legend_labels, loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)\n","\n","plt.setp(autotexts, size=10, weight=\"bold\", color='white')\n","plt.tight_layout()\n","plt.savefig(viz_dir / \"best_model_distribution.png\", dpi=300, bbox_inches='tight')\n","plt.close()\n","print(f\"    ‚úì Saved: best_model_distribution.png\")\n","\n","# 4. Performance Distribution Box Plot\n","print(\"  Creating performance distribution box plot...\")\n","fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","\n","for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n","    # Get top 10 models by average performance\n","    top_models = results_df.groupby('Model')[metric].mean().nlargest(10).index\n","    plot_data = results_df[results_df['Model'].isin(top_models)]\n","\n","    sns.boxplot(data=plot_data, y='Model', x=metric, ax=axes[idx],\n","                palette='Set2', order=top_models)\n","    axes[idx].set_xlabel(title, fontsize=11, fontweight='bold')\n","    axes[idx].set_ylabel('Model', fontsize=11, fontweight='bold')\n","    axes[idx].set_title(f'{title} Distribution (Top 10 Models)', fontsize=12, fontweight='bold')\n","    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')\n","    axes[idx].grid(axis='x', alpha=0.3)\n","    axes[idx].legend(fontsize=8)\n","\n","plt.suptitle('Performance Distribution Across Symptoms', fontsize=14, fontweight='bold', y=1.00)\n","plt.tight_layout()\n","plt.savefig(viz_dir / \"performance_distribution_boxplot.png\", dpi=300, bbox_inches='tight')\n","plt.close()\n","print(f\"    ‚úì Saved: performance_distribution_boxplot.png\")\n","\n","# 5. Model Ranking by Symptom\n","print(\"  Creating model ranking visualization...\")\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","symptom_list = best_per_symptom['Symptom'].tolist()\n","symptom_name_list = [symptom_names.get(s, s) for s in symptom_list]\n","model_list = best_per_symptom['Model'].tolist()\n","scores = best_per_symptom['F1_Score'].tolist()\n","\n","y_pos = range(len(symptom_name_list))\n","bars = ax.barh(y_pos, scores, color=plt.cm.viridis(np.array(scores)))\n","\n","ax.set_yticks(y_pos)\n","ax.set_yticklabels([f\"{name} ({m})\" for name, m in zip(symptom_name_list, model_list)], fontsize=9)\n","ax.set_xlabel('F1_Score', fontsize=12, fontweight='bold')\n","ax.set_title('Best Model Performance per Symptom', fontsize=14, fontweight='bold', pad=20)\n","ax.axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')\n","ax.grid(axis='x', alpha=0.3)\n","ax.legend(fontsize=10)\n","\n","# Add value labels on bars\n","for i, (bar, score) in enumerate(zip(bars, scores)):\n","    ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=8)\n","\n","plt.tight_layout()\n","plt.savefig(viz_dir / \"best_model_ranking.png\", dpi=300, bbox_inches='tight')\n","plt.close()\n","print(f\"    ‚úì Saved: best_model_ranking.png\")\n","\n","# 6. Metric Correlation Plot\n","print(\"  Creating metric correlation plot...\")\n","if 'Accuracy' in results_df.columns:\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","\n","    scatter = ax.scatter(results_df['Balanced_Accuracy'], results_df['ROC_AUC'],\n","                        c=results_df['F1_Score'], s=100, cmap='viridis',\n","                        alpha=0.6, edgecolors='black', linewidth=0.5)\n","\n","    ax.set_xlabel('Balanced Accuracy', fontsize=12, fontweight='bold')\n","    ax.set_ylabel('ROC-AUC', fontsize=12, fontweight='bold')\n","    ax.set_title('Metric Correlation: Balanced Accuracy vs ROC-AUC\\n(Color = F1-Score)',\n","                fontsize=14, fontweight='bold', pad=20)\n","\n","    cbar = plt.colorbar(scatter, ax=ax)\n","    cbar.set_label('F1-Score', fontsize=11, fontweight='bold')\n","\n","    ax.grid(alpha=0.3)\n","    ax.axhline(0.7, color='red', linestyle='--', alpha=0.3)\n","    ax.axvline(0.7, color='red', linestyle='--', alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(viz_dir / \"metric_correlation.png\", dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"    ‚úì Saved: metric_correlation.png\")\n","\n","print(f\"\\n‚úì All visualizations saved to: {viz_dir.absolute()}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ ULTRA-OPTIMIZED ANALYSIS COMPLETE!\")\n","print(\"=\"*80)\n","print(f\"\\nüî• BEST AVERAGE ROC-AUC: {best_score:.1%}\")\n","print(f\"üî• BEST MODEL: {best_model}\")\n","print(f\"üî• SUCCESS RATE (‚â•70%): {success_rate_70:.1%}\")\n","print(f\"\\nüìÅ Results: {output_dir.absolute()}/\")\n","print(f\"üìÅ Models: {models_dir.absolute()}/\")\n","print(f\"üìÅ Visualizations: {viz_dir.absolute()}/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"elapsed":61,"status":"error","timestamp":1763356229772,"user":{"displayName":"Miao Yu","userId":"12559096100817137710"},"user_tz":360},"id":"nIeVr98yLOkW","outputId":"4cd2cf78-1cad-4c91-a084-7439f7aa1fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","================================================================================\n","üî¨ SHAP ANALYSIS - BEST MODEL FEATURE IMPORTANCE (CONSTRUCT LEVEL)\n","================================================================================\n","\n","Analyzing feature importance for the best model of each symptom...\n"]},{"ename":"NameError","evalue":"name 'train_test_data' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2166280595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0msymptom_shap_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msymptom_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphysSx_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0msymptom_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymptom_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysSx_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphysSx_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[{symptom_idx}/{len(train_test_data)}] {symptom_name} ({physSx_var})...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_test_data' is not defined"]}],"source":["# ============================================================================\n","# 8. SHAP ANALYSIS - FEATURE IMPORTANCE FOR BEST MODELS (CONSTRUCT LEVEL)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üî¨ SHAP ANALYSIS - BEST MODEL FEATURE IMPORTANCE (CONSTRUCT LEVEL)\")\n","print(\"=\"*80)\n","print(\"\\nAnalyzing feature importance for the best model of each symptom...\")\n","\n","# Create SHAP output directory\n","shap_dir = output_dir / \"shap_analysis\"\n","shap_dir.mkdir(exist_ok=True)\n","\n","# ---------------------------------------------------------------------------\n","# Map raw feature names to psychological constructs for SHAP interpretation\n","# ---------------------------------------------------------------------------\n","\n","# Map from scale prefixes to human-readable construct labels\n","construct_map = {\n","    'idea_m':           'Identity exploration',\n","    'moa_achievement_m':'Moral achievement',\n","    'moa_importance_m': 'Moral importance',\n","    'stress_m':         'Perceived stress',\n","    'support_m':        'Perceived support',\n","    'belong_m':         'Belongingness',\n","    'mindful_m':        'Mindfulness',\n","    'efficacy_m':       'Self-efficacy',\n","    'exploit_m':        'Exploitation',\n","    'disability_m':     'Functional disability',\n","    'social_conn_m':    'Social media ‚Äì connection',\n","    'social_new_m':     'Social media ‚Äì new people',\n","    'social_info_m':    'Social media ‚Äì information',\n","    'swb_m':            'Subjective well-being',\n","    'transgres_m':      'Transgressions',\n","    'usdream_m':        'American dream',\n","    # add more here if you have other composite scales you want to aggregate\n","}\n","\n","def feature_to_construct(feat: str) -> str:\n","    \"\"\"\n","    Map a detailed feature name (including _std/_max/_min or log/sqrt/squared)\n","    back to its psychological construct. If no mapping is found, return the\n","    original feature name (useful for demographics, sex dummies, etc.).\n","    \"\"\"\n","    # Strip common engineered suffixes\n","    suffixes = ['_std', '_max', '_min', '_log', '_sqrt', '_squared']\n","    base = feat\n","    for s in suffixes:\n","        if base.endswith(s):\n","            base = base[: -len(s)]\n","            break\n","\n","    # Match to one of the composite scale prefixes\n","    for prefix, label in construct_map.items():\n","        if base.startswith(prefix):\n","            return label\n","\n","    # Default: keep raw feature name\n","    return feat\n","\n","# ---------------------------------------------------------------------------\n","# Function to create SHAP plots at the CONSTRUCT level\n","# ---------------------------------------------------------------------------\n","\n","def create_shap_plot(shap_values, X_data, feature_names, symptom_name, model_name, output_path):\n","    \"\"\"\n","    Create mean absolute SHAP value plot for the TOP 15 CONSTRUCTS.\n","    Returns both feature-level and construct-level importance tables.\n","    \"\"\"\n","    try:\n","        # 1. Handle different SHAP formats (list vs array, multiclass, etc.)\n","        if isinstance(shap_values, list):\n","            # For binary/multiclass, pick the last class\n","            shap_vals = shap_values[-1]\n","        else:\n","            shap_vals = shap_values\n","\n","        # If 3D (n_samples x n_features x n_classes), take last class\n","        if shap_vals.ndim == 3:\n","            shap_vals = shap_vals[:, :, -1]\n","\n","        # 2. Mean abs SHAP per *raw feature*\n","        mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n","\n","        feature_importance_df = pd.DataFrame({\n","            'Feature': feature_names,\n","            'Mean_Abs_SHAP': mean_abs_shap\n","        })\n","\n","        # 3. Map each feature to its psychological construct\n","        feature_importance_df['Construct'] = feature_importance_df['Feature'].apply(feature_to_construct)\n","\n","        # 4. Aggregate by construct (sum of SHAP; you could also use mean)\n","        construct_importance = (\n","            feature_importance_df\n","            .groupby('Construct', as_index=False)['Mean_Abs_SHAP']\n","            .sum()\n","            .sort_values('Mean_Abs_SHAP', ascending=True)\n","        )\n","\n","        # 5. Plot top 15 constructs\n","        top_15 = construct_importance.tail(15)\n","\n","        fig, ax = plt.subplots(figsize=(10, 8))\n","        colors = plt.cm.RdYlGn_r(top_15['Mean_Abs_SHAP'] / top_15['Mean_Abs_SHAP'].max())\n","        bars = ax.barh(range(len(top_15)), top_15['Mean_Abs_SHAP'])\n","\n","        for bar, color in zip(bars, colors):\n","            bar.set_color(color)\n","\n","        # Add numeric labels\n","        for i, (bar, val) in enumerate(zip(bars, top_15['Mean_Abs_SHAP'])):\n","            ax.text(val, i, f' {val:.3f}', va='center', fontsize=9)\n","\n","        ax.set_yticks(range(len(top_15)))\n","        ax.set_yticklabels(top_15['Construct'], fontsize=10)\n","        ax.set_xlabel('Mean Absolute SHAP Value (Feature Importance)', fontsize=12)\n","        ax.set_title(\n","            f'{symptom_name} - {model_name}\\nTop 15 Most Important Constructs',\n","            fontsize=14,\n","            fontweight='bold'\n","        )\n","        ax.grid(axis='x', alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n","        plt.close()\n","\n","        # Return both: raw feature-level & construct-level importance\n","        return {\n","            'feature_level': feature_importance_df.sort_values('Mean_Abs_SHAP', ascending=True),\n","            'construct_level': construct_importance\n","        }\n","\n","    except Exception as e:\n","        print(f\"    ‚ö†Ô∏è  Error creating SHAP plot: {e}\")\n","        return None\n","\n","# ---------------------------------------------------------------------------\n","# Run SHAP analysis for each symptom using its best-performing model\n","# ---------------------------------------------------------------------------\n","\n","symptom_shap_results = {}\n","\n","for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):\n","    symptom_name = symptom_names.get(physSx_var, physSx_var)\n","    print(f\"\\n[{symptom_idx}/{len(train_test_data)}] {symptom_name} ({physSx_var})...\")\n","\n","    X_train = data_splits['X_train'].copy()\n","    X_test = data_splits['X_test'].copy()\n","    y_train = data_splits['y_train']\n","    y_test = data_splits['y_test']\n","\n","    # Ensure categoricals are strings (consistent with preprocessing)\n","    for col in categorical_features:\n","        if col in X_train.columns:\n","            X_train[col] = X_train[col].astype(str)\n","            X_test[col] = X_test[col].astype(str)\n","\n","    # Find best model for this symptom\n","    symptom_results = results_df[results_df['Symptom'] == physSx_var]\n","    if len(symptom_results) == 0:\n","        print(f\"  ‚ö†Ô∏è  No results found for {physSx_var}\")\n","        continue\n","\n","    best_model_name = symptom_results.loc[symptom_results['F1_Score'].idxmax(), 'Model']\n","    print(f\"  Best model: {best_model_name}\")\n","\n","    try:\n","        # Determine if model is tree-based or linear\n","        is_tree_model = any(x in best_model_name for x in ['XGBoost', 'CatBoost', 'LightGBM', 'RandomForest'])\n","\n","        if is_tree_model:\n","            # Use UNSCALED preprocessor for tree models\n","            X_train_proc = preprocessor_unscaled.fit_transform(X_train)\n","            X_test_proc = preprocessor_unscaled.transform(X_test)\n","\n","            # Get feature names\n","            feature_names = (\n","                numeric_features +\n","                list(preprocessor_unscaled.named_transformers_['cat']\n","                     .get_feature_names_out(categorical_features))\n","            )\n","        else:\n","            # Use SCALED preprocessor for linear models\n","            X_train_proc = preprocessor_scaled.fit_transform(X_train)\n","            X_test_proc = preprocessor_scaled.transform(X_test)\n","\n","            # Get feature names\n","            feature_names = (\n","                numeric_features +\n","                list(preprocessor_scaled.named_transformers_['cat']\n","                     .get_feature_names_out(categorical_features))\n","            )\n","\n","        # -------------------------------------------------------------------\n","        # Initialize and fit the model corresponding to best_model_name\n","        # -------------------------------------------------------------------\n","        if 'XGBoost' in best_model_name:\n","            model = xgb.XGBClassifier(\n","                n_estimators=100, max_depth=6, learning_rate=0.1,\n","                random_state=42, use_label_encoder=False, eval_metric='logloss'\n","            )\n","        elif 'CatBoost' in best_model_name:\n","            model = CatBoostClassifier(\n","                iterations=100, depth=6, learning_rate=0.1,\n","                random_state=42, verbose=0\n","            )\n","        elif 'LightGBM' in best_model_name:\n","            model = lgb.LGBMClassifier(\n","                n_estimators=100, max_depth=6, learning_rate=0.1,\n","                random_state=42, verbose=-1\n","            )\n","        elif 'RandomForest' in best_model_name:\n","            model = RandomForestClassifier(\n","                n_estimators=100, max_depth=10,\n","                class_weight='balanced',\n","                random_state=42, n_jobs=-1\n","            )\n","        else:  # Linear models (Elastic Net Logistic Regression)\n","            model = LogisticRegression(\n","                penalty='elasticnet', l1_ratio=0.5, solver='saga',\n","                max_iter=1000, class_weight='balanced', random_state=42\n","            )\n","\n","        model.fit(X_train_proc, y_train)\n","\n","        # -------------------------------------------------------------------\n","        # SHAP analysis\n","        # -------------------------------------------------------------------\n","        if is_tree_model:\n","            # Tree-based models: use TreeExplainer with interventional perturbation\n","            explainer = shap.TreeExplainer(\n","                model,\n","                X_train_proc,\n","                feature_perturbation=\"interventional\"\n","            )\n","            shap_vals = explainer.shap_values(X_test_proc, check_additivity=False)\n","        else:\n","            # Linear models: use LinearExplainer\n","            explainer = shap.LinearExplainer(model, X_train_proc)\n","            shap_vals = explainer.shap_values(X_test_proc)\n","\n","        # Create and save construct-level plot\n","        plot_path = shap_dir / f\"{physSx_var}_{best_model_name.replace('/', '_')}_shap_constructs.png\"\n","        importance_info = create_shap_plot(\n","            shap_vals, X_test_proc, feature_names,\n","            symptom_name, best_model_name, plot_path\n","        )\n","\n","        if importance_info is not None:\n","            symptom_shap_results[physSx_var] = {\n","                'symptom_name': symptom_name,\n","                'model': best_model_name,\n","                'feature_importance': importance_info['feature_level'],\n","                'construct_importance': importance_info['construct_level']\n","            }\n","            print(f\"  ‚úì SHAP plot saved: {plot_path.name}\")\n","\n","            # Show top 10 constructs in console\n","            top_10 = importance_info['construct_level'].tail(10)\n","            print(f\"\\n  Top 10 Constructs for {symptom_name}:\")\n","            for _, row in top_10.iterrows():\n","                print(f\"    {row['Construct']}: {row['Mean_Abs_SHAP']:.4f}\")\n","\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è  SHAP analysis failed for {physSx_var}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","# ---------------------------------------------------------------------------\n","# Save SHAP results summary (construct level)\n","# ---------------------------------------------------------------------------\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"Saving SHAP analysis summary (construct level)...\")\n","\n","try:\n","    # Summary of top 15 constructs per symptom\n","    summary_data = []\n","    for symptom, data_obj in symptom_shap_results.items():\n","        symptom_name = data_obj['symptom_name']\n","        model_name = data_obj['model']\n","        construct_imp = data_obj['construct_importance']\n","\n","        top_15 = construct_imp.tail(15)\n","        for _, row in top_15.iterrows():\n","            summary_data.append({\n","                'Symptom_Code': symptom,\n","                'Symptom_Name': symptom_name,\n","                'Model': model_name,\n","                'Construct': row['Construct'],\n","                'Mean_Abs_SHAP': row['Mean_Abs_SHAP']\n","            })\n","\n","    if summary_data:\n","        summary_df = pd.DataFrame(summary_data)\n","        summary_df.to_csv(shap_dir / \"shap_top15_constructs_per_symptom.csv\", index=False)\n","        print(f\"  ‚úì Saved: shap_top15_constructs_per_symptom.csv\")\n","\n","        # Overall construct importance across all symptoms\n","        overall_importance = (\n","            summary_df\n","            .groupby('Construct')['Mean_Abs_SHAP']\n","            .agg(['mean', 'std', 'count'])\n","            .sort_values('mean', ascending=False)\n","        )\n","        overall_importance.to_csv(shap_dir / \"shap_overall_construct_importance.csv\")\n","        print(f\"  ‚úì Saved: shap_overall_construct_importance.csv\")\n","\n","        print(\"\\nüìä Top 15 Most Important Constructs Across All Symptoms:\")\n","        print(overall_importance.head(15))\n","\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Error saving SHAP summaries: {e}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"‚úÖ SHAP ANALYSIS COMPLETE (CONSTRUCT LEVEL)!\")\n","print(\"=\"*80)\n","print(f\"\\nüìÅ SHAP plots saved in: {shap_dir.absolute()}/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2264,"status":"ok","timestamp":1763115854319,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"},"user_tz":360},"id":"rcRZ4gbEA3No","outputId":"08000079-27c8-423e-b5d7-03d70b98131b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","================================================================================\n","üì§ SAVING RESULTS TO GOOGLE DRIVE\n","================================================================================\n","\n","Mounting Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","‚úì Google Drive mounted successfully!\n","\n","Creating directory structure in Google Drive...\n","\n","Saving CSV files to Google Drive...\n","  ‚úì Saved: all_results.csv\n","  ‚úì Saved: best_per_symptom.csv\n","  ‚úì Saved: model_averages.csv\n","\n","Copying trained models to Google Drive...\n","  ‚úì Copied: trained_models/\n","\n","Copying visualizations to Google Drive...\n","  ‚úì Copied: visualizations/\n","\n","Copying SHAP analysis to Google Drive...\n","  ‚úì Copied: shap_analysis/\n","\n","‚úÖ All results successfully saved to Google Drive!\n","   Location: /My Drive/somatic-symptom/Result/results_20251114_102412\n","   Access at: https://drive.google.com/\n","\n","================================================================================\n","üéâ ALL DONE!\n","================================================================================\n","\n","üìÅ Local results: /content/results_ultra_optimized\n"]}],"source":["# ============================================================================\n","# 9. SAVE RESULTS TO GOOGLE DRIVE\n","# ============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"üì§ SAVING RESULTS TO GOOGLE DRIVE\")\n","print(\"=\"*80)\n","\n","try:\n","    from google.colab import drive\n","    import shutil\n","\n","    # Mount Google Drive\n","    print(\"\\nMounting Google Drive...\")\n","    drive.mount('/content/drive')\n","    print(\"‚úì Google Drive mounted successfully!\")\n","\n","    # Define Google Drive paths\n","    gdrive_base = Path('/content/drive/My Drive/somatic-symptom/Result')\n","    gdrive_results = gdrive_base / f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","\n","    # Create directory structure in Google Drive\n","    print(\"\\nCreating directory structure in Google Drive...\")\n","    gdrive_results.mkdir(parents=True, exist_ok=True)\n","\n","    # Save CSV files\n","    print(\"\\nSaving CSV files to Google Drive...\")\n","    results_df.to_csv(gdrive_results / \"all_results.csv\", index=False)\n","    print(f\"  ‚úì Saved: all_results.csv\")\n","\n","    best_per_symptom.to_csv(gdrive_results / \"best_per_symptom.csv\", index=False)\n","    print(f\"  ‚úì Saved: best_per_symptom.csv\")\n","\n","    model_avg.to_csv(gdrive_results / \"model_averages.csv\")\n","    print(f\"  ‚úì Saved: model_averages.csv\")\n","\n","    # Copy trained models directory\n","    if (output_dir / \"trained_models\").exists():\n","        print(\"\\nCopying trained models to Google Drive...\")\n","        shutil.copytree(output_dir / \"trained_models\", gdrive_results / \"trained_models\")\n","        print(f\"  ‚úì Copied: trained_models/\")\n","\n","    # Copy visualizations directory\n","    if (output_dir / \"visualizations\").exists():\n","        print(\"\\nCopying visualizations to Google Drive...\")\n","        shutil.copytree(output_dir / \"visualizations\", gdrive_results / \"visualizations\")\n","        print(f\"  ‚úì Copied: visualizations/\")\n","\n","    # Copy SHAP analysis directory\n","    if (output_dir / \"shap_analysis\").exists():\n","        print(\"\\nCopying SHAP analysis to Google Drive...\")\n","        shutil.copytree(output_dir / \"shap_analysis\", gdrive_results / \"shap_analysis\")\n","        print(f\"  ‚úì Copied: shap_analysis/\")\n","\n","    print(\"\\n‚úÖ All results successfully saved to Google Drive!\")\n","    print(f\"   Location: /My Drive/somatic-symptom/Result/{gdrive_results.name}\")\n","    print(f\"   Access at: https://drive.google.com/\")\n","\n","except ImportError:\n","    print(\"\\n‚ö†Ô∏è  Not running in Google Colab environment\")\n","    print(\"   Results are saved locally only\")\n","except Exception as e:\n","    print(f\"\\n‚ö†Ô∏è  Error saving to Google Drive: {e}\")\n","    print(\"   Results are saved locally but not uploaded to Google Drive\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ ALL DONE!\")\n","print(\"=\"*80)\n","print(f\"\\nüìÅ Local results: {output_dir.absolute()}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
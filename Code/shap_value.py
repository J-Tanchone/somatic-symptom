# -*- coding: utf-8 -*-
"""SHAP value

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-5M0KsJ6QDiNkxrY1G8cjrcEa2e8TDeZ
"""

# ============================================================================
# ULTRA-AGGRESSIVE CONFIGURATION
# ============================================================================
print("="*80)
print("üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•")
print("="*80)

# üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY - TRY EVERYTHING!
ENABLE_ALL_FEATURES = True
ENABLE_DEEP_LEARNING = True
ENABLE_ADVANCED_ENSEMBLES = True
ENABLE_BAYESIAN_OPTIMIZATION = True
OPTUNA_TRIALS = 200  # INCREASED for deeper optimization
CV_FOLDS = 10  # INCREASED for more robust validation
CV_REPEATS = 3  # INCREASED for stability
FEATURE_SELECTION_K = 'all'  # USE ALL FEATURES (no selection = more info)
USE_GPU = True
FOCUS_TREE_MODELS_ONLY = True  # FALSE = Try ALL models (tree, linear, SVM, KNN, NB, DL, etc.)
ENABLE_ALL_MODEL_TYPES = False  # Enable SVM, KNN, NB, LDA, QDA, MLP, etc.
ENABLE_AUTOGLUON = True  # Enable AutoGluon automated ensemble (2h/symptom = 26h total)
AUTOGLUON_TIME_LIMIT = 3600 * 2  # 2 hours per symptom
AUTOGLUON_PRESET = 'best_quality'  # 'best_quality', 'high_quality', 'good_quality', 'medium_quality'

# Optimization target - CHANGED TO MAXIMIZE REGULAR ACCURACY (not balanced)
OPTIMIZE_FOR = 'accuracy'  # Options: 'accuracy', 'balanced_accuracy', 'f1', 'roc_auc'
# accuracy = pure accuracy optimization (allows natural class distribution)

# Advanced techniques
ENABLE_AUTO_ENCODER = True  # Dimensionality reduction with neural network
ENABLE_FOCAL_LOSS = True  # Focus on hard examples
ENABLE_LABEL_SMOOTHING = True  # Regularization technique
ENABLE_MIXUP = True  # Data augmentation for tabular data
ENABLE_PSEUDO_LABELING = False  # Semi-supervised learning (disabled - no unlabeled data)

print("\nüöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY")
print(f"   Optimization Target: {OPTIMIZE_FOR.upper()}")
print(f"   Optuna Trials: {OPTUNA_TRIALS}")
print(f"   CV: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats")
print(f"   Feature Selection: {FEATURE_SELECTION_K.upper() if FEATURE_SELECTION_K == 'all' else f'Top {FEATURE_SELECTION_K}'}")
print(f"   Tree Models Only: {FOCUS_TREE_MODELS_ONLY}")
print(f"   AutoGluon Ensemble: {'‚úÖ Enabled' if ENABLE_AUTOGLUON else '‚ùå Disabled'}")
if ENABLE_AUTOGLUON:
    print(f"      Time Limit: {AUTOGLUON_TIME_LIMIT/3600:.1f}h/symptom")
    print(f"      Preset: {AUTOGLUON_PRESET}")
print(f"   Sampling Strategy: NONE (natural distribution)")
print(f"   GPU Acceleration: {USE_GPU}")
print("="*80)

! pip install catboost

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import subprocess

from pathlib import Path
from scipy.stats import skew, kurtosis
from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                              ExtraTreesClassifier, StackingClassifier, VotingClassifier,
                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_selection import (SelectKBest, mutual_info_classif, f_classif,
                                       chi2, RFE, SelectFromModel, VarianceThreshold)
from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight
from sklearn.decomposition import PCA, TruncatedSVD
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks
from imblearn import FunctionSampler  # For passthrough (no resampling)
from sklearn.model_selection import GridSearchCV

# Gradient Boosting
import xgboost as xgb
import lightgbm as lgb
import shap  # For SHAP analysis


# Metrics
from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix,
                            accuracy_score, balanced_accuracy_score, f1_score,
                            precision_score, recall_score, matthews_corrcoef)

# Model serialization
import joblib
import pickle
import json
from datetime import datetime

np.random.seed(42)

# ============================================================================
# 1. ULTRA-AGGRESSIVE DATA LOADING & PREPROCESSING
# ============================================================================
print("\n[1/10] Loading data with ultra-aggressive preprocessing...")

repo_path = Path("somatic-symptom")
if not repo_path.exists():
    print("Cloning repository from GitHub...")
    subprocess.run([
        "git", "clone",
        "https://J-Tanchone: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@github.com/J-Tanchone/somatic-symptom.git"
    ])
data = pd.read_excel("somatic-symptom/EAMMi2-Data1/EAMMi2-Data1.2.xlsx", sheet_name="EAMMi2_Data")

# Recode variables
data['sibling_c'] = data['sibling'].apply(lambda x: -0.5 if x == 1 else 0.5)
data = data.rename(columns={'marriage2': 'marriage_importance', 'marriage5': 'parental_marriage'})
data = pd.concat([data, pd.get_dummies(data['parental_marriage'].astype('category'),
                                      prefix='parental_marriage', drop_first=True)], axis=1)

# Compute all psychological scales
scales = {
    'idea_m': [f'IDEA_{i}' for i in range(1, 9)],
    'moa_achievement_m': [f'moa1#2_{i}' for i in range(1, 11)] + [f'moa2#1_{i}' for i in range(1, 11)],
    'moa_importance_m': [f'moa2#1_{i}' for i in range(1, 11)] + [f'moa2#2_{i}' for i in range(1, 11)],
    'stress_m': [f'stress_{i}' for i in range(1, 11)],
    'support_m': [f'support_{i}' for i in range(1, 13)],
    'belong_m': [f'belong_{i}' for i in range(1, 11)],
    'mindful_m': [f'mindful_{i}' for i in range(1, 16)],
    'efficacy_m': [f'efficacy_{i}' for i in range(1, 11)],
    'exploit_m': [f'exploit_{i}' for i in range(1, 4)],
    'disability_m': [f'Q10_{i}' for i in range(1, 16)] + ['Q11'] + [f'Q14_{i}' for i in range(1, 7)],
    'social_conn_m': [f'SocMedia_{i}' for i in range(1, 6)],
    'social_new_m': [f'SocMedia_{i}' for i in range(6, 10)],
    'social_info_m': [f'SocMedia_{i}' for i in range(10, 12)],
    'swb_m': [f'swb_{i}' for i in range(1, 7)],
    'transgres_m': [f'transgres_{i}' for i in range(1, 5)],
    'usdream_m': ['usdream_1', 'usdream_2']
}

for name, items in scales.items():
    valid_items = [item for item in items if item in data.columns]
    if valid_items:
        data[name] = data[valid_items].mean(axis=1, skipna=True)
        # Add variance and std as additional features
        data[f'{name}_std'] = data[valid_items].std(axis=1, skipna=True)
        data[f'{name}_max'] = data[valid_items].max(axis=1, skipna=True)
        data[f'{name}_min'] = data[valid_items].min(axis=1, skipna=True)

print(f"‚úì Created {len(scales)*4} scale features (mean, std, max, min)")

# ============================================================================
# 2. ULTRA-AGGRESSIVE FEATURE ENGINEERING (50+ Features)
# ============================================================================
print("\n[2/10] Creating 50+ engineered features...")

feature_count = 0

# Level 1: Basic Interactions (Psychology Literature)
if 'stress_m' in data.columns and 'support_m' in data.columns:
    data['stress_support_interaction'] = data['stress_m'] * (1 - data['support_m'])
    data['stress_support_ratio'] = data['stress_m'] / (data['support_m'] + 0.01)
    feature_count += 2

if 'mindful_m' in data.columns and 'stress_m' in data.columns:
    data['mindful_stress_buffer'] = data['mindful_m'] * (1 - data['stress_m'])
    data['mindfulness_deficit'] = (1 - data['mindful_m']) * data['stress_m']
    feature_count += 2

if 'efficacy_m' in data.columns:
    data['efficacy_stress_interaction'] = data['efficacy_m'] * data['stress_m']
    data['efficacy_belong_interaction'] = data['efficacy_m'] * data['belong_m']
    data['low_efficacy_high_stress'] = (1 - data['efficacy_m']) * data['stress_m']
    feature_count += 3

# Level 2: Composite Indices
if all(col in data.columns for col in ['stress_m', 'support_m', 'belong_m', 'efficacy_m']):
    # Psychological Distress Composite
    data['psychological_distress'] = (
        data['stress_m'] * 0.40 +
        (1 - data['support_m']) * 0.30 +
        (1 - data['belong_m']) * 0.20 +
        (1 - data['efficacy_m']) * 0.10
    )

    # Protective Factors Composite
    data['protective_factors'] = (
        data['mindful_m'] * 0.35 +
        data['support_m'] * 0.30 +
        data['belong_m'] * 0.20 +
        data['efficacy_m'] * 0.15
    )

    # Risk-Protection Balance
    data['risk_protection_ratio'] = data['psychological_distress'] / (data['protective_factors'] + 0.01)
    data['risk_protection_difference'] = data['psychological_distress'] - data['protective_factors']
    feature_count += 4

# Level 3: Domain Expert Features (Somatization Literature)
if all(col in data.columns for col in ['stress_m', 'mindful_m', 'swb_m']):
    # Alexithymia proxy (emotional awareness deficit)
    data['emotional_awareness_deficit'] = data['stress_m'] * (1 - data['mindful_m']) * (1 - data['swb_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'efficacy_m' in data.columns:
    # Catastrophizing tendency
    data['catastrophizing_score'] = (data['stress_m'] ** 2) / (data['efficacy_m'] + 0.01)
    feature_count += 1

if all(col in data.columns for col in ['support_m', 'belong_m', 'social_conn_m']):
    # Social isolation index
    data['social_isolation'] = (1 - data['support_m']) * (1 - data['belong_m']) * (1 - data['social_conn_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'mindful_m' in data.columns:
    # Rumination tendency
    data['rumination_tendency'] = data['stress_m'] * (1 - data['mindful_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'disability_m' in data.columns:
    # Health anxiety proxy
    data['health_anxiety_proxy'] = data['stress_m'] * data['disability_m']
    feature_count += 1

# Level 4: Achievement-Stress Interactions
if 'moa_achievement_m' in data.columns and 'moa_importance_m' in data.columns:
    data['achievement_gap'] = data['moa_importance_m'] - data['moa_achievement_m']
    data['achievement_gap_squared'] = data['achievement_gap'] ** 2
    data['achievement_stress_interaction'] = data['achievement_gap'] * data['stress_m']
    data['perfectionism_stress'] = (data['achievement_gap'] ** 2) * data['stress_m']
    feature_count += 4

# Level 5: Non-linear Transformations
numerical_cols = ['stress_m', 'support_m', 'mindful_m', 'efficacy_m', 'belong_m', 'swb_m']
for col in numerical_cols:
    if col in data.columns:
        data[f'{col}_squared'] = data[col] ** 2
        data[f'{col}_sqrt'] = np.sqrt(data[col])
        data[f'{col}_log'] = np.log1p(data[col])  # log(1+x) to handle zeros
        feature_count += 3

# Level 6: Statistical Aggregations Across Scales
scale_means = [col for col in data.columns if col.endswith('_m')]
if len(scale_means) >= 3:
    data['overall_mean'] = data[scale_means].mean(axis=1)
    data['overall_std'] = data[scale_means].std(axis=1)
    data['overall_skew'] = data[scale_means].apply(lambda x: skew(x.dropna()), axis=1)
    data['overall_kurtosis'] = data[scale_means].apply(lambda x: kurtosis(x.dropna()), axis=1)
    feature_count += 4

# Level 7: Somatization Proneness Index (Multi-component)
if all(col in data.columns for col in ['stress_m', 'support_m', 'mindful_m', 'swb_m', 'efficacy_m']):
    data['somatization_proneness'] = (
        data['stress_m'] * 0.30 +
        (1 - data['support_m']) * 0.20 +
        (1 - data['mindful_m']) * 0.20 +
        (1 - data['swb_m']) * 0.15 +
        (1 - data['efficacy_m']) * 0.15
    )
    feature_count += 1

# Level 8: Demographic Interactions
if 'sex' in data.columns and 'stress_m' in data.columns:
    data['sex_stress'] = data['sex'].astype(float) * data['stress_m']
    feature_count += 1

if 'edu' in data.columns and 'efficacy_m' in data.columns:
    data['edu_efficacy'] = data['edu'].astype(float) * data['efficacy_m']
    feature_count += 1

if 'income' in data.columns and 'stress_m' in data.columns:
    data['income_stress'] = data['income'].astype(float) * data['stress_m']
    feature_count += 1

# Level 9: Resilience vs Vulnerability
if 'psychological_distress' in data.columns and 'protective_factors' in data.columns:
    data['resilience_index'] = data['protective_factors']
    data['vulnerability_index'] = data['psychological_distress']
    data['vulnerability_resilience_balance'] = data['vulnerability_index'] - data['resilience_index']
    data['stress_amplification'] = data['stress_m'] / (data['resilience_index'] + 0.1)
    feature_count += 4

# Level 10: Cross-domain Interactions
if all(col in data.columns for col in ['social_quality', 'disability_m']):
    data['social_disability_interaction'] = data['social_quality'] * data['disability_m']
    feature_count += 1

print(f"‚úì Created {feature_count} engineered features")

# Select final features
final_data = data[[col for col in data.columns if
                  col.endswith(('_m', '_c', '_index', '_ratio', '_balance', '_deficit',
                               '_interaction', '_score', '_isolation', '_tendency', '_proxy',
                               '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                               '_log', '_std', '_max', '_min', '_quality', '_gap')) or
                  col in ['marriage_importance', 'parental_marriage', 'overall_mean',
                          'overall_std', 'overall_skew', 'overall_kurtosis', 'social_quality'] or
                  col.startswith(('parental_marriage_', 'sex_', 'edu_', 'income_'))]]

# Add demographics
for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']:
    if col in data.columns and col not in final_data.columns:
        final_data[col] = data[col]

# Binarize outcomes
physSx_cols = [f'physSx_{i}' for i in range(1, 14)]
for col in physSx_cols:
    if col in data.columns:
        data[col] = data[col].replace({1: 0, 2: 1, 3: 1})
        final_data[col] = data[col]

print(f"‚úì Final dataset: {final_data.shape[0]} samples √ó {final_data.shape[1]} features")

# ============================================================================
# 3. TRAIN/TEST SPLITS WITH STRATIFICATION
# ============================================================================
print("\n[3/10] Creating stratified train/test splits...")

numeric_features = [col for col in final_data.columns if col.endswith(('_m', '_c', '_index', '_ratio',
                    '_balance', '_deficit', '_interaction', '_score', '_isolation', '_tendency',
                    '_proxy', '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                    '_log', '_std', '_max', '_min', '_gap', 'overall_mean', 'overall_std',
                    'overall_skew', 'overall_kurtosis')) and col not in physSx_cols]

categorical_features = [col for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']
                       if col in final_data.columns]

print(f"‚úì {len(numeric_features)} numeric features")
print(f"‚úì {len(categorical_features)} categorical features")

train_test_data = {}
X = final_data.drop(columns=physSx_cols, errors='ignore')

for physSx_var in physSx_cols:
    if physSx_var not in final_data.columns:
        continue

    y = data[physSx_var]
    model_data = X.join(y.rename(physSx_var)).dropna()
    X_clean = model_data.drop(columns=[physSx_var])
    y_clean = model_data[physSx_var]

    if y_clean.sum() < 10:
        continue

    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean
    )

    for col in categorical_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)

    train_test_data[physSx_var] = {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

    pos_pct = (y_clean.sum() / len(y_clean)) * 100
    print(f"  ‚úì {physSx_var}: {len(X_train)} train, {len(X_test)} test | {pos_pct:.1f}% positive")

try:
    # Define repository path
    repo_path = Path("somatic-symptom")
    results_path = "somatic-symptom/Result/all_results.csv"

    # Read all_results_noGLMnet.csv
    model = pd.read_csv(results_path)

    print(f"  ‚úì Found {len(previous_df)} results from previous analysis")
    print(f"  Models: {previous_df['Model'].unique().tolist()}")
except FileNotFoundError:
    print(f"‚ö†Ô∏è  File not found: {results_path}")
    print(f"     Make sure the repository is cloned and the file exists")
except Exception as e:
    print(f"‚ö†Ô∏è  Could not import previous results: {e}")
    import traceback
    traceback.print_exc()

# ============================================================================
# 4. ADVANCED PREPROCESSING PIPELINES
# ============================================================================
print("\n[4/10] Setting up advanced preprocessing pipelines...")

# Multiple preprocessors for different model types
preprocessor_scaled = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_robust = ColumnTransformer(transformers=[
    ('num', RobustScaler(), numeric_features),  # Better for outliers
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_power = ColumnTransformer(transformers=[
    ('num', PowerTransformer(method='yeo-johnson'), numeric_features),  # Normalize distributions
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_unscaled = ColumnTransformer(transformers=[
    ('num', 'passthrough', numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

print("‚úì Created 4 preprocessing strategies")

# ============================================================================
# SHAP ANALYSIS - FEATURE IMPORTANCE FOR BEST MODELS (ORIGINAL PREDICTORS)
# Based on ROC_AUC Performance
# ============================================================================

print("\n" + "="*80)
print("üî¨ SHAP ANALYSIS - BEST MODEL FEATURE IMPORTANCE (ORIGINAL PREDICTORS)")
print("Selection Criterion: ROC_AUC")
print("="*80)
print("\nAnalyzing feature importance for the best model of each symptom...")

# Create SHAP output directory
try:
    from google.colab import drive
    if not Path('/content/drive').exists():
        drive.mount('/content/drive')
    shap_dir = Path('/content/drive/My Drive/somatic-symptom/Result/shap_analysis')
except:
    shap_dir = Path('shap_analysis')

shap_dir.mkdir(parents=True, exist_ok=True)
print(f"üìÅ SHAP plots will be saved to: {shap_dir.absolute()}")

# ---------------------------------------------------------------------------
# Define preprocessors with improved handling
# ---------------------------------------------------------------------------

# Preprocessor for tree-based models (no scaling)
preprocessor_unscaled = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numeric_features),
        ('cat', OneHotEncoder(
            drop='first',
            sparse_output=False,
            handle_unknown='infrequent_if_exist',  # Better handling
            min_frequency=0.01
        ), categorical_features)
    ],
    remainder='drop'
)

# Preprocessor for linear models (with scaling)
preprocessor_scaled = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(
            drop='first',
            sparse_output=False,
            handle_unknown='infrequent_if_exist',  # Better handling
            min_frequency=0.01
        ), categorical_features)
    ],
    remainder='drop'
)

# ---------------------------------------------------------------------------
# Map symptom codes to readable names
# ---------------------------------------------------------------------------

symptom_names = {
    'physSx_1': 'Stomach pain',
    'physSx_2': 'Back pain',
    'physSx_3': 'Limb/joint pain',
    'physSx_4': 'Headache',
    'physSx_5': 'Chest pain',
    'physSx_6': 'Dizziness',
    'physSx_7': 'Fainting spells',
    'physSx_8': 'Heart pound/race',
    'physSx_9': 'Shortness of breath',
    'physSx_10': 'Constipation',
    'physSx_11': 'Nausea/gas/indigestion',
    'physSx_12': 'Fatigue',
    'physSx_13': 'Trouble sleeping'
}

# ---------------------------------------------------------------------------
# Map raw feature names to original predictor names
# ---------------------------------------------------------------------------

predictor_map = {
    'idea_m': "Identity Exploration (IDEA-8)",
    'moa_achievement_m': "Markers of Adulthood - Achievement",
    'moa_importance_m': "Markers of Adulthood - Importance",
    'stress_m': "Perceived Stress",
    'support_m': "Perceived Social Support",
    'belong_m': "Need to Belong",
    'mindful_m': "Mindfulness",
    'efficacy_m': "Efficacy/Competence",
    'npi_m': "Narcissistic Personality (NPI-13)",
    'exploit_m': "Interpersonal Exploitativeness",
    'disability_m': "Disability Identity & Status",
    'social_conn_m': "Social Media - Maintaining Connections",
    'social_new_m': "Social Media - Making New Connections",
    'social_info_m': "Social Media - Information Seeking",
    'swb_m': "Subjective Well-Being",
    'transgres_m': "Interpersonal Transgressions",
    'usdream_m': "American Dream",
    'sibling_c': "Sibling Status",
    'marriage_importance': "Marriage Importance",
    'parental_marriage': "Parental Marriage Status",
    'sex': "Sex",
    'edu': "Education",
    'race': "Race",
    'income': "Income"
}

def feature_to_predictor(feat: str) -> str:
    """
    Map a detailed feature name (including _std/_max/_min or log/sqrt/squared)
    back to its original predictor name. Combines all transformations into
    one predictor.
    """
    # Strip common engineered suffixes
    suffixes = ['_std', '_max', '_min', '_log', '_sqrt', '_squared',
                '_interaction', '_ratio', '_balance', '_deficit', '_score',
                '_isolation', '_tendency', '_proxy', '_stress', '_proneness',
                '_amplification', '_gap', '_buffer', '_index', '_quality']

    base = feat
    for s in suffixes:
        if base.endswith(s):
            base = base[: -len(s)]
            break

    # Match to one of the original predictor names
    for prefix, label in predictor_map.items():
        if base.startswith(prefix) or base == prefix:
            return label

    # Handle one-hot encoded categorical features
    for cat_feat in categorical_features:
        if feat.startswith(cat_feat):
            return predictor_map.get(cat_feat, cat_feat)

    # Default: keep raw feature name
    return feat

# ---------------------------------------------------------------------------
# Function to create SHAP plots at the PREDICTOR level
# ---------------------------------------------------------------------------

def create_shap_plot(shap_values, X_data, feature_names, symptom_name, model_name, output_path):
    """
    Create mean absolute SHAP value plot for the TOP 15 PREDICTORS.
    Returns both feature-level and predictor-level importance tables.
    """
    try:
        # 1. Handle different SHAP formats (list vs array, multiclass, etc.)
        if isinstance(shap_values, list):
            # For binary/multiclass, pick the last class
            shap_vals = shap_values[-1]
        else:
            shap_vals = shap_values

        # If 3D (n_samples x n_features x n_classes), take last class
        if shap_vals.ndim == 3:
            shap_vals = shap_vals[:, :, -1]

        # 2. Mean abs SHAP per *raw feature*
        mean_abs_shap = np.abs(shap_vals).mean(axis=0)

        feature_importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Mean_Abs_SHAP': mean_abs_shap
        })

        # 3. Map each feature to its original predictor
        feature_importance_df['Predictor'] = feature_importance_df['Feature'].apply(feature_to_predictor)

        # 4. Aggregate by predictor (sum of SHAP across all transformations)
        predictor_importance = (
            feature_importance_df
            .groupby('Predictor', as_index=False)['Mean_Abs_SHAP']
            .sum()
            .sort_values('Mean_Abs_SHAP', ascending=True)
        )

        # 5. Plot top 15 predictors
        top_15 = predictor_importance.tail(15)

        fig, ax = plt.subplots(figsize=(10, 8))
        colors = plt.cm.Blues(top_15['Mean_Abs_SHAP'] / top_15['Mean_Abs_SHAP'].max())
        bars = ax.barh(range(len(top_15)), top_15['Mean_Abs_SHAP'])

        for bar, color in zip(bars, colors):
            bar.set_color(color)

        # Add numeric labels
        for i, (bar, val) in enumerate(zip(bars, top_15['Mean_Abs_SHAP'])):
            ax.text(val, i, f' {val:.3f}', va='center', fontsize=9)

        ax.set_yticks(range(len(top_15)))
        ax.set_yticklabels(top_15['Predictor'], fontsize=10)
        ax.set_xlabel('Mean Absolute SHAP Value (Feature Importance)', fontsize=12)
        ax.set_title(
            f'{symptom_name} - {model_name}\nTop 15 Most Important Predictors',
            fontsize=14,
            fontweight='bold'
        )
        ax.grid(axis='x', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        # Return both: raw feature-level & predictor-level importance
        return {
            'feature_level': feature_importance_df.sort_values('Mean_Abs_SHAP', ascending=False),
            'predictor_level': predictor_importance
        }

    except Exception as e:
        print(f"    ‚ö†Ô∏è  Error creating SHAP plot: {e}")
        return None

# ---------------------------------------------------------------------------
# Run SHAP analysis for each symptom
# ---------------------------------------------------------------------------

symptom_shap_results = {}

for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):
    symptom_name = symptom_names.get(physSx_var, physSx_var)
    print(f"\n[{symptom_idx}/{len(train_test_data)}] {symptom_name} ({physSx_var})...")

    X_train = data_splits['X_train'].copy()
    X_test = data_splits['X_test'].copy()
    y_train = data_splits['y_train']
    y_test = data_splits['y_test']

    # Ensure categoricals are strings
    for col in categorical_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)

    # Find best model for this symptom from imported CSV - BASED ON ROC_AUC
    symptom_results = model[model['Symptom'] == physSx_var]
    if len(symptom_results) == 0:
        print(f"  ‚ö†Ô∏è  No results found for {physSx_var}")
        continue

    # Get best model based on ROC_AUC
    best_idx = symptom_results['ROC_AUC'].idxmax()
    best_model_name = symptom_results.loc[best_idx, 'Model']
    best_roc_auc = symptom_results.loc[best_idx, 'ROC_AUC']
    best_f1 = symptom_results.loc[best_idx, 'F1_Score']  # Keep for reference

    print(f"  Best model: {best_model_name} (ROC_AUC: {best_roc_auc:.4f}, F1: {best_f1:.4f})")

    try:
        # Determine if model is tree-based or linear
        is_tree_model = any(x in best_model_name for x in ['XGBoost', 'CatBoost', 'LightGBM', 'RandomForest'])

        if is_tree_model:
            # Use UNSCALED preprocessor for tree models
            X_train_proc = preprocessor_unscaled.fit_transform(X_train)
            X_test_proc = preprocessor_unscaled.transform(X_test)

            # Get feature names
            feature_names = (
                numeric_features +
                list(preprocessor_unscaled.named_transformers_['cat']
                     .get_feature_names_out(categorical_features))
            )
        else:
            # Use SCALED preprocessor for linear models
            X_train_proc = preprocessor_scaled.fit_transform(X_train)
            X_test_proc = preprocessor_scaled.transform(X_test)

            # Get feature names
            feature_names = (
                numeric_features +
                list(preprocessor_scaled.named_transformers_['cat']
                     .get_feature_names_out(categorical_features))
            )

        # Initialize and fit the model
        if 'XGBoost' in best_model_name:
            trained_model = xgb.XGBClassifier(
                n_estimators=100, max_depth=6, learning_rate=0.1,
                random_state=42, use_label_encoder=False, eval_metric='logloss'
            )
        elif 'CatBoost' in best_model_name:
            from catboost import CatBoostClassifier
            trained_model = CatBoostClassifier(
                iterations=100, depth=6, learning_rate=0.1,
                random_state=42, verbose=0
            )
        elif 'LightGBM' in best_model_name:
            trained_model = lgb.LGBMClassifier(
                n_estimators=100, max_depth=6, learning_rate=0.1,
                random_state=42, verbose=-1
            )
        elif 'RandomForest' in best_model_name:
            trained_model = RandomForestClassifier(
                n_estimators=100, max_depth=10,
                class_weight='balanced',
                random_state=42, n_jobs=-1
            )
        else:  # Linear models
            trained_model = LogisticRegression(
                penalty='elasticnet',
                l1_ratio=0.5,
                solver='saga',
                max_iter=5000,  # Increased to prevent convergence warnings
                class_weight='balanced',
                random_state=42
            )

        # Fit the model
        print(f"  Training {best_model_name}...")
        trained_model.fit(X_train_proc, y_train)

        # SHAP analysis
        print(f"  Computing SHAP values...")
        if is_tree_model:
            explainer = shap.TreeExplainer(
                trained_model,
                X_train_proc,
                feature_perturbation="interventional"
            )
            shap_vals = explainer.shap_values(X_test_proc, check_additivity=False)
        else:
            explainer = shap.LinearExplainer(trained_model, X_train_proc)
            shap_vals = explainer.shap_values(X_test_proc)

        # Create and save predictor-level plot
        plot_path = shap_dir / f"{physSx_var}_{best_model_name.replace('/', '_')}_shap_predictors.png"
        importance_info = create_shap_plot(
            shap_vals, X_test_proc, feature_names,
            symptom_name, best_model_name, plot_path
        )

        if importance_info is not None:
            symptom_shap_results[physSx_var] = {
                'symptom_name': symptom_name,
                'model': best_model_name,
                'roc_auc': best_roc_auc,
                'f1_score': best_f1,
                'feature_importance': importance_info['feature_level'],
                'predictor_importance': importance_info['predictor_level']
            }
            print(f"  ‚úì SHAP plot saved: {plot_path.name}")

            # Display top 15 predictors
            top_10 = importance_info['predictor_level'].tail(15)
            print(f"\n  ‚úÖ Top 15 Predictors for {symptom_name}:")
            for idx, row in enumerate(top_10.itertuples(), 1):
                print(f"     {idx}. {row.Predictor}: {row.Mean_Abs_SHAP:.4f}")

    except Exception as e:
        print(f"  ‚ö†Ô∏è  SHAP analysis failed: {e}")
        import traceback
        traceback.print_exc()

# ---------------------------------------------------------------------------
# Save SHAP results summary (predictor level)
# ---------------------------------------------------------------------------

print("\n" + "="*80)
print("Saving SHAP analysis summary (predictor level)...")

try:
    # Summary of top 15 predictors per symptom
    summary_data = []
    for symptom, data_obj in symptom_shap_results.items():
        symptom_name = data_obj['symptom_name']
        model_name = data_obj['model']
        roc_auc = data_obj['roc_auc']
        f1_score = data_obj['f1_score']
        predictor_imp = data_obj['predictor_importance']

        top_15 = predictor_imp.tail(15)
        for _, row in top_15.iterrows():
            summary_data.append({
                'Symptom_Code': symptom,
                'Symptom_Name': symptom_name,
                'Model': model_name,
                'ROC_AUC': roc_auc,
                'F1_Score': f1_score,
                'Predictor': row['Predictor'],
                'Mean_Abs_SHAP': row['Mean_Abs_SHAP']
            })

    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(shap_dir / "shap_top15_predictors_per_symptom_ROC_AUC.csv", index=False)
        print(f"  ‚úì Saved: shap_top15_predictors_per_symptom_ROC_AUC.csv")

        # Overall predictor importance across all symptoms
        overall_importance = (
            summary_df
            .groupby('Predictor')['Mean_Abs_SHAP']
            .agg(['mean', 'std', 'count'])
            .sort_values('mean', ascending=False)
        )
        overall_importance.to_csv(shap_dir / "shap_overall_predictor_importance_ROC_AUC.csv")
        print(f"  ‚úì Saved: shap_overall_predictor_importance_ROC_AUC.csv")

        print("\nüìä Top 15 Most Important Predictors Across All Symptoms:")
        print("-" * 80)
        for idx, (predictor, row) in enumerate(overall_importance.head(15).iterrows(), 1):
            print(f"{idx:2d}. {predictor:50s} | Mean: {row['mean']:.4f} | Std: {row['std']:.4f} | N: {int(row['count'])}")

except Exception as e:
    print(f"‚ö†Ô∏è  Error saving SHAP summaries: {e}")

print("\n" + "="*80)
print("‚úÖ SHAP ANALYSIS COMPLETE (PREDICTOR LEVEL - ROC_AUC SELECTION)!")
print("="*80)
print(f"\nüìÅ SHAP plots saved in: {shap_dir.absolute()}/")
print(f"‚úì Analyzed {len(symptom_shap_results)} symptoms successfully")

# ============================================================================
# SAVE RESULTS TO GOOGLE DRIVE
# ============================================================================
print("\n" + "="*80)
print("üì§ SAVING SHAP RESULTS TO GOOGLE DRIVE")
print("="*80)

try:
    from google.colab import drive
    import shutil

    # Mount Google Drive (if not already mounted)
    if not Path('/content/drive').exists():
        print("\nMounting Google Drive...")
        drive.mount('/content/drive')
        print("‚úì Google Drive mounted successfully!")
    else:
        print("\n‚úì Google Drive already mounted")

    # Define Google Drive paths
    gdrive_base = Path('/content/drive/My Drive/somatic-symptom/Result')
    gdrive_shap_results = gdrive_base / f"shap_analysis_ROC_AUC_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # Create directory structure in Google Drive
    print("\nCreating directory structure in Google Drive...")
    gdrive_shap_results.mkdir(parents=True, exist_ok=True)

    # Save CSV files
    print("\nSaving SHAP CSV files to Google Drive...")
    if (shap_dir / "shap_top15_predictors_per_symptom_ROC_AUC.csv").exists():
        shutil.copy(
            shap_dir / "shap_top15_predictors_per_symptom_ROC_AUC.csv",
            gdrive_shap_results / "shap_top15_predictors_per_symptom_ROC_AUC.csv"
        )
        print(f"  ‚úì Saved: shap_top15_predictors_per_symptom_ROC_AUC.csv")

    if (shap_dir / "shap_overall_predictor_importance_ROC_AUC.csv").exists():
        shutil.copy(
            shap_dir / "shap_overall_predictor_importance_ROC_AUC.csv",
            gdrive_shap_results / "shap_overall_predictor_importance_ROC_AUC.csv"
        )
        print(f"  ‚úì Saved: shap_overall_predictor_importance_ROC_AUC.csv")

    # Copy all SHAP plots
    print("\nCopying SHAP plots to Google Drive...")
    plot_count = 0
    for plot_file in shap_dir.glob("*.png"):
        shutil.copy(plot_file, gdrive_shap_results / plot_file.name)
        plot_count += 1
    print(f"  ‚úì Copied {plot_count} SHAP plots")

    # Save metadata
    print("\nSaving analysis metadata...")
    metadata = {
        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'selection_criterion': 'ROC_AUC',
        'num_symptoms_analyzed': len(symptom_shap_results),
        'symptoms': list(symptom_shap_results.keys()),
        'best_models': {k: v['model'] for k, v in symptom_shap_results.items()}
    }

    with open(gdrive_shap_results / "analysis_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"  ‚úì Saved: analysis_metadata.json")

    print("\n‚úÖ All SHAP results successfully saved to Google Drive!")
    print(f"   Location: /My Drive/somatic-symptom/Result/{gdrive_shap_results.name}")
    print(f"   Access at: https://drive.google.com/")

except ImportError:
    print("\n‚ö†Ô∏è  Not running in Google Colab environment")
    print("   Results are saved locally only")
    print(f"   Local location: {shap_dir.absolute()}")

except Exception as e:
    print(f"\n‚ö†Ô∏è  Error saving to Google Drive: {e}")
    print("   Results are saved locally but not uploaded to Google Drive")
    print(f"   Local location: {shap_dir.absolute()}")
    import traceback
    traceback.print_exc()

print("\n" + "="*80)
print("üéâ SHAP ANALYSIS COMPLETE!")
print("="*80)
print(f"\nüìÅ Local results: {shap_dir.absolute()}")
if 'gdrive_shap_results' in locals():
    print(f"‚òÅÔ∏è  Google Drive: {gdrive_shap_results}")
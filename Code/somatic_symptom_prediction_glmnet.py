# -*- coding: utf-8 -*-
"""somatic_symptom_prediction_GLMnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pr2XK0WkceuHSjDvjNuDsdDOUp2llTIo
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTRA-OPTIMIZED Somatic Symptom Prediction - Maximum Accuracy Mode
Target: 75-85% Balanced Accuracy

AGGRESSIVE IMPROVEMENTS:
- Deep Neural Networks with TabNet (optimized for B200 GPU)
- Advanced Ensemble: Stacking + Voting + Weighted Blending
- Aggressive Feature Engineering: 50+ engineered features
- Multiple Resampling Strategies per model
- Bayesian Hyperparameter Optimization (200+ trials)
- Cost-Sensitive Learning with dynamic weights
- Advanced Feature Selection with multiple methods
- Cross-Validation: Repeated Stratified 10-Fold
- Probability Calibration for all models
- Threshold Optimization per symptom
"""

import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ULTRA-AGGRESSIVE CONFIGURATION
# ============================================================================
print("="*80)
print("üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•")
print("="*80)

# üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY - TRY EVERYTHING!
ENABLE_ALL_FEATURES = True
ENABLE_DEEP_LEARNING = True
ENABLE_ADVANCED_ENSEMBLES = True
ENABLE_BAYESIAN_OPTIMIZATION = True
OPTUNA_TRIALS = 200  # INCREASED for deeper optimization
CV_FOLDS = 10  # INCREASED for more robust validation
CV_REPEATS = 3  # INCREASED for stability
FEATURE_SELECTION_K = 'all'  # USE ALL FEATURES (no selection = more info)
USE_GPU = True
FOCUS_TREE_MODELS_ONLY = True  # FALSE = Try ALL models (tree, linear, SVM, KNN, NB, DL, etc.)
ENABLE_ALL_MODEL_TYPES = False  # Enable SVM, KNN, NB, LDA, QDA, MLP, etc.
ENABLE_AUTOGLUON = True  # Enable AutoGluon automated ensemble (2h/symptom = 26h total)
AUTOGLUON_TIME_LIMIT = 3600 * 2  # 2 hours per symptom
AUTOGLUON_PRESET = 'best_quality'  # 'best_quality', 'high_quality', 'good_quality', 'medium_quality'

# Optimization target - CHANGED TO MAXIMIZE REGULAR ACCURACY (not balanced)
OPTIMIZE_FOR = 'accuracy'  # Options: 'accuracy', 'balanced_accuracy', 'f1', 'roc_auc'
# accuracy = pure accuracy optimization (allows natural class distribution)

# Advanced techniques
ENABLE_AUTO_ENCODER = True  # Dimensionality reduction with neural network
ENABLE_FOCAL_LOSS = True  # Focus on hard examples
ENABLE_LABEL_SMOOTHING = True  # Regularization technique
ENABLE_MIXUP = True  # Data augmentation for tabular data
ENABLE_PSEUDO_LABELING = False  # Semi-supervised learning (disabled - no unlabeled data)

print("\nüöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY")
print(f"   Optimization Target: {OPTIMIZE_FOR.upper()}")
print(f"   Optuna Trials: {OPTUNA_TRIALS}")
print(f"   CV: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats")
print(f"   Feature Selection: {FEATURE_SELECTION_K.upper() if FEATURE_SELECTION_K == 'all' else f'Top {FEATURE_SELECTION_K}'}")
print(f"   Tree Models Only: {FOCUS_TREE_MODELS_ONLY}")
print(f"   AutoGluon Ensemble: {'‚úÖ Enabled' if ENABLE_AUTOGLUON else '‚ùå Disabled'}")
if ENABLE_AUTOGLUON:
    print(f"      Time Limit: {AUTOGLUON_TIME_LIMIT/3600:.1f}h/symptom")
    print(f"      Preset: {AUTOGLUON_PRESET}")
print(f"   Sampling Strategy: NONE (natural distribution)")
print(f"   GPU Acceleration: {USE_GPU}")
print("="*80)

! pip install catboost
! pip install optuna
! pip install pytorch_tabnet

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import subprocess

from pathlib import Path
from scipy.stats import skew, kurtosis
from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                              ExtraTreesClassifier, StackingClassifier, VotingClassifier,
                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_selection import (SelectKBest, mutual_info_classif, f_classif,
                                       chi2, RFE, SelectFromModel, VarianceThreshold)
from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight
from sklearn.decomposition import PCA, TruncatedSVD
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks
from imblearn import FunctionSampler  # For passthrough (no resampling)
from sklearn.model_selection import GridSearchCV

# Gradient Boosting
import xgboost as xgb
import lightgbm as lgb
import shap  # For SHAP analysis
from catboost import CatBoostClassifier

# AutoGluon (optional)
if ENABLE_AUTOGLUON:
    try:
        from autogluon.tabular import TabularPredictor
        AUTOGLUON_AVAILABLE = True
    except ImportError:
        print("‚ö†Ô∏è  AutoGluon not installed. Install with: pip install autogluon")
        AUTOGLUON_AVAILABLE = False
        ENABLE_AUTOGLUON = False
else:
    AUTOGLUON_AVAILABLE = False

# Optuna for hyperparameter optimization
import optuna
optuna.logging.set_verbosity(optuna.logging.ERROR)

# PyTorch and TabNet
import torch
import torch.nn as nn
from pytorch_tabnet.tab_model import TabNetClassifier
from pytorch_tabnet.augmentations import ClassificationSMOTE

# Check GPU
if USE_GPU and torch.cuda.is_available():
    device = 'cuda'
    print(f"\n‚úÖ GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1024**3:.0f}GB)")
else:
    device = 'cpu'
    USE_GPU = False
    print("\n‚ö†Ô∏è  Using CPU (no GPU available)")

# Metrics
from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix,
                            accuracy_score, balanced_accuracy_score, f1_score,
                            precision_score, recall_score, matthews_corrcoef)

# Model serialization
import joblib
import pickle
import json
from datetime import datetime

np.random.seed(42)
torch.manual_seed(42)

# ============================================================================
# 1. ULTRA-AGGRESSIVE DATA LOADING & PREPROCESSING
# ============================================================================
print("\n[1/10] Loading data with ultra-aggressive preprocessing...")

repo_path = Path("somatic-symptom")
if not repo_path.exists():
    print("Cloning repository from GitHub...")
    subprocess.run([
        "git", "clone",
        "https://J-Tanchone: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@github.com/J-Tanchone/somatic-symptom.git"
    ])

data_path = "somatic-symptom/EAMMi2-Data1/EAMMi2-Data1.2.xlsx"
data = pd.read_excel(data_path, sheet_name="EAMMi2_Data")

# Recode variables
data['sibling_c'] = data['sibling'].apply(lambda x: -0.5 if x == 1 else 0.5)
data = data.rename(columns={'marriage2': 'marriage_importance', 'marriage5': 'parental_marriage'})
data = pd.concat([data, pd.get_dummies(data['parental_marriage'].astype('category'),
                                      prefix='parental_marriage', drop_first=True)], axis=1)

# Compute all psychological scales
scales = {
    'idea_m': [f'IDEA_{i}' for i in range(1, 9)],
    'moa_achievement_m': [f'moa1#2_{i}' for i in range(1, 11)] + [f'moa2#1_{i}' for i in range(1, 11)],
    'moa_importance_m': [f'moa2#1_{i}' for i in range(1, 11)] + [f'moa2#2_{i}' for i in range(1, 11)],
    'stress_m': [f'stress_{i}' for i in range(1, 11)],
    'support_m': [f'support_{i}' for i in range(1, 13)],
    'belong_m': [f'belong_{i}' for i in range(1, 11)],
    'mindful_m': [f'mindful_{i}' for i in range(1, 16)],
    'efficacy_m': [f'efficacy_{i}' for i in range(1, 11)],
    'exploit_m': [f'exploit_{i}' for i in range(1, 4)],
    'disability_m': [f'Q10_{i}' for i in range(1, 16)] + ['Q11'] + [f'Q14_{i}' for i in range(1, 7)],
    'social_conn_m': [f'SocMedia_{i}' for i in range(1, 6)],
    'social_new_m': [f'SocMedia_{i}' for i in range(6, 10)],
    'social_info_m': [f'SocMedia_{i}' for i in range(10, 12)],
    'swb_m': [f'swb_{i}' for i in range(1, 7)],
    'transgres_m': [f'transgres_{i}' for i in range(1, 5)],
    'usdream_m': ['usdream_1', 'usdream_2']
}

for name, items in scales.items():
    valid_items = [item for item in items if item in data.columns]
    if valid_items:
        data[name] = data[valid_items].mean(axis=1, skipna=True)
        # Add variance and std as additional features
        data[f'{name}_std'] = data[valid_items].std(axis=1, skipna=True)
        data[f'{name}_max'] = data[valid_items].max(axis=1, skipna=True)
        data[f'{name}_min'] = data[valid_items].min(axis=1, skipna=True)

print(f"‚úì Created {len(scales)*4} scale features (mean, std, max, min)")

# ============================================================================
# 2. ULTRA-AGGRESSIVE FEATURE ENGINEERING (50+ Features)
# ============================================================================
print("\n[2/10] Creating 50+ engineered features...")

feature_count = 0

# Level 1: Basic Interactions (Psychology Literature)
if 'stress_m' in data.columns and 'support_m' in data.columns:
    data['stress_support_interaction'] = data['stress_m'] * (1 - data['support_m'])
    data['stress_support_ratio'] = data['stress_m'] / (data['support_m'] + 0.01)
    feature_count += 2

if 'mindful_m' in data.columns and 'stress_m' in data.columns:
    data['mindful_stress_buffer'] = data['mindful_m'] * (1 - data['stress_m'])
    data['mindfulness_deficit'] = (1 - data['mindful_m']) * data['stress_m']
    feature_count += 2

if 'efficacy_m' in data.columns:
    data['efficacy_stress_interaction'] = data['efficacy_m'] * data['stress_m']
    data['efficacy_belong_interaction'] = data['efficacy_m'] * data['belong_m']
    data['low_efficacy_high_stress'] = (1 - data['efficacy_m']) * data['stress_m']
    feature_count += 3

# Level 2: Composite Indices
if all(col in data.columns for col in ['stress_m', 'support_m', 'belong_m', 'efficacy_m']):
    # Psychological Distress Composite
    data['psychological_distress'] = (
        data['stress_m'] * 0.40 +
        (1 - data['support_m']) * 0.30 +
        (1 - data['belong_m']) * 0.20 +
        (1 - data['efficacy_m']) * 0.10
    )

    # Protective Factors Composite
    data['protective_factors'] = (
        data['mindful_m'] * 0.35 +
        data['support_m'] * 0.30 +
        data['belong_m'] * 0.20 +
        data['efficacy_m'] * 0.15
    )

    # Risk-Protection Balance
    data['risk_protection_ratio'] = data['psychological_distress'] / (data['protective_factors'] + 0.01)
    data['risk_protection_difference'] = data['psychological_distress'] - data['protective_factors']
    feature_count += 4

# Level 3: Domain Expert Features (Somatization Literature)
if all(col in data.columns for col in ['stress_m', 'mindful_m', 'swb_m']):
    # Alexithymia proxy (emotional awareness deficit)
    data['emotional_awareness_deficit'] = data['stress_m'] * (1 - data['mindful_m']) * (1 - data['swb_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'efficacy_m' in data.columns:
    # Catastrophizing tendency
    data['catastrophizing_score'] = (data['stress_m'] ** 2) / (data['efficacy_m'] + 0.01)
    feature_count += 1

if all(col in data.columns for col in ['support_m', 'belong_m', 'social_conn_m']):
    # Social isolation index
    data['social_isolation'] = (1 - data['support_m']) * (1 - data['belong_m']) * (1 - data['social_conn_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'mindful_m' in data.columns:
    # Rumination tendency
    data['rumination_tendency'] = data['stress_m'] * (1 - data['mindful_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'disability_m' in data.columns:
    # Health anxiety proxy
    data['health_anxiety_proxy'] = data['stress_m'] * data['disability_m']
    feature_count += 1

# Level 4: Achievement-Stress Interactions
if 'moa_achievement_m' in data.columns and 'moa_importance_m' in data.columns:
    data['achievement_gap'] = data['moa_importance_m'] - data['moa_achievement_m']
    data['achievement_gap_squared'] = data['achievement_gap'] ** 2
    data['achievement_stress_interaction'] = data['achievement_gap'] * data['stress_m']
    data['perfectionism_stress'] = (data['achievement_gap'] ** 2) * data['stress_m']
    feature_count += 4

# Level 5: Non-linear Transformations
numerical_cols = ['stress_m', 'support_m', 'mindful_m', 'efficacy_m', 'belong_m', 'swb_m']
for col in numerical_cols:
    if col in data.columns:
        data[f'{col}_squared'] = data[col] ** 2
        data[f'{col}_sqrt'] = np.sqrt(data[col])
        data[f'{col}_log'] = np.log1p(data[col])  # log(1+x) to handle zeros
        feature_count += 3

# Level 6: Statistical Aggregations Across Scales
scale_means = [col for col in data.columns if col.endswith('_m')]
if len(scale_means) >= 3:
    data['overall_mean'] = data[scale_means].mean(axis=1)
    data['overall_std'] = data[scale_means].std(axis=1)
    data['overall_skew'] = data[scale_means].apply(lambda x: skew(x.dropna()), axis=1)
    data['overall_kurtosis'] = data[scale_means].apply(lambda x: kurtosis(x.dropna()), axis=1)
    feature_count += 4

# Level 7: Somatization Proneness Index (Multi-component)
if all(col in data.columns for col in ['stress_m', 'support_m', 'mindful_m', 'swb_m', 'efficacy_m']):
    data['somatization_proneness'] = (
        data['stress_m'] * 0.30 +
        (1 - data['support_m']) * 0.20 +
        (1 - data['mindful_m']) * 0.20 +
        (1 - data['swb_m']) * 0.15 +
        (1 - data['efficacy_m']) * 0.15
    )
    feature_count += 1

# Level 8: Demographic Interactions
if 'sex' in data.columns and 'stress_m' in data.columns:
    data['sex_stress'] = data['sex'].astype(float) * data['stress_m']
    feature_count += 1

if 'edu' in data.columns and 'efficacy_m' in data.columns:
    data['edu_efficacy'] = data['edu'].astype(float) * data['efficacy_m']
    feature_count += 1

if 'income' in data.columns and 'stress_m' in data.columns:
    data['income_stress'] = data['income'].astype(float) * data['stress_m']
    feature_count += 1

# Level 9: Resilience vs Vulnerability
if 'psychological_distress' in data.columns and 'protective_factors' in data.columns:
    data['resilience_index'] = data['protective_factors']
    data['vulnerability_index'] = data['psychological_distress']
    data['vulnerability_resilience_balance'] = data['vulnerability_index'] - data['resilience_index']
    data['stress_amplification'] = data['stress_m'] / (data['resilience_index'] + 0.1)
    feature_count += 4

# Level 10: Cross-domain Interactions
if all(col in data.columns for col in ['social_quality', 'disability_m']):
    data['social_disability_interaction'] = data['social_quality'] * data['disability_m']
    feature_count += 1

print(f"‚úì Created {feature_count} engineered features")

# Select final features
final_data = data[[col for col in data.columns if
                  col.endswith(('_m', '_c', '_index', '_ratio', '_balance', '_deficit',
                               '_interaction', '_score', '_isolation', '_tendency', '_proxy',
                               '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                               '_log', '_std', '_max', '_min', '_quality', '_gap')) or
                  col in ['marriage_importance', 'parental_marriage', 'overall_mean',
                          'overall_std', 'overall_skew', 'overall_kurtosis', 'social_quality'] or
                  col.startswith(('parental_marriage_', 'sex_', 'edu_', 'income_'))]]

# Add demographics
for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']:
    if col in data.columns and col not in final_data.columns:
        final_data[col] = data[col]

# Binarize outcomes
physSx_cols = [f'physSx_{i}' for i in range(1, 14)]
for col in physSx_cols:
    if col in data.columns:
        data[col] = data[col].replace({1: 0, 2: 1, 3: 1})
        final_data[col] = data[col]

print(f"‚úì Final dataset: {final_data.shape[0]} samples √ó {final_data.shape[1]} features")

# ============================================================================
# 3. TRAIN/TEST SPLITS WITH STRATIFICATION
# ============================================================================
print("\n[3/10] Creating stratified train/test splits...")

numeric_features = [col for col in final_data.columns if col.endswith(('_m', '_c', '_index', '_ratio',
                    '_balance', '_deficit', '_interaction', '_score', '_isolation', '_tendency',
                    '_proxy', '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                    '_log', '_std', '_max', '_min', '_gap', 'overall_mean', 'overall_std',
                    'overall_skew', 'overall_kurtosis')) and col not in physSx_cols]

categorical_features = [col for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']
                       if col in final_data.columns]

print(f"‚úì {len(numeric_features)} numeric features")
print(f"‚úì {len(categorical_features)} categorical features")

train_test_data = {}
X = final_data.drop(columns=physSx_cols, errors='ignore')

for physSx_var in physSx_cols:
    if physSx_var not in final_data.columns:
        continue

    y = data[physSx_var]
    model_data = X.join(y.rename(physSx_var)).dropna()
    X_clean = model_data.drop(columns=[physSx_var])
    y_clean = model_data[physSx_var]

    if y_clean.sum() < 10:
        continue

    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean
    )

    for col in categorical_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)

    train_test_data[physSx_var] = {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

    pos_pct = (y_clean.sum() / len(y_clean)) * 100
    print(f"  ‚úì {physSx_var}: {len(X_train)} train, {len(X_test)} test | {pos_pct:.1f}% positive")

# ============================================================================
# 4. ADVANCED PREPROCESSING PIPELINES
# ============================================================================
print("\n[4/10] Setting up advanced preprocessing pipelines...")

# Multiple preprocessors for different model types
preprocessor_scaled = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_robust = ColumnTransformer(transformers=[
    ('num', RobustScaler(), numeric_features),  # Better for outliers
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_power = ColumnTransformer(transformers=[
    ('num', PowerTransformer(method='yeo-johnson'), numeric_features),  # Normalize distributions
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_unscaled = ColumnTransformer(transformers=[
    ('num', 'passthrough', numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

print("‚úì Created 4 preprocessing strategies")

# # ============================================================================
# 5. LOGISTIC REGRESSION (GLMnet/Elastic Net) MODEL TRAINING
# ============================================================================
print(f"\n[5/10] Training Logistic Regression models (L1, L2, ElasticNet)...")
print(f"Cross-validation: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats")

all_results = []
cv = RepeatedStratifiedKFold(n_splits=CV_FOLDS, n_repeats=CV_REPEATS, random_state=42)

# Store trained models for SHAP analysis
trained_models = {}

for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):
    print(f"\n{'='*80}")
    print(f"[{symptom_idx}/{len(train_test_data)}] SYMPTOM: {physSx_var}")
    print(f"{'='*80}")

    # Initialize model storage for this symptom
    trained_models[physSx_var] = {}

    X_train = data_splits['X_train']
    X_test = data_splits['X_test']
    y_train = data_splits['y_train']
    y_test = data_splits['y_test']

    # Class imbalance handling
    pos_count = y_train.sum()
    neg_count = len(y_train) - pos_count
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    sample_weights = compute_sample_weight('balanced', y_train)

    print(f"Class balance: {neg_count} neg, {pos_count} pos")

    # NO resampling - natural class distribution
    sampler = FunctionSampler()
    print("  ‚ö° Using NATURAL class distribution (NO resampling)")

    # Feature preprocessing
    print("  Applying feature preprocessing...")
    X_train_proc = preprocessor_scaled.fit_transform(X_train)
    X_test_proc = preprocessor_scaled.transform(X_test)

    if FEATURE_SELECTION_K == 'all':
        X_train_selected = X_train_proc
        X_test_selected = X_test_proc
        print(f"    ‚ö° Using ALL {X_train_selected.shape[1]} features")
    else:
        # Mutual Information feature selection
        selector_mi = SelectKBest(mutual_info_classif, k=min(FEATURE_SELECTION_K, X_train_proc.shape[1]))
        X_train_selected = selector_mi.fit_transform(X_train_proc, y_train)
        X_test_selected = selector_mi.transform(X_test_proc)
        print(f"    ‚úì Selected {X_train_selected.shape[1]} features (from {X_train_proc.shape[1]})")

    # Update data
    X_train = pd.DataFrame(X_train_selected, index=X_train.index)
    X_test = pd.DataFrame(X_test_selected, index=X_test.index)

    # ========================================================================
    # LOGISTIC REGRESSION MODELS (L1, L2, ElasticNet)
    # ========================================================================

    lr_models = {
        'LR-L1': LogisticRegression(penalty='l1', solver='saga', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),
        'LR-L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),
        'LR-ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=2000, C=0.1, class_weight='balanced', random_state=42)
    }

    for name, model in lr_models.items():
        pipe = ImbPipeline([
            ('smote', sampler),
            ('clf', model)
        ])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        y_proba = pipe.predict_proba(X_test)[:, 1]

        bal_acc = balanced_accuracy_score(y_test, y_pred)
        accuracy = accuracy_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_proba)
        f1 = f1_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)

        all_results.append({
            'Symptom': physSx_var,
            'Model': name,
            'Accuracy': accuracy,
            'Balanced_Accuracy': bal_acc,
            'F1_Score': f1,
            'Precision': precision,
            'Recall': recall,
            'ROC_AUC': roc_auc
        })
        print(f"  {name:25s}: Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

        # Store trained model for SHAP analysis
        trained_models[physSx_var][name] = {
            'model': pipe,
            'preprocessor': preprocessor_scaled,
            'performance': {'bal_acc': bal_acc, 'roc_auc': roc_auc, 'f1': f1},
            'hyperparameters': {'C': 0.1, 'penalty': model.penalty}
        }

print(f"\n{'='*80}")
print("‚úÖ ALL LOGISTIC REGRESSION MODELS TRAINED")
print(f"{'='*80}")

# ============================================================================
# 5.5 IMPORT PREVIOUS RESULTS FROM GITHUB REPOSITORY
# ============================================================================
print("\nüì• Importing previous results from GitHub repository...")
results_df = pd.DataFrame(all_results)
results_df = results_df.round(3)

try:
    # Define repository path
    repo_path = Path("somatic-symptom")
    results_path = "somatic-symptom/Result/all_results_noGLMnet.csv"

    # Read all_results_noGLMnet.csv
    previous_df = pd.read_csv(results_path)

    # Filter for specific models if needed (optional)
    # Uncomment the lines below if you only want certain models
    # models_to_import = ['LR-ElasticNet', 'LR-LASSO', 'LR-Ridge', 'XGBoost', 'RandomForest']
    # previous_df = previous_df[previous_df['Model'].isin(models_to_import)]

    print(f"  ‚úì Found {len(previous_df)} results from previous analysis")
    print(f"  Models: {previous_df['Model'].unique().tolist()}")

    # Rename columns to match results_df (if needed)
    column_mapping = {
        'AUC': 'ROC_AUC',
        'Macro_F1': 'F1_Score',
        'Bal_Acc': 'Balanced_Accuracy'
    }
    previous_df = previous_df.rename(columns=column_mapping)

    # Keep only required columns
    required_cols = ['Symptom', 'Model', 'Accuracy', 'Balanced_Accuracy',
                     'F1_Score', 'Precision', 'Recall', 'ROC_AUC']

    # Only keep columns that exist in both dataframes
    available_cols = [col for col in required_cols if col in previous_df.columns]
    previous_df = previous_df[available_cols]

    # Add to results_df
    results_df = pd.concat([results_df, previous_df], ignore_index=True)

    print(f"‚úì Added {len(previous_df)} previous results | Total: {len(results_df)}")

    # Show summary by model type
    print("\n  Summary by Model:")
    model_counts = previous_df['Model'].value_counts()
    for model, count in model_counts.items():
        print(f"    {model}: {count} results")

except FileNotFoundError:
    print(f"‚ö†Ô∏è  File not found: {results_path}")
    print(f"     Make sure the repository is cloned and the file exists")
except Exception as e:
    print(f"‚ö†Ô∏è  Could not import previous results: {e}")
    import traceback
    traceback.print_exc()

print(f"\nüìä Final dataset: {len(results_df)} total results")
print(f"   Current GLMnet results: {len(all_results)}")
print(f"   Models in combined dataset: {results_df['Model'].nunique()}")

# ============================================================================
# 6. RESULTS ANALYSIS
# ============================================================================
print("\n[6/10] Analyzing results...")

# Create symptom name mapping
symptom_names = {
    'physSx_1': 'Stomach pain',
    'physSx_2': 'Back pain',
    'physSx_3': 'Limb/joint pain',
    'physSx_4': 'Headache',
    'physSx_5': 'Chest pain',
    'physSx_6': 'Dizziness',
    'physSx_7': 'Fainting spells',
    'physSx_8': 'Heart pound/race',
    'physSx_9': 'Shortness of breath',
    'physSx_10': 'Constipation',
    'physSx_11': 'Nausea/gas/indigestion',
    'physSx_12': 'Fatigue',
    'physSx_13': 'Trouble sleeping'
}

# Map symptom codes to names
results_df['Symptom_Name'] = results_df['Symptom'].map(symptom_names)

print("\nüìä BEST MODEL PER SYMPTOM:")
print("="*80)
best_per_symptom = results_df.loc[results_df.groupby('Symptom')['ROC_AUC'].idxmax()]
best_per_symptom = best_per_symptom.sort_values('ROC_AUC', ascending=False)
print(best_per_symptom[['Symptom_Name', 'Model', 'Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].to_string(index=False))

print("\n\nüìà AVERAGE PERFORMANCE BY MODEL:")
print("="*80)
model_avg = results_df.groupby('Model')[['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].mean()
model_avg = model_avg.round(3).sort_values('F1_Score', ascending=False)
print(model_avg.to_string())

print("\n\nüèÜ TOP PERFORMING MODEL:")
print("="*80)
best_model = model_avg.index[0]
best_score = model_avg.iloc[0]['ROC_AUC']
print(f"Model: {best_model}")
print(f"Average Balanced Accuracy: {model_avg.iloc[0]['Balanced_Accuracy']:.1%}")
print(f"Average ROC-AUC: {model_avg.iloc[0]['ROC_AUC']:.1%}")
print(f"Average F1-Score: {model_avg.iloc[0]['F1_Score']:.1%}")

# Success metrics
success_rate_60 = (results_df['ROC_AUC'] >= 0.60).mean()
success_rate_70 = (results_df['ROC_AUC'] >= 0.70).mean()
success_rate_75 = (results_df['ROC_AUC'] >= 0.75).mean()

print(f"\nüìä PERFORMANCE DISTRIBUTION:")
print(f"   Models with ROC_AUC ‚â• 60%: {success_rate_60:.1%}")
print(f"   Models with ROC_AUC ‚â• 70%: {success_rate_70:.1%}")
print(f"   Models with ROC_AUC ‚â• 75%: {success_rate_75:.1%}")

# ============================================================================
# 7. SAVE RESULTS
# ============================================================================
print("\n[7/10] Saving results...")

output_dir = Path("results_ultra_optimized")
output_dir.mkdir(exist_ok=True)

results_df.to_csv(output_dir / "all_results.csv", index=False)
best_per_symptom.to_csv(output_dir / "best_per_symptom.csv", index=False)
model_avg.to_csv(output_dir / "model_averages.csv")

print(f"‚úì Results saved to: {output_dir.absolute()}")

# ============================================================================
# 7.5. SAVE TRAINED MODELS & CREATE COMPARISON VISUALIZATIONS
# ============================================================================
print("\n[7.5/10] Saving trained models and creating comparison visualizations...")

# Create models directory
models_dir = output_dir / "trained_models"
models_dir.mkdir(exist_ok=True)

# Create visualization directory
viz_dir = output_dir / "visualizations"
viz_dir.mkdir(exist_ok=True)

# Save best model for each symptom
print("\nüì¶ Saving best models per symptom...")
best_models_info = []

# Get unique symptoms from results_df
unique_symptoms = results_df['Symptom'].unique()

for symptom in unique_symptoms:
    # Filter results for this symptom
    symptom_results = results_df[results_df['Symptom'] == symptom].copy()

    # Find the best model based on F1_Score
    best_idx = symptom_results['F1_Score'].idxmax()
    best_model_row = symptom_results.loc[best_idx]
    best_model_name = best_model_row['Model']
    symptom_name = symptom_names.get(symptom, symptom)  # Get readable name

    # Create symptom-specific directory (use code for folder name)
    symptom_dir = models_dir / symptom
    symptom_dir.mkdir(exist_ok=True)

    # Save model info
    model_info = {
        'symptom_code': symptom,
        'symptom_name': symptom_name,
        'best_model': best_model_name,
        'balanced_accuracy': float(best_model_row['Balanced_Accuracy']),
        'roc_auc': float(best_model_row['ROC_AUC']),
        'f1_score': float(best_model_row['F1_Score']),
        'timestamp': datetime.now().isoformat()
    }

    # Check if this model was trained in this session (available in trained_models)
    if symptom in trained_models and best_model_name in trained_models[symptom]:
        try:
            model_data = trained_models[symptom][best_model_name]

            # Save the trained pipeline
            joblib.dump(model_data['model'], symptom_dir / f"{symptom}_best_model.joblib")

            # Save preprocessor
            joblib.dump(model_data['preprocessor'], symptom_dir / f"{symptom}_preprocessor.joblib")

            # Save model info
            with open(symptom_dir / f"{symptom}_model_info.json", 'w') as f:
                json.dump({
                    **model_info,
                    'hyperparameters': model_data['hyperparameters']
                }, f, indent=2)

            best_models_info.append(model_info)
            print(f"  ‚úì {symptom_name:30s}: {best_model_name:20s} (F1={best_model_row['F1_Score']:.3f}) - SAVED")

        except Exception as e:
            print(f"  ‚ö†Ô∏è  {symptom_name:30s}: {best_model_name:20s} - Failed to save: {str(e)[:40]}")
    else:
        # Best model is from imported results (not trained in this session)
        # Save only the metadata
        with open(symptom_dir / f"{symptom}_model_info.json", 'w') as f:
            json.dump(model_info, f, indent=2)

        best_models_info.append(model_info)
        print(f"  ‚ÑπÔ∏è  {symptom_name:30s}: {best_model_name:20s} (F1={best_model_row['F1_Score']:.3f}) - Imported (no .joblib)")

# Save summary of best models
with open(models_dir / "best_models_summary.json", 'w') as f:
    json.dump(best_models_info, f, indent=2)

print(f"\n‚úì Processed {len(best_models_info)} symptoms")
print(f"  Models saved to: {models_dir.absolute()}")

# Show summary of which model types won
print("\nüìä Best Model Summary:")
best_models_df = pd.DataFrame(best_models_info)
model_wins = best_models_df['best_model'].value_counts()
for model, count in model_wins.items():
    print(f"  {model:20s}: {count:2d} symptoms ({count/len(best_models_df)*100:.1f}%)")

# ============================================================================
# CREATE MODEL COMPARISON VISUALIZATIONS
# ============================================================================
print("\nüìä Creating model comparison visualizations...")

# 1. Model Performance Comparison (Bar Plot)
print("  Creating model performance comparison chart...")
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

metrics = ['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']
titles = ['Balanced Accuracy', 'ROC-AUC', 'F1-Score']
colors = ['#2ecc71', '#3498db', '#e74c3c']

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    model_scores = results_df.groupby('Model')[metric].mean().sort_values(ascending=True)

    axes[idx].barh(range(len(model_scores)), model_scores.values, color=color, alpha=0.7)
    axes[idx].set_yticks(range(len(model_scores)))
    axes[idx].set_yticklabels(model_scores.index, fontsize=9)
    axes[idx].set_xlabel(f'Average {title}', fontsize=11, fontweight='bold')
    axes[idx].set_title(f'{title} by Model', fontsize=12, fontweight='bold')
    axes[idx].grid(axis='x', alpha=0.3)
    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
    axes[idx].legend(fontsize=8)

plt.suptitle('Model Performance Comparison Across All Symptoms', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig(viz_dir / "model_performance_comparison.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    ‚úì Saved: model_performance_comparison.png")

# 2. Symptom-wise Performance Heatmap
print("  Creating symptom-wise performance heatmap...")
pivot_data = results_df.pivot_table(index='Model', columns='Symptom', values='ROC_AUC', aggfunc='first')

# Rename columns to symptom names
pivot_data.columns = [symptom_names.get(col, col) for col in pivot_data.columns]

fig, ax = plt.subplots(figsize=(16, 10))
sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', center=0.6,
            vmin=0.5, vmax=0.8, cbar_kws={'label': 'ROC_AUC'},
            linewidths=0.5, ax=ax)
ax.set_title('Model Performance Heatmap: ROC_AUC per Symptom',
             fontsize=14, fontweight='bold', pad=20)
ax.set_xlabel('Symptom', fontsize=12, fontweight='bold')
ax.set_ylabel('Model', fontsize=12, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig(viz_dir / "symptom_model_heatmap.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    ‚úì Saved: symptom_model_heatmap.png")

# 3. Best Model Distribution (Pie Chart)
print("  Creating best model distribution chart...")
best_model_counts = best_per_symptom['Model'].value_counts()

fig, ax = plt.subplots(figsize=(10, 8))
wedges, texts, autotexts = ax.pie(best_model_counts.values, labels=best_model_counts.index,
                                    autopct='%1.1f%%', startangle=90,
                                    colors=plt.cm.Set3(range(len(best_model_counts))))
ax.set_title('Distribution of Best Models Across Symptoms', fontsize=14, fontweight='bold', pad=20)

# Add legend with counts
legend_labels = [f"{model}: {count} symptoms" for model, count in best_model_counts.items()]
ax.legend(legend_labels, loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)

plt.setp(autotexts, size=10, weight="bold", color='white')
plt.tight_layout()
plt.savefig(viz_dir / "best_model_distribution.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    ‚úì Saved: best_model_distribution.png")

# 4. Performance Distribution Box Plot
print("  Creating performance distribution box plot...")
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    # Get top 10 models by average performance
    top_models = results_df.groupby('Model')[metric].mean().nlargest(10).index
    plot_data = results_df[results_df['Model'].isin(top_models)]

    sns.boxplot(data=plot_data, y='Model', x=metric, ax=axes[idx],
                palette='Set2', order=top_models)
    axes[idx].set_xlabel(title, fontsize=11, fontweight='bold')
    axes[idx].set_ylabel('Model', fontsize=11, fontweight='bold')
    axes[idx].set_title(f'{title} Distribution (Top 10 Models)', fontsize=12, fontweight='bold')
    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
    axes[idx].grid(axis='x', alpha=0.3)
    axes[idx].legend(fontsize=8)

plt.suptitle('Performance Distribution Across Symptoms', fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(viz_dir / "performance_distribution_boxplot.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    ‚úì Saved: performance_distribution_boxplot.png")

# 5. Model Ranking by Symptom
print("  Creating model ranking visualization...")
fig, ax = plt.subplots(figsize=(12, 8))

symptom_list = best_per_symptom['Symptom'].tolist()
symptom_name_list = [symptom_names.get(s, s) for s in symptom_list]
model_list = best_per_symptom['Model'].tolist()
scores = best_per_symptom['ROC_AUC'].tolist()

y_pos = range(len(symptom_name_list))
bars = ax.barh(y_pos, scores, color=plt.cm.viridis(np.array(scores)))

ax.set_yticks(y_pos)
ax.set_yticklabels([f"{name} ({m})" for name, m in zip(symptom_name_list, model_list)], fontsize=9)
ax.set_xlabel('ROC_AUC', fontsize=12, fontweight='bold')
ax.set_title('Best Model Performance per Symptom', fontsize=14, fontweight='bold', pad=20)
ax.axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
ax.grid(axis='x', alpha=0.3)
ax.legend(fontsize=10)

# Add value labels on bars
for i, (bar, score) in enumerate(zip(bars, scores)):
    ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=8)

plt.tight_layout()
plt.savefig(viz_dir / "best_model_ranking.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    ‚úì Saved: best_model_ranking.png")

# 6. Metric Correlation Plot
print("  Creating metric correlation plot...")
if 'Accuracy' in results_df.columns:
    fig, ax = plt.subplots(figsize=(8, 6))

    scatter = ax.scatter(results_df['Balanced_Accuracy'], results_df['ROC_AUC'],
                        c=results_df['F1_Score'], s=100, cmap='viridis',
                        alpha=0.6, edgecolors='black', linewidth=0.5)

    ax.set_xlabel('Balanced Accuracy', fontsize=12, fontweight='bold')
    ax.set_ylabel('ROC-AUC', fontsize=12, fontweight='bold')
    ax.set_title('Metric Correlation: Balanced Accuracy vs ROC-AUC\n(Color = F1-Score)',
                fontsize=14, fontweight='bold', pad=20)

    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('F1-Score', fontsize=11, fontweight='bold')

    ax.grid(alpha=0.3)
    ax.axhline(0.7, color='red', linestyle='--', alpha=0.3)
    ax.axvline(0.7, color='red', linestyle='--', alpha=0.3)

    plt.tight_layout()
    plt.savefig(viz_dir / "metric_correlation.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    ‚úì Saved: metric_correlation.png")

print(f"\n‚úì All visualizations saved to: {viz_dir.absolute()}")

print("\n" + "="*80)
print("üéâ ULTRA-OPTIMIZED ANALYSIS COMPLETE!")
print("="*80)
print(f"\nüî• BEST AVERAGE ROC-AUC: {best_score:.1%}")
print(f"üî• BEST MODEL: {best_model}")
print(f"üî• SUCCESS RATE (‚â•70%): {success_rate_70:.1%}")
print(f"\nüìÅ Results: {output_dir.absolute()}/")
print(f"üìÅ Models: {models_dir.absolute()}/")
print(f"üìÅ Visualizations: {viz_dir.absolute()}/")

# ============================================================================
# 9. SAVE RESULTS TO GOOGLE DRIVE
# ============================================================================
print("\n" + "="*80)
print("üì§ SAVING RESULTS TO GOOGLE DRIVE")
print("="*80)

try:
    from google.colab import drive
    import shutil

    # Mount Google Drive
    print("\nMounting Google Drive...")
    drive.mount('/content/drive')
    print("‚úì Google Drive mounted successfully!")

    # Define Google Drive paths
    gdrive_base = Path('/content/drive/My Drive/somatic-symptom/Result')
    gdrive_results = gdrive_base / f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # Create directory structure in Google Drive
    print("\nCreating directory structure in Google Drive...")
    gdrive_results.mkdir(parents=True, exist_ok=True)

    # Save CSV files
    print("\nSaving CSV files to Google Drive...")
    results_df.to_csv(gdrive_results / "all_results.csv", index=False)
    print(f"  ‚úì Saved: all_results.csv")

    best_per_symptom.to_csv(gdrive_results / "best_per_symptom.csv", index=False)
    print(f"  ‚úì Saved: best_per_symptom.csv")

    model_avg.to_csv(gdrive_results / "model_averages.csv")
    print(f"  ‚úì Saved: model_averages.csv")

    # Copy trained models directory
    if (output_dir / "trained_models").exists():
        print("\nCopying trained models to Google Drive...")
        shutil.copytree(output_dir / "trained_models", gdrive_results / "trained_models")
        print(f"  ‚úì Copied: trained_models/")

    # Copy visualizations directory
    if (output_dir / "visualizations").exists():
        print("\nCopying visualizations to Google Drive...")
        shutil.copytree(output_dir / "visualizations", gdrive_results / "visualizations")
        print(f"  ‚úì Copied: visualizations/")

    # Copy SHAP analysis directory
    if (output_dir / "shap_analysis").exists():
        print("\nCopying SHAP analysis to Google Drive...")
        shutil.copytree(output_dir / "shap_analysis", gdrive_results / "shap_analysis")
        print(f"  ‚úì Copied: shap_analysis/")

    print("\n‚úÖ All results successfully saved to Google Drive!")
    print(f"   Location: /My Drive/somatic-symptom/Result/{gdrive_results.name}")
    print(f"   Access at: https://drive.google.com/")

except ImportError:
    print("\n‚ö†Ô∏è  Not running in Google Colab environment")
    print("   Results are saved locally only")
except Exception as e:
    print(f"\n‚ö†Ô∏è  Error saving to Google Drive: {e}")
    print("   Results are saved locally but not uploaded to Google Drive")

print("\n" + "="*80)
print("üéâ ALL DONE!")
print("="*80)
print(f"\nüìÅ Local results: {output_dir.absolute()}")
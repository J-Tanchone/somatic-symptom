{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOstQpkSgDbdNzJO7VfahaF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ============================================================================\n","# ULTRA-AGGRESSIVE CONFIGURATION\n","# ============================================================================\n","print(\"=\"*80)\n","print(\"üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•\")\n","print(\"=\"*80)\n","\n","# üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY - TRY EVERYTHING!\n","ENABLE_ALL_FEATURES = True\n","ENABLE_DEEP_LEARNING = True\n","ENABLE_ADVANCED_ENSEMBLES = True\n","ENABLE_BAYESIAN_OPTIMIZATION = True\n","OPTUNA_TRIALS = 200  # INCREASED for deeper optimization\n","CV_FOLDS = 10  # INCREASED for more robust validation\n","CV_REPEATS = 3  # INCREASED for stability\n","FEATURE_SELECTION_K = 'all'  # USE ALL FEATURES (no selection = more info)\n","USE_GPU = True\n","FOCUS_TREE_MODELS_ONLY = True  # FALSE = Try ALL models (tree, linear, SVM, KNN, NB, DL, etc.)\n","ENABLE_ALL_MODEL_TYPES = False  # Enable SVM, KNN, NB, LDA, QDA, MLP, etc.\n","ENABLE_AUTOGLUON = True  # Enable AutoGluon automated ensemble (2h/symptom = 26h total)\n","AUTOGLUON_TIME_LIMIT = 3600 * 2  # 2 hours per symptom\n","AUTOGLUON_PRESET = 'best_quality'  # 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n","\n","# Optimization target - CHANGED TO MAXIMIZE REGULAR ACCURACY (not balanced)\n","OPTIMIZE_FOR = 'accuracy'  # Options: 'accuracy', 'balanced_accuracy', 'f1', 'roc_auc'\n","# accuracy = pure accuracy optimization (allows natural class distribution)\n","\n","# Advanced techniques\n","ENABLE_AUTO_ENCODER = True  # Dimensionality reduction with neural network\n","ENABLE_FOCAL_LOSS = True  # Focus on hard examples\n","ENABLE_LABEL_SMOOTHING = True  # Regularization technique\n","ENABLE_MIXUP = True  # Data augmentation for tabular data\n","ENABLE_PSEUDO_LABELING = False  # Semi-supervised learning (disabled - no unlabeled data)\n","\n","print(\"\\nüöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY\")\n","print(f\"   Optimization Target: {OPTIMIZE_FOR.upper()}\")\n","print(f\"   Optuna Trials: {OPTUNA_TRIALS}\")\n","print(f\"   CV: {CV_FOLDS}-fold √ó {CV_REPEATS} repeats\")\n","print(f\"   Feature Selection: {FEATURE_SELECTION_K.upper() if FEATURE_SELECTION_K == 'all' else f'Top {FEATURE_SELECTION_K}'}\")\n","print(f\"   Tree Models Only: {FOCUS_TREE_MODELS_ONLY}\")\n","print(f\"   AutoGluon Ensemble: {'‚úÖ Enabled' if ENABLE_AUTOGLUON else '‚ùå Disabled'}\")\n","if ENABLE_AUTOGLUON:\n","    print(f\"      Time Limit: {AUTOGLUON_TIME_LIMIT/3600:.1f}h/symptom\")\n","    print(f\"      Preset: {AUTOGLUON_PRESET}\")\n","print(f\"   Sampling Strategy: NONE (natural distribution)\")\n","print(f\"   GPU Acceleration: {USE_GPU}\")\n","print(\"=\"*80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrgSfN6nTfDR","executionInfo":{"status":"ok","timestamp":1763498954133,"user_tz":360,"elapsed":41,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}},"outputId":"2a2f30f4-81c7-4522-fae8-98c6ae364529"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","üî• ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST üî•\n","================================================================================\n","\n","üöÄ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY\n","   Optimization Target: ACCURACY\n","   Optuna Trials: 200\n","   CV: 10-fold √ó 3 repeats\n","   Feature Selection: ALL\n","   Tree Models Only: True\n","   AutoGluon Ensemble: ‚úÖ Enabled\n","      Time Limit: 2.0h/symptom\n","      Preset: best_quality\n","   Sampling Strategy: NONE (natural distribution)\n","   GPU Acceleration: True\n","================================================================================\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# IMPORTS\n","# ============================================================================\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import subprocess\n","\n","from pathlib import Path\n","from scipy.stats import skew, kurtosis\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n","from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n","                              ExtraTreesClassifier, StackingClassifier, VotingClassifier,\n","                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.feature_selection import (SelectKBest, mutual_info_classif, f_classif,\n","                                       chi2, RFE, SelectFromModel, VarianceThreshold)\n","from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n","from sklearn.decomposition import PCA, TruncatedSVD\n","from imblearn.pipeline import Pipeline as ImbPipeline\n","from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n","from imblearn import FunctionSampler  # For passthrough (no resampling)\n","from sklearn.model_selection import GridSearchCV\n","\n","# Gradient Boosting\n","import xgboost as xgb\n","import lightgbm as lgb\n","import shap  # For SHAP analysis\n","\n","\n","# Metrics\n","from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix,\n","                            accuracy_score, balanced_accuracy_score, f1_score,\n","                            precision_score, recall_score, matthews_corrcoef)\n","\n","# Model serialization\n","import joblib\n","import pickle\n","import json\n","from datetime import datetime\n","\n","np.random.seed(42)"],"metadata":{"id":"krWCrbBNPfhC","executionInfo":{"status":"ok","timestamp":1763498980939,"user_tz":360,"elapsed":26807,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# 1. ULTRA-AGGRESSIVE DATA LOADING & PREPROCESSING\n","# ============================================================================\n","print(\"\\n[1/10] Loading data with ultra-aggressive preprocessing...\")\n","\n","repo_path = Path(\"somatic-symptom\")\n","if not repo_path.exists():\n","    print(\"Cloning repository from GitHub...\")\n","    subprocess.run([\n","        \"git\", \"clone\",\n","        \"https://J-Tanchone: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@github.com/J-Tanchone/somatic-symptom.git\"\n","    ])\n","data = pd.read_excel(\"somatic-symptom/EAMMi2-Data1/EAMMi2-Data1.2.xlsx\", sheet_name=\"EAMMi2_Data\")\n","\n","# Recode variables\n","data['sibling_c'] = data['sibling'].apply(lambda x: -0.5 if x == 1 else 0.5)\n","data = data.rename(columns={'marriage2': 'marriage_importance', 'marriage5': 'parental_marriage'})\n","data = pd.concat([data, pd.get_dummies(data['parental_marriage'].astype('category'),\n","                                      prefix='parental_marriage', drop_first=True)], axis=1)\n","\n","# Compute all psychological scales\n","scales = {\n","    'idea_m': [f'IDEA_{i}' for i in range(1, 9)],\n","    'moa_achievement_m': [f'moa1#2_{i}' for i in range(1, 11)] + [f'moa2#1_{i}' for i in range(1, 11)],\n","    'moa_importance_m': [f'moa2#1_{i}' for i in range(1, 11)] + [f'moa2#2_{i}' for i in range(1, 11)],\n","    'stress_m': [f'stress_{i}' for i in range(1, 11)],\n","    'support_m': [f'support_{i}' for i in range(1, 13)],\n","    'belong_m': [f'belong_{i}' for i in range(1, 11)],\n","    'mindful_m': [f'mindful_{i}' for i in range(1, 16)],\n","    'efficacy_m': [f'efficacy_{i}' for i in range(1, 11)],\n","    'exploit_m': [f'exploit_{i}' for i in range(1, 4)],\n","    'disability_m': [f'Q10_{i}' for i in range(1, 16)] + ['Q11'] + [f'Q14_{i}' for i in range(1, 7)],\n","    'social_conn_m': [f'SocMedia_{i}' for i in range(1, 6)],\n","    'social_new_m': [f'SocMedia_{i}' for i in range(6, 10)],\n","    'social_info_m': [f'SocMedia_{i}' for i in range(10, 12)],\n","    'swb_m': [f'swb_{i}' for i in range(1, 7)],\n","    'transgres_m': [f'transgres_{i}' for i in range(1, 5)],\n","    'usdream_m': ['usdream_1', 'usdream_2']\n","}\n","\n","for name, items in scales.items():\n","    valid_items = [item for item in items if item in data.columns]\n","    if valid_items:\n","        data[name] = data[valid_items].mean(axis=1, skipna=True)\n","        # Add variance and std as additional features\n","        data[f'{name}_std'] = data[valid_items].std(axis=1, skipna=True)\n","        data[f'{name}_max'] = data[valid_items].max(axis=1, skipna=True)\n","        data[f'{name}_min'] = data[valid_items].min(axis=1, skipna=True)\n","\n","print(f\"‚úì Created {len(scales)*4} scale features (mean, std, max, min)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"DgRrdC91Rkk5","executionInfo":{"status":"error","timestamp":1763881323591,"user_tz":300,"elapsed":19,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}},"outputId":"4f1687fe-8637-44a0-ed3e-ec8a93879d67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[1/10] Loading data with ultra-aggressive preprocessing...\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'Path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-910162886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[1/10] Loading data with ultra-aggressive preprocessing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrepo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"somatic-symptom\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepo_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cloning repository from GitHub...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"]}]},{"cell_type":"code","source":["# ============================================================================\n","# 2. ULTRA-AGGRESSIVE FEATURE ENGINEERING (50+ Features)\n","# ============================================================================\n","print(\"\\n[2/10] Creating 50+ engineered features...\")\n","\n","feature_count = 0\n","\n","# Level 1: Basic Interactions (Psychology Literature)\n","if 'stress_m' in data.columns and 'support_m' in data.columns:\n","    data['stress_support_interaction'] = data['stress_m'] * (1 - data['support_m'])\n","    data['stress_support_ratio'] = data['stress_m'] / (data['support_m'] + 0.01)\n","    feature_count += 2\n","\n","if 'mindful_m' in data.columns and 'stress_m' in data.columns:\n","    data['mindful_stress_buffer'] = data['mindful_m'] * (1 - data['stress_m'])\n","    data['mindfulness_deficit'] = (1 - data['mindful_m']) * data['stress_m']\n","    feature_count += 2\n","\n","if 'efficacy_m' in data.columns:\n","    data['efficacy_stress_interaction'] = data['efficacy_m'] * data['stress_m']\n","    data['efficacy_belong_interaction'] = data['efficacy_m'] * data['belong_m']\n","    data['low_efficacy_high_stress'] = (1 - data['efficacy_m']) * data['stress_m']\n","    feature_count += 3\n","\n","# Level 2: Composite Indices\n","if all(col in data.columns for col in ['stress_m', 'support_m', 'belong_m', 'efficacy_m']):\n","    # Psychological Distress Composite\n","    data['psychological_distress'] = (\n","        data['stress_m'] * 0.40 +\n","        (1 - data['support_m']) * 0.30 +\n","        (1 - data['belong_m']) * 0.20 +\n","        (1 - data['efficacy_m']) * 0.10\n","    )\n","\n","    # Protective Factors Composite\n","    data['protective_factors'] = (\n","        data['mindful_m'] * 0.35 +\n","        data['support_m'] * 0.30 +\n","        data['belong_m'] * 0.20 +\n","        data['efficacy_m'] * 0.15\n","    )\n","\n","    # Risk-Protection Balance\n","    data['risk_protection_ratio'] = data['psychological_distress'] / (data['protective_factors'] + 0.01)\n","    data['risk_protection_difference'] = data['psychological_distress'] - data['protective_factors']\n","    feature_count += 4\n","\n","# Level 3: Domain Expert Features (Somatization Literature)\n","if all(col in data.columns for col in ['stress_m', 'mindful_m', 'swb_m']):\n","    # Alexithymia proxy (emotional awareness deficit)\n","    data['emotional_awareness_deficit'] = data['stress_m'] * (1 - data['mindful_m']) * (1 - data['swb_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'efficacy_m' in data.columns:\n","    # Catastrophizing tendency\n","    data['catastrophizing_score'] = (data['stress_m'] ** 2) / (data['efficacy_m'] + 0.01)\n","    feature_count += 1\n","\n","if all(col in data.columns for col in ['support_m', 'belong_m', 'social_conn_m']):\n","    # Social isolation index\n","    data['social_isolation'] = (1 - data['support_m']) * (1 - data['belong_m']) * (1 - data['social_conn_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'mindful_m' in data.columns:\n","    # Rumination tendency\n","    data['rumination_tendency'] = data['stress_m'] * (1 - data['mindful_m'])\n","    feature_count += 1\n","\n","if 'stress_m' in data.columns and 'disability_m' in data.columns:\n","    # Health anxiety proxy\n","    data['health_anxiety_proxy'] = data['stress_m'] * data['disability_m']\n","    feature_count += 1\n","\n","# Level 4: Achievement-Stress Interactions\n","if 'moa_achievement_m' in data.columns and 'moa_importance_m' in data.columns:\n","    data['achievement_gap'] = data['moa_importance_m'] - data['moa_achievement_m']\n","    data['achievement_gap_squared'] = data['achievement_gap'] ** 2\n","    data['achievement_stress_interaction'] = data['achievement_gap'] * data['stress_m']\n","    data['perfectionism_stress'] = (data['achievement_gap'] ** 2) * data['stress_m']\n","    feature_count += 4\n","\n","# Level 5: Non-linear Transformations\n","numerical_cols = ['stress_m', 'support_m', 'mindful_m', 'efficacy_m', 'belong_m', 'swb_m']\n","for col in numerical_cols:\n","    if col in data.columns:\n","        data[f'{col}_squared'] = data[col] ** 2\n","        data[f'{col}_sqrt'] = np.sqrt(data[col])\n","        data[f'{col}_log'] = np.log1p(data[col])  # log(1+x) to handle zeros\n","        feature_count += 3\n","\n","# Level 6: Statistical Aggregations Across Scales\n","scale_means = [col for col in data.columns if col.endswith('_m')]\n","if len(scale_means) >= 3:\n","    data['overall_mean'] = data[scale_means].mean(axis=1)\n","    data['overall_std'] = data[scale_means].std(axis=1)\n","    data['overall_skew'] = data[scale_means].apply(lambda x: skew(x.dropna()), axis=1)\n","    data['overall_kurtosis'] = data[scale_means].apply(lambda x: kurtosis(x.dropna()), axis=1)\n","    feature_count += 4\n","\n","# Level 7: Somatization Proneness Index (Multi-component)\n","if all(col in data.columns for col in ['stress_m', 'support_m', 'mindful_m', 'swb_m', 'efficacy_m']):\n","    data['somatization_proneness'] = (\n","        data['stress_m'] * 0.30 +\n","        (1 - data['support_m']) * 0.20 +\n","        (1 - data['mindful_m']) * 0.20 +\n","        (1 - data['swb_m']) * 0.15 +\n","        (1 - data['efficacy_m']) * 0.15\n","    )\n","    feature_count += 1\n","\n","# Level 8: Demographic Interactions\n","if 'sex' in data.columns and 'stress_m' in data.columns:\n","    data['sex_stress'] = data['sex'].astype(float) * data['stress_m']\n","    feature_count += 1\n","\n","if 'edu' in data.columns and 'efficacy_m' in data.columns:\n","    data['edu_efficacy'] = data['edu'].astype(float) * data['efficacy_m']\n","    feature_count += 1\n","\n","if 'income' in data.columns and 'stress_m' in data.columns:\n","    data['income_stress'] = data['income'].astype(float) * data['stress_m']\n","    feature_count += 1\n","\n","# Level 9: Resilience vs Vulnerability\n","if 'psychological_distress' in data.columns and 'protective_factors' in data.columns:\n","    data['resilience_index'] = data['protective_factors']\n","    data['vulnerability_index'] = data['psychological_distress']\n","    data['vulnerability_resilience_balance'] = data['vulnerability_index'] - data['resilience_index']\n","    data['stress_amplification'] = data['stress_m'] / (data['resilience_index'] + 0.1)\n","    feature_count += 4\n","\n","# Level 10: Cross-domain Interactions\n","if all(col in data.columns for col in ['social_quality', 'disability_m']):\n","    data['social_disability_interaction'] = data['social_quality'] * data['disability_m']\n","    feature_count += 1\n","\n","print(f\"‚úì Created {feature_count} engineered features\")\n","\n","# Select final features\n","final_data = data[[col for col in data.columns if\n","                  col.endswith(('_m', '_c', '_index', '_ratio', '_balance', '_deficit',\n","                               '_interaction', '_score', '_isolation', '_tendency', '_proxy',\n","                               '_stress', '_proneness', '_amplification', '_squared', '_sqrt',\n","                               '_log', '_std', '_max', '_min', '_quality', '_gap')) or\n","                  col in ['marriage_importance', 'parental_marriage', 'overall_mean',\n","                          'overall_std', 'overall_skew', 'overall_kurtosis', 'social_quality'] or\n","                  col.startswith(('parental_marriage_', 'sex_', 'edu_', 'income_'))]]\n","\n","# Add demographics\n","for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']:\n","    if col in data.columns and col not in final_data.columns:\n","        final_data[col] = data[col]\n","\n","# Binarize outcomes\n","physSx_cols = [f'physSx_{i}' for i in range(1, 14)]\n","for col in physSx_cols:\n","    if col in data.columns:\n","        data[col] = data[col].replace({1: 0, 2: 1, 3: 1})\n","        final_data[col] = data[col]\n","\n","print(f\"‚úì Final dataset: {final_data.shape[0]} samples √ó {final_data.shape[1]} features\")"],"metadata":{"id":"Cm8gOJGCR0is","executionInfo":{"status":"aborted","timestamp":1763499073900,"user_tz":360,"elapsed":119840,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# 3. TRAIN/TEST SPLITS WITH STRATIFICATION\n","# ============================================================================\n","print(\"\\n[3/10] Creating stratified train/test splits...\")\n","\n","numeric_features = [col for col in final_data.columns if col.endswith(('_m', '_c', '_index', '_ratio',\n","                    '_balance', '_deficit', '_interaction', '_score', '_isolation', '_tendency',\n","                    '_proxy', '_stress', '_proneness', '_amplification', '_squared', '_sqrt',\n","                    '_log', '_std', '_max', '_min', '_gap', 'overall_mean', 'overall_std',\n","                    'overall_skew', 'overall_kurtosis')) and col not in physSx_cols]\n","\n","categorical_features = [col for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']\n","                       if col in final_data.columns]\n","\n","print(f\"‚úì {len(numeric_features)} numeric features\")\n","print(f\"‚úì {len(categorical_features)} categorical features\")\n","\n","train_test_data = {}\n","X = final_data.drop(columns=physSx_cols, errors='ignore')\n","\n","for physSx_var in physSx_cols:\n","    if physSx_var not in final_data.columns:\n","        continue\n","\n","    y = data[physSx_var]\n","    model_data = X.join(y.rename(physSx_var)).dropna()\n","    X_clean = model_data.drop(columns=[physSx_var])\n","    y_clean = model_data[physSx_var]\n","\n","    if y_clean.sum() < 10:\n","        continue\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n","    )\n","\n","    for col in categorical_features:\n","        if col in X_train.columns:\n","            X_train[col] = X_train[col].astype(str)\n","            X_test[col] = X_test[col].astype(str)\n","\n","    train_test_data[physSx_var] = {\n","        'X_train': X_train,\n","        'X_test': X_test,\n","        'y_train': y_train,\n","        'y_test': y_test\n","    }\n","\n","    pos_pct = (y_clean.sum() / len(y_clean)) * 100\n","    print(f\"  ‚úì {physSx_var}: {len(X_train)} train, {len(X_test)} test | {pos_pct:.1f}% positive\")"],"metadata":{"id":"6PVgD4q0R4Rh","executionInfo":{"status":"aborted","timestamp":1763499073907,"user_tz":360,"elapsed":119844,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXh7nie_Ociu","executionInfo":{"status":"aborted","timestamp":1763499074252,"user_tz":360,"elapsed":120189,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"outputs":[],"source":["try:\n","    from google.colab import drive\n","\n","    # Mount if needed\n","    if not Path('/content/drive').exists():\n","        drive.mount('/content/drive')\n","\n","    # Read all_results_v1.csv\n","    path = '/content/drive/My Drive/somatic-symptom/Result/results_complete_ROC_AUC/all_results.csv'\n","    model = pd.read_csv(path)\n","except FileNotFoundError:\n","    print(f\"‚ö†Ô∏è  File not found: {path}\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Could not import Logistic Regression results: {e}\")"]},{"cell_type":"code","source":["# ============================================================================\n","# 4. ADVANCED PREPROCESSING PIPELINES\n","# ============================================================================\n","print(\"\\n[4/10] Setting up advanced preprocessing pipelines...\")\n","\n","# Multiple preprocessors for different model types\n","preprocessor_scaled = ColumnTransformer(transformers=[\n","    ('num', StandardScaler(), numeric_features),\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_robust = ColumnTransformer(transformers=[\n","    ('num', RobustScaler(), numeric_features),  # Better for outliers\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_power = ColumnTransformer(transformers=[\n","    ('num', PowerTransformer(method='yeo-johnson'), numeric_features),  # Normalize distributions\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","preprocessor_unscaled = ColumnTransformer(transformers=[\n","    ('num', 'passthrough', numeric_features),\n","    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n","])\n","\n","print(\"‚úì Created 4 preprocessing strategies\")\n"],"metadata":{"id":"x-gcShTcXgx4","executionInfo":{"status":"aborted","timestamp":1763499074486,"user_tz":360,"elapsed":120420,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# SHAP ANALYSIS - FEATURE IMPORTANCE FOR BEST MODELS (ORIGINAL PREDICTORS)\n","# Based on ROC_AUC Performance\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üî¨ SHAP ANALYSIS - BEST MODEL FEATURE IMPORTANCE (ORIGINAL PREDICTORS)\")\n","print(\"Selection Criterion: ROC_AUC\")\n","print(\"=\"*80)\n","print(\"\\nAnalyzing feature importance for the best model of each symptom...\")\n","\n","# Create SHAP output directory\n","try:\n","    from google.colab import drive\n","    if not Path('/content/drive').exists():\n","        drive.mount('/content/drive')\n","    shap_dir = Path('/content/drive/My Drive/somatic-symptom/Result/shap_analysis')\n","except:\n","    shap_dir = Path('shap_analysis')\n","\n","shap_dir.mkdir(parents=True, exist_ok=True)\n","print(f\"üìÅ SHAP plots will be saved to: {shap_dir.absolute()}\")\n","\n","# ---------------------------------------------------------------------------\n","# Define preprocessors with improved handling\n","# ---------------------------------------------------------------------------\n","\n","# Preprocessor for tree-based models (no scaling)\n","preprocessor_unscaled = ColumnTransformer(\n","    transformers=[\n","        ('num', 'passthrough', numeric_features),\n","        ('cat', OneHotEncoder(\n","            drop='first',\n","            sparse_output=False,\n","            handle_unknown='infrequent_if_exist',  # Better handling\n","            min_frequency=0.01\n","        ), categorical_features)\n","    ],\n","    remainder='drop'\n",")\n","\n","# Preprocessor for linear models (with scaling)\n","preprocessor_scaled = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numeric_features),\n","        ('cat', OneHotEncoder(\n","            drop='first',\n","            sparse_output=False,\n","            handle_unknown='infrequent_if_exist',  # Better handling\n","            min_frequency=0.01\n","        ), categorical_features)\n","    ],\n","    remainder='drop'\n",")\n","\n","# ---------------------------------------------------------------------------\n","# Map symptom codes to readable names\n","# ---------------------------------------------------------------------------\n","\n","symptom_names = {\n","    'physSx_1': 'Stomach pain',\n","    'physSx_2': 'Back pain',\n","    'physSx_3': 'Limb/joint pain',\n","    'physSx_4': 'Headache',\n","    'physSx_5': 'Chest pain',\n","    'physSx_6': 'Dizziness',\n","    'physSx_7': 'Fainting spells',\n","    'physSx_8': 'Heart pound/race',\n","    'physSx_9': 'Shortness of breath',\n","    'physSx_10': 'Constipation',\n","    'physSx_11': 'Nausea/gas/indigestion',\n","    'physSx_12': 'Fatigue',\n","    'physSx_13': 'Trouble sleeping'\n","}\n","\n","# ---------------------------------------------------------------------------\n","# Map raw feature names to original predictor names\n","# ---------------------------------------------------------------------------\n","\n","predictor_map = {\n","    'idea_m': \"Identity Exploration (IDEA-8)\",\n","    'moa_achievement_m': \"Markers of Adulthood - Achievement\",\n","    'moa_importance_m': \"Markers of Adulthood - Importance\",\n","    'stress_m': \"Perceived Stress\",\n","    'support_m': \"Perceived Social Support\",\n","    'belong_m': \"Need to Belong\",\n","    'mindful_m': \"Mindfulness\",\n","    'efficacy_m': \"Efficacy/Competence\",\n","    'npi_m': \"Narcissistic Personality (NPI-13)\",\n","    'exploit_m': \"Interpersonal Exploitativeness\",\n","    'disability_m': \"Disability Identity & Status\",\n","    'social_conn_m': \"Social Media - Maintaining Connections\",\n","    'social_new_m': \"Social Media - Making New Connections\",\n","    'social_info_m': \"Social Media - Information Seeking\",\n","    'swb_m': \"Subjective Well-Being\",\n","    'transgres_m': \"Interpersonal Transgressions\",\n","    'usdream_m': \"American Dream\",\n","    'sibling_c': \"Sibling Status\",\n","    'marriage_importance': \"Marriage Importance\",\n","    'parental_marriage': \"Parental Marriage Status\",\n","    'sex': \"Sex\",\n","    'edu': \"Education\",\n","    'race': \"Race\",\n","    'income': \"Income\"\n","}\n","\n","def feature_to_predictor(feat: str) -> str:\n","    \"\"\"\n","    Map a detailed feature name (including _std/_max/_min or log/sqrt/squared)\n","    back to its original predictor name. Combines all transformations into\n","    one predictor.\n","    \"\"\"\n","    # Strip common engineered suffixes\n","    suffixes = ['_std', '_max', '_min', '_log', '_sqrt', '_squared',\n","                '_interaction', '_ratio', '_balance', '_deficit', '_score',\n","                '_isolation', '_tendency', '_proxy', '_stress', '_proneness',\n","                '_amplification', '_gap', '_buffer', '_index', '_quality']\n","\n","    base = feat\n","    for s in suffixes:\n","        if base.endswith(s):\n","            base = base[: -len(s)]\n","            break\n","\n","    # Match to one of the original predictor names\n","    for prefix, label in predictor_map.items():\n","        if base.startswith(prefix) or base == prefix:\n","            return label\n","\n","    # Handle one-hot encoded categorical features\n","    for cat_feat in categorical_features:\n","        if feat.startswith(cat_feat):\n","            return predictor_map.get(cat_feat, cat_feat)\n","\n","    # Default: keep raw feature name\n","    return feat\n","\n","# ---------------------------------------------------------------------------\n","# Function to create SHAP plots at the PREDICTOR level\n","# ---------------------------------------------------------------------------\n","\n","def create_shap_plot(shap_values, X_data, feature_names, symptom_name, model_name, output_path):\n","    \"\"\"\n","    Create mean absolute SHAP value plot for the TOP 15 PREDICTORS.\n","    Returns both feature-level and predictor-level importance tables.\n","    \"\"\"\n","    try:\n","        # 1. Handle different SHAP formats (list vs array, multiclass, etc.)\n","        if isinstance(shap_values, list):\n","            # For binary/multiclass, pick the last class\n","            shap_vals = shap_values[-1]\n","        else:\n","            shap_vals = shap_values\n","\n","        # If 3D (n_samples x n_features x n_classes), take last class\n","        if shap_vals.ndim == 3:\n","            shap_vals = shap_vals[:, :, -1]\n","\n","        # 2. Mean abs SHAP per *raw feature*\n","        mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n","\n","        feature_importance_df = pd.DataFrame({\n","            'Feature': feature_names,\n","            'Mean_Abs_SHAP': mean_abs_shap\n","        })\n","\n","        # 3. Map each feature to its original predictor\n","        feature_importance_df['Predictor'] = feature_importance_df['Feature'].apply(feature_to_predictor)\n","\n","        # 4. Aggregate by predictor (sum of SHAP across all transformations)\n","        predictor_importance = (\n","            feature_importance_df\n","            .groupby('Predictor', as_index=False)['Mean_Abs_SHAP']\n","            .sum()\n","            .sort_values('Mean_Abs_SHAP', ascending=True)\n","        )\n","\n","        # 5. Plot top 15 predictors\n","        top_15 = predictor_importance.tail(15)\n","\n","        fig, ax = plt.subplots(figsize=(10, 8))\n","        colors = plt.cm.RdYlGn_r(top_15['Mean_Abs_SHAP'] / top_15['Mean_Abs_SHAP'].max())\n","        bars = ax.barh(range(len(top_15)), top_15['Mean_Abs_SHAP'])\n","\n","        for bar, color in zip(bars, colors):\n","            bar.set_color(color)\n","\n","        # Add numeric labels\n","        for i, (bar, val) in enumerate(zip(bars, top_15['Mean_Abs_SHAP'])):\n","            ax.text(val, i, f' {val:.3f}', va='center', fontsize=9)\n","\n","        ax.set_yticks(range(len(top_15)))\n","        ax.set_yticklabels(top_15['Predictor'], fontsize=10)\n","        ax.set_xlabel('Mean Absolute SHAP Value (Feature Importance)', fontsize=12)\n","        ax.set_title(\n","            f'{symptom_name} - {model_name}\\nTop 15 Most Important Predictors',\n","            fontsize=14,\n","            fontweight='bold'\n","        )\n","        ax.grid(axis='x', alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n","        plt.close()\n","\n","        # Return both: raw feature-level & predictor-level importance\n","        return {\n","            'feature_level': feature_importance_df.sort_values('Mean_Abs_SHAP', ascending=False),\n","            'predictor_level': predictor_importance\n","        }\n","\n","    except Exception as e:\n","        print(f\"    ‚ö†Ô∏è  Error creating SHAP plot: {e}\")\n","        return None\n","\n","# ---------------------------------------------------------------------------\n","# Run SHAP analysis for each symptom\n","# ---------------------------------------------------------------------------\n","\n","symptom_shap_results = {}\n","\n","for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):\n","    symptom_name = symptom_names.get(physSx_var, physSx_var)\n","    print(f\"\\n[{symptom_idx}/{len(train_test_data)}] {symptom_name} ({physSx_var})...\")\n","\n","    X_train = data_splits['X_train'].copy()\n","    X_test = data_splits['X_test'].copy()\n","    y_train = data_splits['y_train']\n","    y_test = data_splits['y_test']\n","\n","    # Ensure categoricals are strings\n","    for col in categorical_features:\n","        if col in X_train.columns:\n","            X_train[col] = X_train[col].astype(str)\n","            X_test[col] = X_test[col].astype(str)\n","\n","    # Find best model for this symptom from imported CSV - BASED ON ROC_AUC\n","    symptom_results = model[model['Symptom'] == physSx_var]\n","    if len(symptom_results) == 0:\n","        print(f\"  ‚ö†Ô∏è  No results found for {physSx_var}\")\n","        continue\n","\n","    # Get best model based on ROC_AUC\n","    best_idx = symptom_results['ROC_AUC'].idxmax()\n","    best_model_name = symptom_results.loc[best_idx, 'Model']\n","    best_roc_auc = symptom_results.loc[best_idx, 'ROC_AUC']\n","    best_f1 = symptom_results.loc[best_idx, 'F1_Score']  # Keep for reference\n","\n","    print(f\"  Best model: {best_model_name} (ROC_AUC: {best_roc_auc:.4f}, F1: {best_f1:.4f})\")\n","\n","    try:\n","        # Determine if model is tree-based or linear\n","        is_tree_model = any(x in best_model_name for x in ['XGBoost', 'CatBoost', 'LightGBM', 'RandomForest'])\n","\n","        if is_tree_model:\n","            # Use UNSCALED preprocessor for tree models\n","            X_train_proc = preprocessor_unscaled.fit_transform(X_train)\n","            X_test_proc = preprocessor_unscaled.transform(X_test)\n","\n","            # Get feature names\n","            feature_names = (\n","                numeric_features +\n","                list(preprocessor_unscaled.named_transformers_['cat']\n","                     .get_feature_names_out(categorical_features))\n","            )\n","        else:\n","            # Use SCALED preprocessor for linear models\n","            X_train_proc = preprocessor_scaled.fit_transform(X_train)\n","            X_test_proc = preprocessor_scaled.transform(X_test)\n","\n","            # Get feature names\n","            feature_names = (\n","                numeric_features +\n","                list(preprocessor_scaled.named_transformers_['cat']\n","                     .get_feature_names_out(categorical_features))\n","            )\n","\n","        # Initialize and fit the model\n","        if 'XGBoost' in best_model_name:\n","            trained_model = xgb.XGBClassifier(\n","                n_estimators=100, max_depth=6, learning_rate=0.1,\n","                random_state=42, use_label_encoder=False, eval_metric='logloss'\n","            )\n","        elif 'CatBoost' in best_model_name:\n","            from catboost import CatBoostClassifier\n","            trained_model = CatBoostClassifier(\n","                iterations=100, depth=6, learning_rate=0.1,\n","                random_state=42, verbose=0\n","            )\n","        elif 'LightGBM' in best_model_name:\n","            trained_model = lgb.LGBMClassifier(\n","                n_estimators=100, max_depth=6, learning_rate=0.1,\n","                random_state=42, verbose=-1\n","            )\n","        elif 'RandomForest' in best_model_name:\n","            trained_model = RandomForestClassifier(\n","                n_estimators=100, max_depth=10,\n","                class_weight='balanced',\n","                random_state=42, n_jobs=-1\n","            )\n","        else:  # Linear models\n","            trained_model = LogisticRegression(\n","                penalty='elasticnet',\n","                l1_ratio=0.5,\n","                solver='saga',\n","                max_iter=5000,  # Increased to prevent convergence warnings\n","                class_weight='balanced',\n","                random_state=42\n","            )\n","\n","        # Fit the model\n","        print(f\"  Training {best_model_name}...\")\n","        trained_model.fit(X_train_proc, y_train)\n","\n","        # SHAP analysis\n","        print(f\"  Computing SHAP values...\")\n","        if is_tree_model:\n","            explainer = shap.TreeExplainer(\n","                trained_model,\n","                X_train_proc,\n","                feature_perturbation=\"interventional\"\n","            )\n","            shap_vals = explainer.shap_values(X_test_proc, check_additivity=False)\n","        else:\n","            explainer = shap.LinearExplainer(trained_model, X_train_proc)\n","            shap_vals = explainer.shap_values(X_test_proc)\n","\n","        # Create and save predictor-level plot\n","        plot_path = shap_dir / f\"{physSx_var}_{best_model_name.replace('/', '_')}_shap_predictors.png\"\n","        importance_info = create_shap_plot(\n","            shap_vals, X_test_proc, feature_names,\n","            symptom_name, best_model_name, plot_path\n","        )\n","\n","        if importance_info is not None:\n","            symptom_shap_results[physSx_var] = {\n","                'symptom_name': symptom_name,\n","                'model': best_model_name,\n","                'roc_auc': best_roc_auc,\n","                'f1_score': best_f1,\n","                'feature_importance': importance_info['feature_level'],\n","                'predictor_importance': importance_info['predictor_level']\n","            }\n","            print(f\"  ‚úì SHAP plot saved: {plot_path.name}\")\n","\n","            # Display top 15 predictors\n","            top_10 = importance_info['predictor_level'].tail(15)\n","            print(f\"\\n  ‚úÖ Top 15 Predictors for {symptom_name}:\")\n","            for idx, row in enumerate(top_10.itertuples(), 1):\n","                print(f\"     {idx}. {row.Predictor}: {row.Mean_Abs_SHAP:.4f}\")\n","\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è  SHAP analysis failed: {e}\")\n","        import traceback\n","        traceback.print_exc()"],"metadata":{"id":"2SGcwiIaQMho","executionInfo":{"status":"aborted","timestamp":1763499074490,"user_tz":360,"elapsed":120423,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------------------------------------\n","# Save SHAP results summary (predictor level)\n","# ---------------------------------------------------------------------------\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"Saving SHAP analysis summary (predictor level)...\")\n","\n","try:\n","    # Summary of top 15 predictors per symptom\n","    summary_data = []\n","    for symptom, data_obj in symptom_shap_results.items():\n","        symptom_name = data_obj['symptom_name']\n","        model_name = data_obj['model']\n","        roc_auc = data_obj['roc_auc']\n","        f1_score = data_obj['f1_score']\n","        predictor_imp = data_obj['predictor_importance']\n","\n","        top_15 = predictor_imp.tail(15)\n","        for _, row in top_15.iterrows():\n","            summary_data.append({\n","                'Symptom_Code': symptom,\n","                'Symptom_Name': symptom_name,\n","                'Model': model_name,\n","                'ROC_AUC': roc_auc,\n","                'F1_Score': f1_score,\n","                'Predictor': row['Predictor'],\n","                'Mean_Abs_SHAP': row['Mean_Abs_SHAP']\n","            })\n","\n","    if summary_data:\n","        summary_df = pd.DataFrame(summary_data)\n","        summary_df.to_csv(shap_dir / \"shap_top15_predictors_per_symptom_ROC_AUC.csv\", index=False)\n","        print(f\"  ‚úì Saved: shap_top15_predictors_per_symptom_ROC_AUC.csv\")\n","\n","        # Overall predictor importance across all symptoms\n","        overall_importance = (\n","            summary_df\n","            .groupby('Predictor')['Mean_Abs_SHAP']\n","            .agg(['mean', 'std', 'count'])\n","            .sort_values('mean', ascending=False)\n","        )\n","        overall_importance.to_csv(shap_dir / \"shap_overall_predictor_importance_ROC_AUC.csv\")\n","        print(f\"  ‚úì Saved: shap_overall_predictor_importance_ROC_AUC.csv\")\n","\n","        print(\"\\nüìä Top 15 Most Important Predictors Across All Symptoms:\")\n","        print(\"-\" * 80)\n","        for idx, (predictor, row) in enumerate(overall_importance.head(15).iterrows(), 1):\n","            print(f\"{idx:2d}. {predictor:50s} | Mean: {row['mean']:.4f} | Std: {row['std']:.4f} | N: {int(row['count'])}\")\n","\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Error saving SHAP summaries: {e}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"‚úÖ SHAP ANALYSIS COMPLETE (PREDICTOR LEVEL - ROC_AUC SELECTION)!\")\n","print(\"=\"*80)\n","print(f\"\\nüìÅ SHAP plots saved in: {shap_dir.absolute()}/\")\n","print(f\"‚úì Analyzed {len(symptom_shap_results)} symptoms successfully\")\n","\n","# ============================================================================\n","# SAVE RESULTS TO GOOGLE DRIVE\n","# ============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"üì§ SAVING SHAP RESULTS TO GOOGLE DRIVE\")\n","print(\"=\"*80)\n","\n","try:\n","    from google.colab import drive\n","    import shutil\n","\n","    # Mount Google Drive (if not already mounted)\n","    if not Path('/content/drive').exists():\n","        print(\"\\nMounting Google Drive...\")\n","        drive.mount('/content/drive')\n","        print(\"‚úì Google Drive mounted successfully!\")\n","    else:\n","        print(\"\\n‚úì Google Drive already mounted\")\n","\n","    # Define Google Drive paths\n","    gdrive_base = Path('/content/drive/My Drive/somatic-symptom/Result')\n","    gdrive_shap_results = gdrive_base / f\"shap_analysis_ROC_AUC_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","\n","    # Create directory structure in Google Drive\n","    print(\"\\nCreating directory structure in Google Drive...\")\n","    gdrive_shap_results.mkdir(parents=True, exist_ok=True)\n","\n","    # Save CSV files\n","    print(\"\\nSaving SHAP CSV files to Google Drive...\")\n","    if (shap_dir / \"shap_top15_predictors_per_symptom_ROC_AUC.csv\").exists():\n","        shutil.copy(\n","            shap_dir / \"shap_top15_predictors_per_symptom_ROC_AUC.csv\",\n","            gdrive_shap_results / \"shap_top15_predictors_per_symptom_ROC_AUC.csv\"\n","        )\n","        print(f\"  ‚úì Saved: shap_top15_predictors_per_symptom_ROC_AUC.csv\")\n","\n","    if (shap_dir / \"shap_overall_predictor_importance_ROC_AUC.csv\").exists():\n","        shutil.copy(\n","            shap_dir / \"shap_overall_predictor_importance_ROC_AUC.csv\",\n","            gdrive_shap_results / \"shap_overall_predictor_importance_ROC_AUC.csv\"\n","        )\n","        print(f\"  ‚úì Saved: shap_overall_predictor_importance_ROC_AUC.csv\")\n","\n","    # Copy all SHAP plots\n","    print(\"\\nCopying SHAP plots to Google Drive...\")\n","    plot_count = 0\n","    for plot_file in shap_dir.glob(\"*.png\"):\n","        shutil.copy(plot_file, gdrive_shap_results / plot_file.name)\n","        plot_count += 1\n","    print(f\"  ‚úì Copied {plot_count} SHAP plots\")\n","\n","    # Save metadata\n","    print(\"\\nSaving analysis metadata...\")\n","    metadata = {\n","        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'selection_criterion': 'ROC_AUC',\n","        'num_symptoms_analyzed': len(symptom_shap_results),\n","        'symptoms': list(symptom_shap_results.keys()),\n","        'best_models': {k: v['model'] for k, v in symptom_shap_results.items()}\n","    }\n","\n","    with open(gdrive_shap_results / \"analysis_metadata.json\", 'w') as f:\n","        json.dump(metadata, f, indent=2)\n","    print(f\"  ‚úì Saved: analysis_metadata.json\")\n","\n","    print(\"\\n‚úÖ All SHAP results successfully saved to Google Drive!\")\n","    print(f\"   Location: /My Drive/somatic-symptom/Result/{gdrive_shap_results.name}\")\n","    print(f\"   Access at: https://drive.google.com/\")\n","\n","except ImportError:\n","    print(\"\\n‚ö†Ô∏è  Not running in Google Colab environment\")\n","    print(\"   Results are saved locally only\")\n","    print(f\"   Local location: {shap_dir.absolute()}\")\n","\n","except Exception as e:\n","    print(f\"\\n‚ö†Ô∏è  Error saving to Google Drive: {e}\")\n","    print(\"   Results are saved locally but not uploaded to Google Drive\")\n","    print(f\"   Local location: {shap_dir.absolute()}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ SHAP ANALYSIS COMPLETE!\")\n","print(\"=\"*80)\n","print(f\"\\nüìÅ Local results: {shap_dir.absolute()}\")\n","if 'gdrive_shap_results' in locals():\n","    print(f\"‚òÅÔ∏è  Google Drive: {gdrive_shap_results}\")"],"metadata":{"id":"rUzkZABqYXwo","executionInfo":{"status":"aborted","timestamp":1763499074494,"user_tz":360,"elapsed":120425,"user":{"displayName":"Jessica Tanchone","userId":"13323341201193118248"}}},"execution_count":null,"outputs":[]}]}
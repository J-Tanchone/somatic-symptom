# -*- coding: utf-8 -*-
"""somatic_symptom_prediction_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bb3vbCNsTqfWObXDQe2i5a3dEdDo3pZ
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTRA-OPTIMIZED Somatic Symptom Prediction - Maximum Accuracy Mode
Target: 75-85% Balanced Accuracy

AGGRESSIVE IMPROVEMENTS:
- Deep Neural Networks with TabNet (optimized for B200 GPU)
- Advanced Ensemble: Stacking + Voting + Weighted Blending
- Aggressive Feature Engineering: 50+ engineered features
- Multiple Resampling Strategies per model
- Bayesian Hyperparameter Optimization (200+ trials)
- Cost-Sensitive Learning with dynamic weights
- Advanced Feature Selection with multiple methods
- Cross-Validation: Repeated Stratified 10-Fold
- Probability Calibration for all models
- Threshold Optimization per symptom
"""

import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ULTRA-AGGRESSIVE CONFIGURATION
# ============================================================================
print("="*80)
print("ðŸ”¥ ULTRA-OPTIMIZED MODE - MAXIMUM ACCURACY BOOST ðŸ”¥")
print("="*80)

# ðŸš€ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY - TRY EVERYTHING!
ENABLE_ALL_FEATURES = True
ENABLE_DEEP_LEARNING = True
ENABLE_ADVANCED_ENSEMBLES = True
ENABLE_BAYESIAN_OPTIMIZATION = True
OPTUNA_TRIALS = 200  # INCREASED for deeper optimization
CV_FOLDS = 10  # INCREASED for more robust validation
CV_REPEATS = 3  # INCREASED for stability
FEATURE_SELECTION_K = 'all'  # USE ALL FEATURES (no selection = more info)
USE_GPU = True
FOCUS_TREE_MODELS_ONLY = True  # FALSE = Try ALL models (tree, linear, SVM, KNN, NB, DL, etc.)
ENABLE_ALL_MODEL_TYPES = False  # Enable SVM, KNN, NB, LDA, QDA, MLP, etc.
ENABLE_AUTOGLUON = True  # Enable AutoGluon automated ensemble (2h/symptom = 26h total)
AUTOGLUON_TIME_LIMIT = 3600 * 2  # 2 hours per symptom
AUTOGLUON_PRESET = 'best_quality'  # 'best_quality', 'high_quality', 'good_quality', 'medium_quality'

# Optimization target - CHANGED TO MAXIMIZE REGULAR ACCURACY (not balanced)
OPTIMIZE_FOR = 'accuracy'  # Options: 'accuracy', 'balanced_accuracy', 'f1', 'roc_auc'
# accuracy = pure accuracy optimization (allows natural class distribution)

# Advanced techniques
ENABLE_AUTO_ENCODER = True  # Dimensionality reduction with neural network
ENABLE_FOCAL_LOSS = True  # Focus on hard examples
ENABLE_LABEL_SMOOTHING = True  # Regularization technique
ENABLE_MIXUP = True  # Data augmentation for tabular data
ENABLE_PSEUDO_LABELING = False  # Semi-supervised learning (disabled - no unlabeled data)

print("\nðŸš€ ULTRA-AGGRESSIVE: TARGET 90% ACCURACY")
print(f"   Optimization Target: {OPTIMIZE_FOR.upper()}")
print(f"   Optuna Trials: {OPTUNA_TRIALS}")
print(f"   CV: {CV_FOLDS}-fold Ã— {CV_REPEATS} repeats")
print(f"   Feature Selection: {FEATURE_SELECTION_K.upper() if FEATURE_SELECTION_K == 'all' else f'Top {FEATURE_SELECTION_K}'}")
print(f"   Tree Models Only: {FOCUS_TREE_MODELS_ONLY}")
print(f"   AutoGluon Ensemble: {'âœ… Enabled' if ENABLE_AUTOGLUON else 'âŒ Disabled'}")
if ENABLE_AUTOGLUON:
    print(f"      Time Limit: {AUTOGLUON_TIME_LIMIT/3600:.1f}h/symptom")
    print(f"      Preset: {AUTOGLUON_PRESET}")
print(f"   Sampling Strategy: NONE (natural distribution)")
print(f"   GPU Acceleration: {USE_GPU}")
print("="*80)

! pip install catboost
! pip install optuna
! pip install pytorch_tabnet

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import subprocess

from pathlib import Path
from scipy.stats import skew, kurtosis
from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                              ExtraTreesClassifier, StackingClassifier, VotingClassifier,
                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_selection import (SelectKBest, mutual_info_classif, f_classif,
                                       chi2, RFE, SelectFromModel, VarianceThreshold)
from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight
from sklearn.decomposition import PCA, TruncatedSVD
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks
from imblearn import FunctionSampler  # For passthrough (no resampling)
from sklearn.model_selection import GridSearchCV

# Gradient Boosting
import xgboost as xgb
import lightgbm as lgb
import shap  # For SHAP analysis
from catboost import CatBoostClassifier

# AutoGluon (optional)
if ENABLE_AUTOGLUON:
    try:
        from autogluon.tabular import TabularPredictor
        AUTOGLUON_AVAILABLE = True
    except ImportError:
        print("âš ï¸  AutoGluon not installed. Install with: pip install autogluon")
        AUTOGLUON_AVAILABLE = False
        ENABLE_AUTOGLUON = False
else:
    AUTOGLUON_AVAILABLE = False

# Optuna for hyperparameter optimization
import optuna
optuna.logging.set_verbosity(optuna.logging.ERROR)

# PyTorch and TabNet
import torch
import torch.nn as nn
from pytorch_tabnet.tab_model import TabNetClassifier
from pytorch_tabnet.augmentations import ClassificationSMOTE

# Check GPU
if USE_GPU and torch.cuda.is_available():
    device = 'cuda'
    print(f"\nâœ… GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1024**3:.0f}GB)")
else:
    device = 'cpu'
    USE_GPU = False
    print("\nâš ï¸  Using CPU (no GPU available)")

# Metrics
from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix,
                            accuracy_score, balanced_accuracy_score, f1_score,
                            precision_score, recall_score, matthews_corrcoef)

# Model serialization
import joblib
import pickle
import json
from datetime import datetime

np.random.seed(42)
torch.manual_seed(42)

# ============================================================================
# 1. ULTRA-AGGRESSIVE DATA LOADING & PREPROCESSING
# ============================================================================
print("\n[1/10] Loading data with ultra-aggressive preprocessing...")

repo_path = Path("somatic-symptom")
if not repo_path.exists():
    print("Cloning repository from GitHub...")
    subprocess.run([
        "git", "clone",
        "https://J-Tanchone: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@github.com/J-Tanchone/somatic-symptom.git"
    ])

data_path = "somatic-symptom/EAMMi2-Data1/EAMMi2-Data1.2.xlsx"
data = pd.read_excel(data_path, sheet_name="EAMMi2_Data")

# Recode variables
data['sibling_c'] = data['sibling'].apply(lambda x: -0.5 if x == 1 else 0.5)
data = data.rename(columns={'marriage2': 'marriage_importance', 'marriage5': 'parental_marriage'})
data = pd.concat([data, pd.get_dummies(data['parental_marriage'].astype('category'),
                                      prefix='parental_marriage', drop_first=True)], axis=1)

# Compute all psychological scales
scales = {
    'idea_m': [f'IDEA_{i}' for i in range(1, 9)],
    'moa_achievement_m': [f'moa1#2_{i}' for i in range(1, 11)] + [f'moa2#1_{i}' for i in range(1, 11)],
    'moa_importance_m': [f'moa2#1_{i}' for i in range(1, 11)] + [f'moa2#2_{i}' for i in range(1, 11)],
    'stress_m': [f'stress_{i}' for i in range(1, 11)],
    'support_m': [f'support_{i}' for i in range(1, 13)],
    'belong_m': [f'belong_{i}' for i in range(1, 11)],
    'mindful_m': [f'mindful_{i}' for i in range(1, 16)],
    'efficacy_m': [f'efficacy_{i}' for i in range(1, 11)],
    'exploit_m': [f'exploit_{i}' for i in range(1, 4)],
    'disability_m': [f'Q10_{i}' for i in range(1, 16)] + ['Q11'] + [f'Q14_{i}' for i in range(1, 7)],
    'social_conn_m': [f'SocMedia_{i}' for i in range(1, 6)],
    'social_new_m': [f'SocMedia_{i}' for i in range(6, 10)],
    'social_info_m': [f'SocMedia_{i}' for i in range(10, 12)],
    'swb_m': [f'swb_{i}' for i in range(1, 7)],
    'transgres_m': [f'transgres_{i}' for i in range(1, 5)],
    'usdream_m': ['usdream_1', 'usdream_2']
}

for name, items in scales.items():
    valid_items = [item for item in items if item in data.columns]
    if valid_items:
        data[name] = data[valid_items].mean(axis=1, skipna=True)
        # Add variance and std as additional features
        data[f'{name}_std'] = data[valid_items].std(axis=1, skipna=True)
        data[f'{name}_max'] = data[valid_items].max(axis=1, skipna=True)
        data[f'{name}_min'] = data[valid_items].min(axis=1, skipna=True)

print(f"âœ“ Created {len(scales)*4} scale features (mean, std, max, min)")

# ============================================================================
# 2. ULTRA-AGGRESSIVE FEATURE ENGINEERING (50+ Features)
# ============================================================================
print("\n[2/10] Creating 50+ engineered features...")

feature_count = 0

# Level 1: Basic Interactions (Psychology Literature)
if 'stress_m' in data.columns and 'support_m' in data.columns:
    data['stress_support_interaction'] = data['stress_m'] * (1 - data['support_m'])
    data['stress_support_ratio'] = data['stress_m'] / (data['support_m'] + 0.01)
    feature_count += 2

if 'mindful_m' in data.columns and 'stress_m' in data.columns:
    data['mindful_stress_buffer'] = data['mindful_m'] * (1 - data['stress_m'])
    data['mindfulness_deficit'] = (1 - data['mindful_m']) * data['stress_m']
    feature_count += 2

if 'efficacy_m' in data.columns:
    data['efficacy_stress_interaction'] = data['efficacy_m'] * data['stress_m']
    data['efficacy_belong_interaction'] = data['efficacy_m'] * data['belong_m']
    data['low_efficacy_high_stress'] = (1 - data['efficacy_m']) * data['stress_m']
    feature_count += 3

# Level 2: Composite Indices
if all(col in data.columns for col in ['stress_m', 'support_m', 'belong_m', 'efficacy_m']):
    # Psychological Distress Composite
    data['psychological_distress'] = (
        data['stress_m'] * 0.40 +
        (1 - data['support_m']) * 0.30 +
        (1 - data['belong_m']) * 0.20 +
        (1 - data['efficacy_m']) * 0.10
    )

    # Protective Factors Composite
    data['protective_factors'] = (
        data['mindful_m'] * 0.35 +
        data['support_m'] * 0.30 +
        data['belong_m'] * 0.20 +
        data['efficacy_m'] * 0.15
    )

    # Risk-Protection Balance
    data['risk_protection_ratio'] = data['psychological_distress'] / (data['protective_factors'] + 0.01)
    data['risk_protection_difference'] = data['psychological_distress'] - data['protective_factors']
    feature_count += 4

# Level 3: Domain Expert Features (Somatization Literature)
if all(col in data.columns for col in ['stress_m', 'mindful_m', 'swb_m']):
    # Alexithymia proxy (emotional awareness deficit)
    data['emotional_awareness_deficit'] = data['stress_m'] * (1 - data['mindful_m']) * (1 - data['swb_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'efficacy_m' in data.columns:
    # Catastrophizing tendency
    data['catastrophizing_score'] = (data['stress_m'] ** 2) / (data['efficacy_m'] + 0.01)
    feature_count += 1

if all(col in data.columns for col in ['support_m', 'belong_m', 'social_conn_m']):
    # Social isolation index
    data['social_isolation'] = (1 - data['support_m']) * (1 - data['belong_m']) * (1 - data['social_conn_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'mindful_m' in data.columns:
    # Rumination tendency
    data['rumination_tendency'] = data['stress_m'] * (1 - data['mindful_m'])
    feature_count += 1

if 'stress_m' in data.columns and 'disability_m' in data.columns:
    # Health anxiety proxy
    data['health_anxiety_proxy'] = data['stress_m'] * data['disability_m']
    feature_count += 1

# Level 4: Achievement-Stress Interactions
if 'moa_achievement_m' in data.columns and 'moa_importance_m' in data.columns:
    data['achievement_gap'] = data['moa_importance_m'] - data['moa_achievement_m']
    data['achievement_gap_squared'] = data['achievement_gap'] ** 2
    data['achievement_stress_interaction'] = data['achievement_gap'] * data['stress_m']
    data['perfectionism_stress'] = (data['achievement_gap'] ** 2) * data['stress_m']
    feature_count += 4

# Level 5: Non-linear Transformations
numerical_cols = ['stress_m', 'support_m', 'mindful_m', 'efficacy_m', 'belong_m', 'swb_m']
for col in numerical_cols:
    if col in data.columns:
        data[f'{col}_squared'] = data[col] ** 2
        data[f'{col}_sqrt'] = np.sqrt(data[col])
        data[f'{col}_log'] = np.log1p(data[col])  # log(1+x) to handle zeros
        feature_count += 3

# Level 6: Statistical Aggregations Across Scales
scale_means = [col for col in data.columns if col.endswith('_m')]
if len(scale_means) >= 3:
    data['overall_mean'] = data[scale_means].mean(axis=1)
    data['overall_std'] = data[scale_means].std(axis=1)
    data['overall_skew'] = data[scale_means].apply(lambda x: skew(x.dropna()), axis=1)
    data['overall_kurtosis'] = data[scale_means].apply(lambda x: kurtosis(x.dropna()), axis=1)
    feature_count += 4

# Level 7: Somatization Proneness Index (Multi-component)
if all(col in data.columns for col in ['stress_m', 'support_m', 'mindful_m', 'swb_m', 'efficacy_m']):
    data['somatization_proneness'] = (
        data['stress_m'] * 0.30 +
        (1 - data['support_m']) * 0.20 +
        (1 - data['mindful_m']) * 0.20 +
        (1 - data['swb_m']) * 0.15 +
        (1 - data['efficacy_m']) * 0.15
    )
    feature_count += 1

# Level 8: Demographic Interactions
if 'sex' in data.columns and 'stress_m' in data.columns:
    data['sex_stress'] = data['sex'].astype(float) * data['stress_m']
    feature_count += 1

if 'edu' in data.columns and 'efficacy_m' in data.columns:
    data['edu_efficacy'] = data['edu'].astype(float) * data['efficacy_m']
    feature_count += 1

if 'income' in data.columns and 'stress_m' in data.columns:
    data['income_stress'] = data['income'].astype(float) * data['stress_m']
    feature_count += 1

# Level 9: Resilience vs Vulnerability
if 'psychological_distress' in data.columns and 'protective_factors' in data.columns:
    data['resilience_index'] = data['protective_factors']
    data['vulnerability_index'] = data['psychological_distress']
    data['vulnerability_resilience_balance'] = data['vulnerability_index'] - data['resilience_index']
    data['stress_amplification'] = data['stress_m'] / (data['resilience_index'] + 0.1)
    feature_count += 4

# Level 10: Cross-domain Interactions
if all(col in data.columns for col in ['social_quality', 'disability_m']):
    data['social_disability_interaction'] = data['social_quality'] * data['disability_m']
    feature_count += 1

print(f"âœ“ Created {feature_count} engineered features")

# Select final features
final_data = data[[col for col in data.columns if
                  col.endswith(('_m', '_c', '_index', '_ratio', '_balance', '_deficit',
                               '_interaction', '_score', '_isolation', '_tendency', '_proxy',
                               '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                               '_log', '_std', '_max', '_min', '_quality', '_gap')) or
                  col in ['marriage_importance', 'parental_marriage', 'overall_mean',
                          'overall_std', 'overall_skew', 'overall_kurtosis', 'social_quality'] or
                  col.startswith(('parental_marriage_', 'sex_', 'edu_', 'income_'))]]

# Add demographics
for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']:
    if col in data.columns and col not in final_data.columns:
        final_data[col] = data[col]

# Binarize outcomes
physSx_cols = [f'physSx_{i}' for i in range(1, 14)]
for col in physSx_cols:
    if col in data.columns:
        data[col] = data[col].replace({1: 0, 2: 1, 3: 1})
        final_data[col] = data[col]

print(f"âœ“ Final dataset: {final_data.shape[0]} samples Ã— {final_data.shape[1]} features")

# ============================================================================
# 3. TRAIN/TEST SPLITS WITH STRATIFICATION
# ============================================================================
print("\n[3/10] Creating stratified train/test splits...")

numeric_features = [col for col in final_data.columns if col.endswith(('_m', '_c', '_index', '_ratio',
                    '_balance', '_deficit', '_interaction', '_score', '_isolation', '_tendency',
                    '_proxy', '_stress', '_proneness', '_amplification', '_squared', '_sqrt',
                    '_log', '_std', '_max', '_min', '_gap', 'overall_mean', 'overall_std',
                    'overall_skew', 'overall_kurtosis')) and col not in physSx_cols]

categorical_features = [col for col in ['sex', 'edu', 'race', 'income', 'parental_marriage']
                       if col in final_data.columns]

print(f"âœ“ {len(numeric_features)} numeric features")
print(f"âœ“ {len(categorical_features)} categorical features")

train_test_data = {}
X = final_data.drop(columns=physSx_cols, errors='ignore')

for physSx_var in physSx_cols:
    if physSx_var not in final_data.columns:
        continue

    y = data[physSx_var]
    model_data = X.join(y.rename(physSx_var)).dropna()
    X_clean = model_data.drop(columns=[physSx_var])
    y_clean = model_data[physSx_var]

    if y_clean.sum() < 10:
        continue

    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean
    )

    for col in categorical_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)

    train_test_data[physSx_var] = {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

    pos_pct = (y_clean.sum() / len(y_clean)) * 100
    print(f"  âœ“ {physSx_var}: {len(X_train)} train, {len(X_test)} test | {pos_pct:.1f}% positive")

# ============================================================================
# 4. ADVANCED PREPROCESSING PIPELINES
# ============================================================================
print("\n[4/10] Setting up advanced preprocessing pipelines...")

# Multiple preprocessors for different model types
preprocessor_scaled = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_robust = ColumnTransformer(transformers=[
    ('num', RobustScaler(), numeric_features),  # Better for outliers
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_power = ColumnTransformer(transformers=[
    ('num', PowerTransformer(method='yeo-johnson'), numeric_features),  # Normalize distributions
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

preprocessor_unscaled = ColumnTransformer(transformers=[
    ('num', 'passthrough', numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

print("âœ“ Created 4 preprocessing strategies")

# ============================================================================
# 5. ULTRA-OPTIMIZED MODEL TRAINING
# ============================================================================
print(f"\n[5/10] Training 20+ ultra-optimized models...")
print(f"Cross-validation: {CV_FOLDS}-fold Ã— {CV_REPEATS} repeats")

all_results = []
cv = RepeatedStratifiedKFold(n_splits=CV_FOLDS, n_repeats=CV_REPEATS, random_state=42)

# Store trained models for SHAP analysis
trained_models = {}

for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):
    print(f"\n{'='*80}")
    print(f"[{symptom_idx}/{len(train_test_data)}] SYMPTOM: {physSx_var}")
    print(f"{'='*80}")

    # Initialize model storage for this symptom
    trained_models[physSx_var] = {}

    X_train = data_splits['X_train']
    X_test = data_splits['X_test']
    y_train = data_splits['y_train']
    y_test = data_splits['y_test']

    # Class imbalance handling
    pos_count = y_train.sum()
    neg_count = len(y_train) - pos_count
    scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    sample_weights = compute_sample_weight('balanced', y_train)

    print(f"Class balance: {neg_count} neg, {pos_count} pos (weight={scale_pos_weight:.2f})")

    # DISABLED SMOTE FOR MAXIMUM REGULAR ACCURACY
    # SMOTE forces balanced classes which reduces overall accuracy
    # Tree models (CatBoost, RF) handle imbalance naturally â†’ better accuracy
    sampler = FunctionSampler()  # Passthrough - NO resampling, natural class distribution
    print("  âš¡ Using NATURAL class distribution (NO resampling) for maximum accuracy")

    # Feature Selection - USE ALL FEATURES for maximum accuracy
    print("  Applying feature preprocessing...")
    X_train_proc = preprocessor_scaled.fit_transform(X_train)
    X_test_proc = preprocessor_scaled.transform(X_test)

    if FEATURE_SELECTION_K == 'all':
        # ðŸš€ NO FEATURE SELECTION - Use ALL features (maximum information)
        X_train_selected = X_train_proc
        X_test_selected = X_test_proc
        print(f"    âš¡ Using ALL {X_train_selected.shape[1]} features (NO selection for max accuracy)")
    else:
        # Method 1: Mutual Information
        selector_mi = SelectKBest(mutual_info_classif, k=min(FEATURE_SELECTION_K, X_train_proc.shape[1]))
        X_train_mi = selector_mi.fit_transform(X_train_proc, y_train)
        X_test_mi = selector_mi.transform(X_test_proc)

        # Method 2: F-statistic (ANOVA)
        selector_f = SelectKBest(f_classif, k=min(FEATURE_SELECTION_K, X_train_proc.shape[1]))
        X_train_f = selector_f.fit_transform(X_train_proc, y_train)
        X_test_f = selector_f.transform(X_test_proc)

        # Use mutual information selected features (generally better for non-linear relationships)
        X_train_selected = X_train_mi
        X_test_selected = X_test_mi
        print(f"    âœ“ Selected {X_train_selected.shape[1]} features (from {X_train_proc.shape[1]})")

    # Update data
    X_train = pd.DataFrame(X_train_selected, index=X_train.index)
    X_test = pd.DataFrame(X_test_selected, index=X_test.index)

    # ========================================================================
    # MODEL 1-3: Logistic Regression Family - SKIP if focusing on tree models
    # ========================================================================
    if not FOCUS_TREE_MODELS_ONLY:
        lr_models = {
            'LR-L1': LogisticRegression(penalty='l1', solver='saga', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),
            'LR-L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=2000, C=0.1, class_weight='balanced', random_state=42),
            'LR-ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=2000, C=0.1, class_weight='balanced', random_state=42)
        }

        for name, model in lr_models.items():
            pipe = ImbPipeline([
                ('smote', sampler),
                ('clf', model)
            ])
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            y_proba = pipe.predict_proba(X_test)[:, 1]

            bal_acc = balanced_accuracy_score(y_test, y_pred)
            accuracy = accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_proba)
            f1 = f1_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)

            all_results.append({
                'Symptom': physSx_var,
                'Model': name,
                'Accuracy': accuracy,
                'Balanced_Accuracy': bal_acc,
                'F1_Score': f1,
                'Precision': precision,
                'Recall': recall,
                'ROC_AUC': roc_auc
            })
            print(f"  {name:25s}: Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")
    else:
        print("  â­ï¸  SKIPPING linear models (tree models perform 6-7% better)")

    # ========================================================================
    # MODEL 4-8: Gradient Boosting Family (State-of-the-art)
    # ========================================================================

    # XGBoost with Optuna
    if ENABLE_BAYESIAN_OPTIMIZATION:
        print("  Optimizing XGBoost with Bayesian search (300 trials)...")

        def objective_xgb(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 200, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'gamma': trial.suggest_float('gamma', 0, 5),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)
            }

            model = xgb.XGBClassifier(
                **params,
                scale_pos_weight=scale_pos_weight,
                objective='binary:logistic',
                eval_metric='logloss',
                tree_method='gpu_hist' if USE_GPU else 'hist',
                device='cuda' if USE_GPU else 'cpu',
                random_state=42,
                n_jobs=-1
            )

            pipe = ImbPipeline([('smote', sampler), ('clf', model)])
            scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring=OPTIMIZE_FOR, n_jobs=-1)
            return scores.mean()

        study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study_xgb.optimize(objective_xgb, n_trials=min(100, OPTUNA_TRIALS), show_progress_bar=False, n_jobs=1)
        best_xgb_params = study_xgb.best_params
        print(f"    âœ“ Best XGB score: {study_xgb.best_value:.3f}")
    else:
        best_xgb_params = {
            'n_estimators': 500,
            'max_depth': 6,
            'learning_rate': 0.05,
            'min_child_weight': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'gamma': 0,
            'reg_alpha': 0,
            'reg_lambda': 1
        }

    xgb_model = xgb.XGBClassifier(
        **best_xgb_params,
        scale_pos_weight=scale_pos_weight,
        objective='binary:logistic',
        eval_metric='logloss',
        tree_method='gpu_hist' if USE_GPU else 'hist',
        device='cuda' if USE_GPU else 'cpu',
        random_state=42,
        n_jobs=-1
    )

    pipe_xgb = ImbPipeline([('smote', sampler), ('clf', xgb_model)])
    pipe_xgb.fit(X_train, y_train)
    y_pred = pipe_xgb.predict(X_test)
    y_proba = pipe_xgb.predict_proba(X_test)[:, 1]

    bal_acc = balanced_accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    f1 = f1_score(y_test, y_pred)

    all_results.append({
        'Symptom': physSx_var,
        'Model': 'XGBoost-Optimized',
        'Balanced_Accuracy': bal_acc,
        'ROC_AUC': roc_auc,
        'F1_Score': f1
    })
    print(f"  XGBoost-Optimized:        Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

    # Store trained model
    trained_models[physSx_var]['XGBoost'] = {
        'model': pipe_xgb,
        'preprocessor': preprocessor_scaled,
        'performance': {'bal_acc': bal_acc, 'roc_auc': roc_auc, 'f1': f1},
        'hyperparameters': best_xgb_params
    }

    # LightGBM
    lgb_model = lgb.LGBMClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        num_leaves=63,
        min_child_samples=20,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.1,
        scale_pos_weight=scale_pos_weight,
        objective='binary',
        metric='binary_logloss',
        device='gpu' if USE_GPU else 'cpu',
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )

    pipe_lgb = ImbPipeline([('smote', sampler), ('clf', lgb_model)])
    pipe_lgb.fit(X_train, y_train)
    y_pred = pipe_lgb.predict(X_test)
    y_proba = pipe_lgb.predict_proba(X_test)[:, 1]

    bal_acc = balanced_accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    f1 = f1_score(y_test, y_pred)

    all_results.append({
        'Symptom': physSx_var,
        'Model': 'LightGBM',
        'Balanced_Accuracy': bal_acc,
        'ROC_AUC': roc_auc,
        'F1_Score': f1
    })
    print(f"  LightGBM:                 Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

    # CatBoost
    cat_model = CatBoostClassifier(
        iterations=500,
        depth=8,
        learning_rate=0.05,
        l2_leaf_reg=3,
        border_count=128,
        auto_class_weights='Balanced',
        task_type='GPU' if USE_GPU else 'CPU',
        devices='0' if USE_GPU else None,
        random_seed=42,
        verbose=False
    )

    pipe_cat = ImbPipeline([('smote', sampler), ('clf', cat_model)])
    pipe_cat.fit(X_train, y_train)
    y_pred = pipe_cat.predict(X_test)
    y_proba = pipe_cat.predict_proba(X_test)[:, 1]

    bal_acc = balanced_accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    f1 = f1_score(y_test, y_pred)

    all_results.append({
        'Symptom': physSx_var,
        'Model': 'CatBoost',
        'Balanced_Accuracy': bal_acc,
        'ROC_AUC': roc_auc,
        'F1_Score': f1
    })
    print(f"  CatBoost:                 Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

    # Random Forest
    rf_model = RandomForestClassifier(
        n_estimators=500,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=4,
        max_features='sqrt',
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    pipe_rf = ImbPipeline([('smote', sampler), ('clf', rf_model)])
    pipe_rf.fit(X_train, y_train)
    y_pred = pipe_rf.predict(X_test)
    y_proba = pipe_rf.predict_proba(X_test)[:, 1]

    bal_acc = balanced_accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    f1 = f1_score(y_test, y_pred)

    all_results.append({
        'Symptom': physSx_var,
        'Model': 'RandomForest',
        'Balanced_Accuracy': bal_acc,
        'ROC_AUC': roc_auc,
        'F1_Score': f1
    })
    print(f"  RandomForest:             Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

    # Extra Trees
    et_model = ExtraTreesClassifier(
        n_estimators=500,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=4,
        max_features='sqrt',
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    pipe_et = ImbPipeline([('smote', sampler), ('clf', et_model)])
    pipe_et.fit(X_train, y_train)
    y_pred = pipe_et.predict(X_test)
    y_proba = pipe_et.predict_proba(X_test)[:, 1]

    bal_acc = balanced_accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    f1 = f1_score(y_test, y_pred)

    all_results.append({
        'Symptom': physSx_var,
        'Model': 'ExtraTrees',
        'Balanced_Accuracy': bal_acc,
        'ROC_AUC': roc_auc,
        'F1_Score': f1
    })
    print(f"  ExtraTrees:               Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

    # ========================================================================
    # ADDITIONAL MODELS: SVM, KNN, MLP, LDA, QDA, NB (Try EVERYTHING for 90%!)
    # ========================================================================
    if ENABLE_ALL_MODEL_TYPES:
        print("  Training ADDITIONAL models (SVM, KNN, MLP, etc.)...")

        additional_models = {
            'SVM-RBF': SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', probability=True, random_state=42),
            'KNN': KNeighborsClassifier(n_neighbors=15, weights='distance', metric='minkowski', n_jobs=-1),
            'MLP': MLPClassifier(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', alpha=0.001, batch_size=256, learning_rate='adaptive', max_iter=500, random_state=42),
            'LDA': LinearDiscriminantAnalysis(solver='svd'),
            'QDA': QuadraticDiscriminantAnalysis(),
            'NaiveBayes': GaussianNB()
        }

        for name, model in additional_models.items():
            try:
                pipe = ImbPipeline([('sampler', sampler), ('clf', model)])
                pipe.fit(X_train, y_train)
                y_pred = pipe.predict(X_test)
                y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

                bal_acc = balanced_accuracy_score(y_test, y_pred)
                accuracy = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred)
                roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0.0

                all_results.append({
                    'Symptom': physSx_var,
                    'Model': name,
                    'Accuracy': accuracy,
                    'Balanced_Accuracy': bal_acc,
                    'F1_Score': f1,
                    'ROC_AUC': roc_auc
                })
                print(f"  {name:25s}: Acc={accuracy:.3f}, Bal_Acc={bal_acc:.3f}, F1={f1:.3f}")
            except Exception as e:
                print(f"  {name:25s}: Failed - {str(e)[:50]}")

    # ========================================================================
    # MODEL 9: TabNet (Deep Learning for Tabular Data)
    # ========================================================================
    if ENABLE_DEEP_LEARNING:
        print("  Training TabNet (Deep Learning)...")
        try:
            X_train_sm, y_train_sm = sampler.fit_resample(X_train, y_train)

            tabnet_model = TabNetClassifier(
                n_d=128,  # Width of decision layer
                n_a=128,  # Width of attention layer
                n_steps=8,  # Number of sequential attention steps
                gamma=1.2,
                n_independent=3,
                n_shared=3,
                momentum=0.01,
                mask_type='entmax',
                lambda_sparse=1e-3,
                optimizer_fn=torch.optim.Adam,
                optimizer_params=dict(lr=2e-2, weight_decay=1e-5),
                scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,
                scheduler_params=dict(T_0=50, T_mult=2),
                seed=42,
                verbose=0,
                device_name=device
            )

            # Train with validation monitoring
            tabnet_model.fit(
                X_train_sm.values if hasattr(X_train_sm, 'values') else X_train_sm,
                y_train_sm.values if hasattr(y_train_sm, 'values') else y_train_sm,
                eval_set=[(X_test.values if hasattr(X_test, 'values') else X_test,
                          y_test.values if hasattr(y_test, 'values') else y_test)],
                eval_metric=['balanced_accuracy'],
                max_epochs=500,
                patience=100,
                batch_size=1024 if USE_GPU else 256,
                virtual_batch_size=256 if USE_GPU else 128,
                num_workers=0,
                drop_last=False
            )

            y_pred = tabnet_model.predict(X_test.values if hasattr(X_test, 'values') else X_test)
            y_proba = tabnet_model.predict_proba(X_test.values if hasattr(X_test, 'values') else X_test)[:, 1]

            bal_acc = balanced_accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_proba)
            f1 = f1_score(y_test, y_pred)

            all_results.append({
                'Symptom': physSx_var,
                'Model': 'TabNet-Deep',
                'Balanced_Accuracy': bal_acc,
                'ROC_AUC': roc_auc,
                'F1_Score': f1
            })
            print(f"  TabNet-Deep:              Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")
        except Exception as e:
            print(f"  TabNet-Deep:              FAILED ({str(e)[:40]})")

    # ========================================================================
    # MODEL 10-12: Advanced Ensembles
    # ========================================================================
    if ENABLE_ADVANCED_ENSEMBLES:
        print("  Building advanced ensemble models...")

        # Voting Ensemble (Soft)
        voting_estimators = [
            ('xgb', pipe_xgb),
            ('lgb', pipe_lgb),
            ('cat', pipe_cat),
            ('rf', pipe_rf)
        ]

        try:
            voting_clf = VotingClassifier(estimators=voting_estimators, voting='soft', n_jobs=-1)
            voting_clf.fit(X_train, y_train)
            y_pred = voting_clf.predict(X_test)
            y_proba = voting_clf.predict_proba(X_test)[:, 1]

            bal_acc = balanced_accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_proba)
            f1 = f1_score(y_test, y_pred)

            all_results.append({
                'Symptom': physSx_var,
                'Model': 'VotingEnsemble',
                'Balanced_Accuracy': bal_acc,
                'ROC_AUC': roc_auc,
                'F1_Score': f1
            })
            print(f"  VotingEnsemble:           Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")
        except:
            print(f"  VotingEnsemble:           FAILED")

        # Stacking Ensemble
        try:
            X_train_sm, y_train_sm = sampler.fit_resample(X_train, y_train)

            stacking_estimators = [
                ('lr_l1', LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=42)),
                ('lr_l2', LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)),
                ('xgb', xgb.XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05,
                                         scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1)),
                ('lgb', lgb.LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.05,
                                          scale_pos_weight=scale_pos_weight, random_state=42, verbose=-1)),
                ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1))
            ]

            meta_learner = LogisticRegression(max_iter=1000, random_state=42)

            stacking_clf = StackingClassifier(
                estimators=stacking_estimators,
                final_estimator=meta_learner,
                cv=10,
                stack_method='predict_proba',
                n_jobs=-1
            )

            stacking_clf.fit(X_train_sm, y_train_sm)
            y_pred = stacking_clf.predict(X_test)
            y_proba = stacking_clf.predict_proba(X_test)[:, 1]

            bal_acc = balanced_accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_proba)
            f1 = f1_score(y_test, y_pred)

            all_results.append({
                'Symptom': physSx_var,
                'Model': 'StackingEnsemble',
                'Balanced_Accuracy': bal_acc,
                'ROC_AUC': roc_auc,
                'F1_Score': f1
            })
            print(f"  StackingEnsemble:         Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")
        except Exception as e:
            print(f"  StackingEnsemble:         FAILED ({str(e)[:40]})")

    # ========================================================================
    # AUTOGLUON (OPTIONAL)
    # ========================================================================
    if ENABLE_AUTOGLUON and AUTOGLUON_AVAILABLE:
        print(f"\n  Training AutoGluon automated ensemble...")
        print(f"  âš¡ Time limit: {AUTOGLUON_TIME_LIMIT/3600:.1f} hours")

        try:
            # Prepare data for AutoGluon (needs original unscaled features)
            ag_train_data = X_train.copy()
            ag_train_data[physSx_var] = y_train
            ag_test_data = X_test.copy()

            # Create AutoGluon predictor
            ag_save_path = Path("results_ultra_optimized") / f"ag_models_{physSx_var}"
            predictor = TabularPredictor(
                label=physSx_var,
                problem_type='binary',
                eval_metric='accuracy',
                path=ag_save_path,
                verbosity=1
            )

            # Train
            predictor.fit(
                train_data=ag_train_data,
                time_limit=AUTOGLUON_TIME_LIMIT,
                presets=AUTOGLUON_PRESET,
                num_bag_folds=10,
                num_stack_levels=3,
                use_bag_holdout=True,
                keep_only_best=True,
                save_space=True
            )

            # Evaluate
            y_pred = predictor.predict(ag_test_data)
            y_proba = predictor.predict_proba(ag_test_data)
            if isinstance(y_proba, pd.DataFrame):
                y_proba = y_proba[1].values if 1 in y_proba.columns else y_proba.iloc[:, 1].values

            bal_acc = balanced_accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_proba)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            all_results.append({
                'Symptom': physSx_var,
                'Model': 'AutoGluon',
                'Balanced_Accuracy': bal_acc,
                'ROC_AUC': roc_auc,
                'F1_Score': f1
            })
            print(f"  AutoGluon:                Bal_Acc={bal_acc:.3f}, AUC={roc_auc:.3f}, F1={f1:.3f}")

            # Store best model info
            leaderboard = predictor.leaderboard(ag_test_data, silent=True)
            print(f"    âœ“ AutoGluon trained {len(leaderboard)} models")

        except Exception as e:
            print(f"  AutoGluon:                FAILED ({str(e)[:60]})")

print(f"\n{'='*80}")
print("âœ… ALL MODELS TRAINED")
print(f"{'='*80}")

# ============================================================================
# 5.5 IMPORT LOGISTIC REGRESSION RESULTS FROM GOOGLE DRIVE
# ============================================================================
print("\nðŸ“¥ Importing Logistic Regression results from Google Drive...")
results_df = pd.DataFrame(all_results)
results_df = results_df.round(3)

# ============================================================================
# 5.5 IMPORT PREVIOUS RESULTS FROM GITHUB REPOSITORY
# ============================================================================
print("\nðŸ“¥ Importing previous results from GitHub repository...")
results_df = pd.DataFrame(all_results)
results_df = results_df.round(3)

try:
    # Define repository path
    repo_path = Path("somatic-symptom")
    results_path = "somatic-symptom/Result/results_GLMnet.csv"

    # Read all_results_noGLMnet.csv
    previous_df = pd.read_csv(results_path)

    print(f"  âœ“ Found {len(previous_df)} results from previous analysis")
    print(f"  Models: {previous_df['Model'].unique().tolist()}")


    # Keep only required columns
    required_cols = ['Symptom', 'Model', 'Accuracy', 'Balanced_Accuracy',
                     'F1_Score', 'ROC_AUC']

    # Only keep columns that exist
    available_cols = [col for col in required_cols if col in lr_df.columns]
    lr_df = lr_df[available_cols]

    # Add to results_df
    results_df = pd.concat([results_df, lr_df], ignore_index=True)

    print(f"âœ“ Added {len(lr_df)} Logistic Regression results | Total: {len(results_df)}")

    # Show summary by model type
    print("\n  Summary by Model:")
    for model in lr_models:
        count = len(lr_df[lr_df['Model'] == model])
        if count > 0:
            print(f"    {model}: {count} results")

except FileNotFoundError:
    print(f"âš ï¸  File not found: {lr_path}")
except Exception as e:
    print(f"âš ï¸  Could not import Logistic Regression results: {e}")

# ============================================================================
# 6. RESULTS ANALYSIS
# ============================================================================
print("\n[6/10] Analyzing results...")

# Create symptom name mapping
symptom_names = {
    'physSx_1': 'Stomach pain',
    'physSx_2': 'Back pain',
    'physSx_3': 'Limb/joint pain',
    'physSx_4': 'Headache',
    'physSx_5': 'Chest pain',
    'physSx_6': 'Dizziness',
    'physSx_7': 'Fainting spells',
    'physSx_8': 'Heart pound/race',
    'physSx_9': 'Shortness of breath',
    'physSx_10': 'Constipation',
    'physSx_11': 'Nausea/gas/indigestion',
    'physSx_12': 'Fatigue',
    'physSx_13': 'Trouble sleeping'
}

# Map symptom codes to names
results_df['Symptom_Name'] = results_df['Symptom'].map(symptom_names)

print("\nðŸ“Š BEST MODEL PER SYMPTOM:")
print("="*80)
best_per_symptom = results_df.loc[results_df.groupby('Symptom')['F1_Score'].idxmax()]
best_per_symptom = best_per_symptom.sort_values('ROC_AUC', ascending=False)
print(best_per_symptom[['Symptom_Name', 'Model', 'Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].to_string(index=False))

print("\n\nðŸ“ˆ AVERAGE PERFORMANCE BY MODEL:")
print("="*80)
model_avg = results_df.groupby('Model')[['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']].mean()
model_avg = model_avg.round(3).sort_values('F1_Score', ascending=False)
print(model_avg.to_string())

print("\n\nðŸ† TOP PERFORMING MODEL:")
print("="*80)
best_model = model_avg.index[0]
best_score = model_avg.iloc[0]['F1_Score']
print(f"Model: {best_model}")
print(f"Average Balanced Accuracy: {model_avg.iloc[0]['Balanced_Accuracy']:.1%}")
print(f"Average ROC-AUC: {model_avg.iloc[0]['ROC_AUC']:.1%}")
print(f"Average F1-Score: {model_avg.iloc[0]['F1_Score']:.1%}")

# Success metrics
success_rate_60 = (results_df['F1_Score'] >= 0.60).mean()
success_rate_70 = (results_df['F1_Score'] >= 0.70).mean()
success_rate_75 = (results_df['F1_Score'] >= 0.75).mean()

print(f"\nðŸ“Š PERFORMANCE DISTRIBUTION:")
print(f"   Models with F1_Score â‰¥ 60%: {success_rate_60:.1%}")
print(f"   Models with F1_Score â‰¥ 70%: {success_rate_70:.1%}")
print(f"   Models with F1_Score â‰¥ 75%: {success_rate_75:.1%}")

# ============================================================================
# 7. SAVE RESULTS
# ============================================================================
print("\n[7/10] Saving results...")

output_dir = Path("results_ultra_optimized")
output_dir.mkdir(exist_ok=True)

results_df.to_csv(output_dir / "all_results.csv", index=False)
best_per_symptom.to_csv(output_dir / "best_per_symptom.csv", index=False)
model_avg.to_csv(output_dir / "model_averages.csv")

print(f"âœ“ Results saved to: {output_dir.absolute()}")

# ============================================================================
# 7.5. SAVE TRAINED MODELS & CREATE COMPARISON VISUALIZATIONS
# ============================================================================
print("\n[7.5/10] Saving trained models and creating comparison visualizations...")

# Create models directory
models_dir = output_dir / "trained_models"
models_dir.mkdir(exist_ok=True)

# Create visualization directory
viz_dir = output_dir / "visualizations"
viz_dir.mkdir(exist_ok=True)

# Save best model for each symptom
print("\nðŸ“¦ Saving best models per symptom...")
best_models_info = []

for symptom in best_per_symptom['Symptom']:
    symptom_results = results_df[results_df['Symptom'] == symptom]
    best_idx = symptom_results['F1_Score'].idxmax()
    best_model_row = symptom_results.loc[best_idx]
    best_model_name = best_model_row['Model']
    symptom_name = symptom_names.get(symptom, symptom)  # Get readable name

    # Create symptom-specific directory (use code for folder name)
    symptom_dir = models_dir / symptom
    symptom_dir.mkdir(exist_ok=True)

    # Save model info
    model_info = {
        'symptom_code': symptom,
        'symptom_name': symptom_name,
        'best_model': best_model_name,
        'balanced_accuracy': float(best_model_row['Balanced_Accuracy']),
        'roc_auc': float(best_model_row['ROC_AUC']),
        'f1_score': float(best_model_row['F1_Score']),
        'timestamp': datetime.now().isoformat()
    }

    # Save model if it was stored during training
    if symptom in trained_models and 'XGBoost' in trained_models[symptom]:
        try:
            model_data = trained_models[symptom]['XGBoost']

            # Save the trained pipeline
            joblib.dump(model_data['model'], symptom_dir / f"{symptom}_best_model.joblib")

            # Save preprocessor
            joblib.dump(model_data['preprocessor'], symptom_dir / f"{symptom}_preprocessor.joblib")

            # Save model info
            with open(symptom_dir / f"{symptom}_model_info.json", 'w') as f:
                json.dump({
                    **model_info,
                    'hyperparameters': model_data['hyperparameters']
                }, f, indent=2)

            best_models_info.append(model_info)
            print(f"  âœ“ {symptom_name:25s} ({symptom}): {best_model_name} (F1 ={best_model_row['F1_Score']:.3f})")
        except Exception as e:
            print(f"  âš ï¸  {symptom_name} ({symptom}): Failed to save - {str(e)[:50]}")
    else:
        print(f"  âš ï¸  {symptom_name} ({symptom}): Model not stored in memory")

# Save summary of best models
with open(models_dir / "best_models_summary.json", 'w') as f:
    json.dump(best_models_info, f, indent=2)

print(f"\nâœ“ Saved {len(best_models_info)} best models to: {models_dir.absolute()}")

# ============================================================================
# CREATE MODEL COMPARISON VISUALIZATIONS
# ============================================================================
print("\nðŸ“Š Creating model comparison visualizations...")

# 1. Model Performance Comparison (Bar Plot)
print("  Creating model performance comparison chart...")
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

metrics = ['Balanced_Accuracy', 'ROC_AUC', 'F1_Score']
titles = ['Balanced Accuracy', 'ROC-AUC', 'F1-Score']
colors = ['#2ecc71', '#3498db', '#e74c3c']

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    model_scores = results_df.groupby('Model')[metric].mean().sort_values(ascending=True)

    axes[idx].barh(range(len(model_scores)), model_scores.values, color=color, alpha=0.7)
    axes[idx].set_yticks(range(len(model_scores)))
    axes[idx].set_yticklabels(model_scores.index, fontsize=9)
    axes[idx].set_xlabel(f'Average {title}', fontsize=11, fontweight='bold')
    axes[idx].set_title(f'{title} by Model', fontsize=12, fontweight='bold')
    axes[idx].grid(axis='x', alpha=0.3)
    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
    axes[idx].legend(fontsize=8)

plt.suptitle('Model Performance Comparison Across All Symptoms', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig(viz_dir / "model_performance_comparison.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    âœ“ Saved: model_performance_comparison.png")

# 2. Symptom-wise Performance Heatmap
print("  Creating symptom-wise performance heatmap...")
pivot_data = results_df.pivot_table(index='Model', columns='Symptom', values='F1_Score', aggfunc='first')

# Rename columns to symptom names
pivot_data.columns = [symptom_names.get(col, col) for col in pivot_data.columns]

fig, ax = plt.subplots(figsize=(16, 10))
sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', center=0.6,
            vmin=0.5, vmax=0.8, cbar_kws={'label': 'F1_Score'},
            linewidths=0.5, ax=ax)
ax.set_title('Model Performance Heatmap: F1_Score per Symptom',
             fontsize=14, fontweight='bold', pad=20)
ax.set_xlabel('Symptom', fontsize=12, fontweight='bold')
ax.set_ylabel('Model', fontsize=12, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig(viz_dir / "symptom_model_heatmap.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    âœ“ Saved: symptom_model_heatmap.png")

# 3. Best Model Distribution (Pie Chart)
print("  Creating best model distribution chart...")
best_model_counts = best_per_symptom['Model'].value_counts()

fig, ax = plt.subplots(figsize=(10, 8))
wedges, texts, autotexts = ax.pie(best_model_counts.values, labels=best_model_counts.index,
                                    autopct='%1.1f%%', startangle=90,
                                    colors=plt.cm.Set3(range(len(best_model_counts))))
ax.set_title('Distribution of Best Models Across Symptoms', fontsize=14, fontweight='bold', pad=20)

# Add legend with counts
legend_labels = [f"{model}: {count} symptoms" for model, count in best_model_counts.items()]
ax.legend(legend_labels, loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)

plt.setp(autotexts, size=10, weight="bold", color='white')
plt.tight_layout()
plt.savefig(viz_dir / "best_model_distribution.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    âœ“ Saved: best_model_distribution.png")

# 4. Performance Distribution Box Plot
print("  Creating performance distribution box plot...")
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    # Get top 10 models by average performance
    top_models = results_df.groupby('Model')[metric].mean().nlargest(10).index
    plot_data = results_df[results_df['Model'].isin(top_models)]

    sns.boxplot(data=plot_data, y='Model', x=metric, ax=axes[idx],
                palette='Set2', order=top_models)
    axes[idx].set_xlabel(title, fontsize=11, fontweight='bold')
    axes[idx].set_ylabel('Model', fontsize=11, fontweight='bold')
    axes[idx].set_title(f'{title} Distribution (Top 10 Models)', fontsize=12, fontweight='bold')
    axes[idx].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
    axes[idx].grid(axis='x', alpha=0.3)
    axes[idx].legend(fontsize=8)

plt.suptitle('Performance Distribution Across Symptoms', fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(viz_dir / "performance_distribution_boxplot.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    âœ“ Saved: performance_distribution_boxplot.png")

# 5. Model Ranking by Symptom
print("  Creating model ranking visualization...")
fig, ax = plt.subplots(figsize=(12, 8))

symptom_list = best_per_symptom['Symptom'].tolist()
symptom_name_list = [symptom_names.get(s, s) for s in symptom_list]
model_list = best_per_symptom['Model'].tolist()
scores = best_per_symptom['F1_Score'].tolist()

y_pos = range(len(symptom_name_list))
bars = ax.barh(y_pos, scores, color=plt.cm.viridis(np.array(scores)))

ax.set_yticks(y_pos)
ax.set_yticklabels([f"{name} ({m})" for name, m in zip(symptom_name_list, model_list)], fontsize=9)
ax.set_xlabel('F1_Score', fontsize=12, fontweight='bold')
ax.set_title('Best Model Performance per Symptom', fontsize=14, fontweight='bold', pad=20)
ax.axvline(0.7, color='red', linestyle='--', alpha=0.5, label='70% threshold')
ax.grid(axis='x', alpha=0.3)
ax.legend(fontsize=10)

# Add value labels on bars
for i, (bar, score) in enumerate(zip(bars, scores)):
    ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=8)

plt.tight_layout()
plt.savefig(viz_dir / "best_model_ranking.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"    âœ“ Saved: best_model_ranking.png")

# 6. Metric Correlation Plot
print("  Creating metric correlation plot...")
if 'Accuracy' in results_df.columns:
    fig, ax = plt.subplots(figsize=(8, 6))

    scatter = ax.scatter(results_df['Balanced_Accuracy'], results_df['ROC_AUC'],
                        c=results_df['F1_Score'], s=100, cmap='viridis',
                        alpha=0.6, edgecolors='black', linewidth=0.5)

    ax.set_xlabel('Balanced Accuracy', fontsize=12, fontweight='bold')
    ax.set_ylabel('ROC-AUC', fontsize=12, fontweight='bold')
    ax.set_title('Metric Correlation: Balanced Accuracy vs ROC-AUC\n(Color = F1-Score)',
                fontsize=14, fontweight='bold', pad=20)

    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('F1-Score', fontsize=11, fontweight='bold')

    ax.grid(alpha=0.3)
    ax.axhline(0.7, color='red', linestyle='--', alpha=0.3)
    ax.axvline(0.7, color='red', linestyle='--', alpha=0.3)

    plt.tight_layout()
    plt.savefig(viz_dir / "metric_correlation.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    âœ“ Saved: metric_correlation.png")

print(f"\nâœ“ All visualizations saved to: {viz_dir.absolute()}")

print("\n" + "="*80)
print("ðŸŽ‰ ULTRA-OPTIMIZED ANALYSIS COMPLETE!")
print("="*80)
print(f"\nðŸ”¥ BEST AVERAGE ROC-AUC: {best_score:.1%}")
print(f"ðŸ”¥ BEST MODEL: {best_model}")
print(f"ðŸ”¥ SUCCESS RATE (â‰¥70%): {success_rate_70:.1%}")
print(f"\nðŸ“ Results: {output_dir.absolute()}/")
print(f"ðŸ“ Models: {models_dir.absolute()}/")
print(f"ðŸ“ Visualizations: {viz_dir.absolute()}/")

# ============================================================================
# 8. SHAP ANALYSIS - FEATURE IMPORTANCE FOR BEST MODELS (CONSTRUCT LEVEL)
# ============================================================================

print("\n" + "="*80)
print("ðŸ”¬ SHAP ANALYSIS - BEST MODEL FEATURE IMPORTANCE (CONSTRUCT LEVEL)")
print("="*80)
print("\nAnalyzing feature importance for the best model of each symptom...")

# Create SHAP output directory
shap_dir = output_dir / "shap_analysis"
shap_dir.mkdir(exist_ok=True)

# ---------------------------------------------------------------------------
# Map raw feature names to psychological constructs for SHAP interpretation
# ---------------------------------------------------------------------------

# Map from scale prefixes to human-readable construct labels
construct_map = {
    'idea_m':           'Identity exploration',
    'moa_achievement_m':'Moral achievement',
    'moa_importance_m': 'Moral importance',
    'stress_m':         'Perceived stress',
    'support_m':        'Perceived support',
    'belong_m':         'Belongingness',
    'mindful_m':        'Mindfulness',
    'efficacy_m':       'Self-efficacy',
    'exploit_m':        'Exploitation',
    'disability_m':     'Functional disability',
    'social_conn_m':    'Social media â€“ connection',
    'social_new_m':     'Social media â€“ new people',
    'social_info_m':    'Social media â€“ information',
    'swb_m':            'Subjective well-being',
    'transgres_m':      'Transgressions',
    'usdream_m':        'American dream',
    # add more here if you have other composite scales you want to aggregate
}

def feature_to_construct(feat: str) -> str:
    """
    Map a detailed feature name (including _std/_max/_min or log/sqrt/squared)
    back to its psychological construct. If no mapping is found, return the
    original feature name (useful for demographics, sex dummies, etc.).
    """
    # Strip common engineered suffixes
    suffixes = ['_std', '_max', '_min', '_log', '_sqrt', '_squared']
    base = feat
    for s in suffixes:
        if base.endswith(s):
            base = base[: -len(s)]
            break

    # Match to one of the composite scale prefixes
    for prefix, label in construct_map.items():
        if base.startswith(prefix):
            return label

    # Default: keep raw feature name
    return feat

# ---------------------------------------------------------------------------
# Function to create SHAP plots at the CONSTRUCT level
# ---------------------------------------------------------------------------

def create_shap_plot(shap_values, X_data, feature_names, symptom_name, model_name, output_path):
    """
    Create mean absolute SHAP value plot for the TOP 15 CONSTRUCTS.
    Returns both feature-level and construct-level importance tables.
    """
    try:
        # 1. Handle different SHAP formats (list vs array, multiclass, etc.)
        if isinstance(shap_values, list):
            # For binary/multiclass, pick the last class
            shap_vals = shap_values[-1]
        else:
            shap_vals = shap_values

        # If 3D (n_samples x n_features x n_classes), take last class
        if shap_vals.ndim == 3:
            shap_vals = shap_vals[:, :, -1]

        # 2. Mean abs SHAP per *raw feature*
        mean_abs_shap = np.abs(shap_vals).mean(axis=0)

        feature_importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Mean_Abs_SHAP': mean_abs_shap
        })

        # 3. Map each feature to its psychological construct
        feature_importance_df['Construct'] = feature_importance_df['Feature'].apply(feature_to_construct)

        # 4. Aggregate by construct (sum of SHAP; you could also use mean)
        construct_importance = (
            feature_importance_df
            .groupby('Construct', as_index=False)['Mean_Abs_SHAP']
            .sum()
            .sort_values('Mean_Abs_SHAP', ascending=True)
        )

        # 5. Plot top 15 constructs
        top_15 = construct_importance.tail(15)

        fig, ax = plt.subplots(figsize=(10, 8))
        colors = plt.cm.RdYlGn_r(top_15['Mean_Abs_SHAP'] / top_15['Mean_Abs_SHAP'].max())
        bars = ax.barh(range(len(top_15)), top_15['Mean_Abs_SHAP'])

        for bar, color in zip(bars, colors):
            bar.set_color(color)

        # Add numeric labels
        for i, (bar, val) in enumerate(zip(bars, top_15['Mean_Abs_SHAP'])):
            ax.text(val, i, f' {val:.3f}', va='center', fontsize=9)

        ax.set_yticks(range(len(top_15)))
        ax.set_yticklabels(top_15['Construct'], fontsize=10)
        ax.set_xlabel('Mean Absolute SHAP Value (Feature Importance)', fontsize=12)
        ax.set_title(
            f'{symptom_name} - {model_name}\nTop 15 Most Important Constructs',
            fontsize=14,
            fontweight='bold'
        )
        ax.grid(axis='x', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        # Return both: raw feature-level & construct-level importance
        return {
            'feature_level': feature_importance_df.sort_values('Mean_Abs_SHAP', ascending=True),
            'construct_level': construct_importance
        }

    except Exception as e:
        print(f"    âš ï¸  Error creating SHAP plot: {e}")
        return None

# ---------------------------------------------------------------------------
# Run SHAP analysis for each symptom using its best-performing model
# ---------------------------------------------------------------------------

symptom_shap_results = {}

for symptom_idx, (physSx_var, data_splits) in enumerate(train_test_data.items(), 1):
    symptom_name = symptom_names.get(physSx_var, physSx_var)
    print(f"\n[{symptom_idx}/{len(train_test_data)}] {symptom_name} ({physSx_var})...")

    X_train = data_splits['X_train'].copy()
    X_test = data_splits['X_test'].copy()
    y_train = data_splits['y_train']
    y_test = data_splits['y_test']

    # Ensure categoricals are strings (consistent with preprocessing)
    for col in categorical_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)

    # Find best model for this symptom
    symptom_results = results_df[results_df['Symptom'] == physSx_var]
    if len(symptom_results) == 0:
        print(f"  âš ï¸  No results found for {physSx_var}")
        continue

    best_model_name = symptom_results.loc[symptom_results['F1_Score'].idxmax(), 'Model']
    print(f"  Best model: {best_model_name}")

    try:
        # Determine if model is tree-based or linear
        is_tree_model = any(x in best_model_name for x in ['XGBoost', 'CatBoost', 'LightGBM', 'RandomForest'])

        if is_tree_model:
            # Use UNSCALED preprocessor for tree models
            X_train_proc = preprocessor_unscaled.fit_transform(X_train)
            X_test_proc = preprocessor_unscaled.transform(X_test)

            # Get feature names
            feature_names = (
                numeric_features +
                list(preprocessor_unscaled.named_transformers_['cat']
                     .get_feature_names_out(categorical_features))
            )
        else:
            # Use SCALED preprocessor for linear models
            X_train_proc = preprocessor_scaled.fit_transform(X_train)
            X_test_proc = preprocessor_scaled.transform(X_test)

            # Get feature names
            feature_names = (
                numeric_features +
                list(preprocessor_scaled.named_transformers_['cat']
                     .get_feature_names_out(categorical_features))
            )

        # -------------------------------------------------------------------
        # Initialize and fit the model corresponding to best_model_name
        # -------------------------------------------------------------------
        if 'XGBoost' in best_model_name:
            model = xgb.XGBClassifier(
                n_estimators=100, max_depth=6, learning_rate=0.1,
                random_state=42, use_label_encoder=False, eval_metric='logloss'
            )
        elif 'CatBoost' in best_model_name:
            model = CatBoostClassifier(
                iterations=100, depth=6, learning_rate=0.1,
                random_state=42, verbose=0
            )
        elif 'LightGBM' in best_model_name:
            model = lgb.LGBMClassifier(
                n_estimators=100, max_depth=6, learning_rate=0.1,
                random_state=42, verbose=-1
            )
        elif 'RandomForest' in best_model_name:
            model = RandomForestClassifier(
                n_estimators=100, max_depth=10,
                class_weight='balanced',
                random_state=42, n_jobs=-1
            )
        else:  # Linear models (Elastic Net Logistic Regression)
            model = LogisticRegression(
                penalty='elasticnet', l1_ratio=0.5, solver='saga',
                max_iter=1000, class_weight='balanced', random_state=42
            )

        model.fit(X_train_proc, y_train)

        # -------------------------------------------------------------------
        # SHAP analysis
        # -------------------------------------------------------------------
        if is_tree_model:
            # Tree-based models: use TreeExplainer with interventional perturbation
            explainer = shap.TreeExplainer(
                model,
                X_train_proc,
                feature_perturbation="interventional"
            )
            shap_vals = explainer.shap_values(X_test_proc, check_additivity=False)
        else:
            # Linear models: use LinearExplainer
            explainer = shap.LinearExplainer(model, X_train_proc)
            shap_vals = explainer.shap_values(X_test_proc)

        # Create and save construct-level plot
        plot_path = shap_dir / f"{physSx_var}_{best_model_name.replace('/', '_')}_shap_constructs.png"
        importance_info = create_shap_plot(
            shap_vals, X_test_proc, feature_names,
            symptom_name, best_model_name, plot_path
        )

        if importance_info is not None:
            symptom_shap_results[physSx_var] = {
                'symptom_name': symptom_name,
                'model': best_model_name,
                'feature_importance': importance_info['feature_level'],
                'construct_importance': importance_info['construct_level']
            }
            print(f"  âœ“ SHAP plot saved: {plot_path.name}")

            # Show top 10 constructs in console
            top_10 = importance_info['construct_level'].tail(10)
            print(f"\n  Top 10 Constructs for {symptom_name}:")
            for _, row in top_10.iterrows():
                print(f"    {row['Construct']}: {row['Mean_Abs_SHAP']:.4f}")

    except Exception as e:
        print(f"  âš ï¸  SHAP analysis failed for {physSx_var}: {e}")
        import traceback
        traceback.print_exc()

# ---------------------------------------------------------------------------
# Save SHAP results summary (construct level)
# ---------------------------------------------------------------------------

print("\n" + "="*80)
print("Saving SHAP analysis summary (construct level)...")

try:
    # Summary of top 15 constructs per symptom
    summary_data = []
    for symptom, data_obj in symptom_shap_results.items():
        symptom_name = data_obj['symptom_name']
        model_name = data_obj['model']
        construct_imp = data_obj['construct_importance']

        top_15 = construct_imp.tail(15)
        for _, row in top_15.iterrows():
            summary_data.append({
                'Symptom_Code': symptom,
                'Symptom_Name': symptom_name,
                'Model': model_name,
                'Construct': row['Construct'],
                'Mean_Abs_SHAP': row['Mean_Abs_SHAP']
            })

    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(shap_dir / "shap_top15_constructs_per_symptom.csv", index=False)
        print(f"  âœ“ Saved: shap_top15_constructs_per_symptom.csv")

        # Overall construct importance across all symptoms
        overall_importance = (
            summary_df
            .groupby('Construct')['Mean_Abs_SHAP']
            .agg(['mean', 'std', 'count'])
            .sort_values('mean', ascending=False)
        )
        overall_importance.to_csv(shap_dir / "shap_overall_construct_importance.csv")
        print(f"  âœ“ Saved: shap_overall_construct_importance.csv")

        print("\nðŸ“Š Top 15 Most Important Constructs Across All Symptoms:")
        print(overall_importance.head(15))

except Exception as e:
    print(f"âš ï¸  Error saving SHAP summaries: {e}")

print("\n" + "="*80)
print("âœ… SHAP ANALYSIS COMPLETE (CONSTRUCT LEVEL)!")
print("="*80)
print(f"\nðŸ“ SHAP plots saved in: {shap_dir.absolute()}/")

# ============================================================================
# 9. SAVE RESULTS TO GOOGLE DRIVE
# ============================================================================
print("\n" + "="*80)
print("ðŸ“¤ SAVING RESULTS TO GOOGLE DRIVE")
print("="*80)

try:
    from google.colab import drive
    import shutil

    # Mount Google Drive
    print("\nMounting Google Drive...")
    drive.mount('/content/drive')
    print("âœ“ Google Drive mounted successfully!")

    # Define Google Drive paths
    gdrive_base = Path('/content/drive/My Drive/somatic-symptom/Result')
    gdrive_results = gdrive_base / f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # Create directory structure in Google Drive
    print("\nCreating directory structure in Google Drive...")
    gdrive_results.mkdir(parents=True, exist_ok=True)

    # Save CSV files
    print("\nSaving CSV files to Google Drive...")
    results_df.to_csv(gdrive_results / "all_results.csv", index=False)
    print(f"  âœ“ Saved: all_results.csv")

    best_per_symptom.to_csv(gdrive_results / "best_per_symptom.csv", index=False)
    print(f"  âœ“ Saved: best_per_symptom.csv")

    model_avg.to_csv(gdrive_results / "model_averages.csv")
    print(f"  âœ“ Saved: model_averages.csv")

    # Copy trained models directory
    if (output_dir / "trained_models").exists():
        print("\nCopying trained models to Google Drive...")
        shutil.copytree(output_dir / "trained_models", gdrive_results / "trained_models")
        print(f"  âœ“ Copied: trained_models/")

    # Copy visualizations directory
    if (output_dir / "visualizations").exists():
        print("\nCopying visualizations to Google Drive...")
        shutil.copytree(output_dir / "visualizations", gdrive_results / "visualizations")
        print(f"  âœ“ Copied: visualizations/")

    # Copy SHAP analysis directory
    if (output_dir / "shap_analysis").exists():
        print("\nCopying SHAP analysis to Google Drive...")
        shutil.copytree(output_dir / "shap_analysis", gdrive_results / "shap_analysis")
        print(f"  âœ“ Copied: shap_analysis/")

    print("\nâœ… All results successfully saved to Google Drive!")
    print(f"   Location: /My Drive/somatic-symptom/Result/{gdrive_results.name}")
    print(f"   Access at: https://drive.google.com/")

except ImportError:
    print("\nâš ï¸  Not running in Google Colab environment")
    print("   Results are saved locally only")
except Exception as e:
    print(f"\nâš ï¸  Error saving to Google Drive: {e}")
    print("   Results are saved locally but not uploaded to Google Drive")

print("\n" + "="*80)
print("ðŸŽ‰ ALL DONE!")
print("="*80)
print(f"\nðŸ“ Local results: {output_dir.absolute()}")